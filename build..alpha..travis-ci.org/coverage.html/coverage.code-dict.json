{"/home/travis/build/npmtest/node-npmtest-limdu/test.js":"/* istanbul instrument in package npmtest_limdu */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        switch (local.modeJs) {\n        // re-init local from window.local\n        case 'browser':\n            local = local.global.utility2.objectSetDefault(\n                local.global.utility2_rollup || local.global.local,\n                local.global.utility2\n            );\n            break;\n        // re-init local from example.js\n        case 'node':\n            local = (local.global.utility2_rollup || require('utility2'))\n                .requireExampleJsFromReadme();\n            break;\n        }\n        // export local\n        local.global.local = local;\n    }());\n\n\n\n    // run shared js-env code - function\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - function\n    case 'browser':\n        break;\n\n\n\n    // run node js-env code - function\n    case 'node':\n        break;\n    }\n\n\n\n    // run shared js-env code - post-init\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - post-init\n    case 'browser':\n        local.testCase_browser_nullCase = local.testCase_browser_nullCase || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test browsers's null-case handling-behavior-behavior\n         */\n            onError(null, options);\n        };\n\n        // run tests\n        local.nop(local.modeTest &&\n            document.querySelector('#testRunButton1') &&\n            document.querySelector('#testRunButton1').click());\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        local.testCase_buildApidoc_default = local.testCase_buildApidoc_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApidoc's default handling-behavior-behavior\n         */\n            options = { modulePathList: module.paths };\n            local.buildApidoc(options, onError);\n        };\n\n        local.testCase_buildApp_default = local.testCase_buildApp_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApp's default handling-behavior-behavior\n         */\n            local.testCase_buildReadme_default(options, local.onErrorThrow);\n            local.testCase_buildLib_default(options, local.onErrorThrow);\n            local.testCase_buildTest_default(options, local.onErrorThrow);\n            local.testCase_buildCustomOrg_default(options, local.onErrorThrow);\n            options = [];\n            local.buildApp(options, onError);\n        };\n\n        local.testCase_buildCustomOrg_default = local.testCase_buildCustomOrg_default ||\n            function (options, onError) {\n            /*\n             * this function will test buildCustomOrg's default handling-behavior\n             */\n                options = {};\n                local.buildCustomOrg(options, onError);\n            };\n\n        local.testCase_buildLib_default = local.testCase_buildLib_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildLib's default handling-behavior\n         */\n            options = {};\n            local.buildLib(options, onError);\n        };\n\n        local.testCase_buildReadme_default = local.testCase_buildReadme_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildReadme's default handling-behavior-behavior\n         */\n            options = {};\n            local.buildReadme(options, onError);\n        };\n\n        local.testCase_buildTest_default = local.testCase_buildTest_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildTest's default handling-behavior\n         */\n            options = {};\n            local.buildTest(options, onError);\n        };\n\n        local.testCase_webpage_default = local.testCase_webpage_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test webpage's default handling-behavior\n         */\n            options = { modeCoverageMerge: true, url: local.serverLocalHost + '?modeTest=1' };\n            local.browserTest(options, onError);\n        };\n\n        // run test-server\n        local.testRunServer(local);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-limdu/lib.npmtest_limdu.js":"/* istanbul instrument in package npmtest_limdu */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || local;\n        // init lib\n        local.local = local.npmtest_limdu = local;\n        // init exports\n        if (local.modeJs === 'browser') {\n            local.global.utility2_npmtest_limdu = local;\n        } else {\n            module.exports = local;\n            module.exports.__dirname = __dirname;\n            module.exports.module = module;\n        }\n    }());\n}());\n","/home/travis/build/npmtest/node-npmtest-limdu/example.js":"/*\nexample.js\n\nquickstart example\n\ninstruction\n    1. save this script as example.js\n    2. run the shell command:\n        $ npm install npmtest-limdu && PORT=8081 node example.js\n    3. play with the browser-demo on http://127.0.0.1:8081\n*/\n\n\n\n/* istanbul instrument in package npmtest_limdu */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || (local.modeJs === 'browser'\n            ? local.global.utility2_npmtest_limdu\n            : global.utility2_moduleExports);\n        // export local\n        local.global.local = local;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // post-init\n    // run browser js-env code - post-init\n    /* istanbul ignore next */\n    case 'browser':\n        local.testRunBrowser = function (event) {\n            if (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('onreset'))) {\n                // reset output\n                Array.from(\n                    document.querySelectorAll('body > .resettable')\n                ).forEach(function (element) {\n                    switch (element.tagName) {\n                    case 'INPUT':\n                    case 'TEXTAREA':\n                        element.value = '';\n                        break;\n                    default:\n                        element.textContent = '';\n                    }\n                });\n            }\n            switch (event && event.currentTarget && event.currentTarget.id) {\n            case 'testRunButton1':\n                // show tests\n                if (document.querySelector('#testReportDiv1').style.display === 'none') {\n                    document.querySelector('#testReportDiv1').style.display = 'block';\n                    document.querySelector('#testRunButton1').textContent =\n                        'hide internal test';\n                    local.modeTest = true;\n                    local.testRunDefault(local);\n                // hide tests\n                } else {\n                    document.querySelector('#testReportDiv1').style.display = 'none';\n                    document.querySelector('#testRunButton1').textContent = 'run internal test';\n                }\n                break;\n            // custom-case\n            default:\n                break;\n            }\n            if (document.querySelector('#inputTextareaEval1') && (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('oneval')))) {\n                // try to eval input-code\n                try {\n                    /*jslint evil: true*/\n                    eval(document.querySelector('#inputTextareaEval1').value);\n                } catch (errorCaught) {\n                    console.error(errorCaught);\n                }\n            }\n        };\n        // log stderr and stdout to #outputTextareaStdout1\n        ['error', 'log'].forEach(function (key) {\n            console[key + '_original'] = console[key];\n            console[key] = function () {\n                var element;\n                console[key + '_original'].apply(console, arguments);\n                element = document.querySelector('#outputTextareaStdout1');\n                if (!element) {\n                    return;\n                }\n                // append text to #outputTextareaStdout1\n                element.value += Array.from(arguments).map(function (arg) {\n                    return typeof arg === 'string'\n                        ? arg\n                        : JSON.stringify(arg, null, 4);\n                }).join(' ') + '\\n';\n                // scroll textarea to bottom\n                element.scrollTop = element.scrollHeight;\n            };\n        });\n        // init event-handling\n        ['change', 'click', 'keyup'].forEach(function (event) {\n            Array.from(document.querySelectorAll('.on' + event)).forEach(function (element) {\n                element.addEventListener(event, local.testRunBrowser);\n            });\n        });\n        // run tests\n        local.testRunBrowser();\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        // export local\n        module.exports = local;\n        // require modules\n        local.fs = require('fs');\n        local.http = require('http');\n        local.url = require('url');\n        // init assets\n        local.assetsDict = local.assetsDict || {};\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.index.template.html'] = '\\\n<!doctype html>\\n\\\n<html lang=\"en\">\\n\\\n<head>\\n\\\n<meta charset=\"UTF-8\">\\n\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\\n<title>{{env.npm_package_name}} (v{{env.npm_package_version}})</title>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n    box-sizing: false,\\n\\\n    universal-selector: false\\n\\\n*/\\n\\\n* {\\n\\\n    box-sizing: border-box;\\n\\\n}\\n\\\nbody {\\n\\\n    background: #dde;\\n\\\n    font-family: Arial, Helvetica, sans-serif;\\n\\\n    margin: 2rem;\\n\\\n}\\n\\\nbody > * {\\n\\\n    margin-bottom: 1rem;\\n\\\n}\\n\\\n.utility2FooterDiv {\\n\\\n    margin-top: 20px;\\n\\\n    text-align: center;\\n\\\n}\\n\\\n</style>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n*/\\n\\\ntextarea {\\n\\\n    font-family: monospace;\\n\\\n    height: 10rem;\\n\\\n    width: 100%;\\n\\\n}\\n\\\ntextarea[readonly] {\\n\\\n    background: #ddd;\\n\\\n}\\n\\\n</style>\\n\\\n</head>\\n\\\n<body>\\n\\\n<!-- utility2-comment\\n\\\n<div id=\"ajaxProgressDiv1\" style=\"background: #d00; height: 2px; left: 0; margin: 0; padding: 0; position: fixed; top: 0; transition: background 0.5s, width 1.5s; width: 25%;\"></div>\\n\\\nutility2-comment -->\\n\\\n<h1>\\n\\\n<!-- utility2-comment\\n\\\n    <a\\n\\\n        {{#if env.npm_package_homepage}}\\n\\\n        href=\"{{env.npm_package_homepage}}\"\\n\\\n        {{/if env.npm_package_homepage}}\\n\\\n        target=\"_blank\"\\n\\\n    >\\n\\\nutility2-comment -->\\n\\\n        {{env.npm_package_name}} (v{{env.npm_package_version}})\\n\\\n<!-- utility2-comment\\n\\\n    </a>\\n\\\nutility2-comment -->\\n\\\n</h1>\\n\\\n<h3>{{env.npm_package_description}}</h3>\\n\\\n<!-- utility2-comment\\n\\\n<h4><a download href=\"assets.app.js\">download standalone app</a></h4>\\n\\\n<button class=\"onclick onreset\" id=\"testRunButton1\">run internal test</button><br>\\n\\\n<div id=\"testReportDiv1\" style=\"display: none;\"></div>\\n\\\nutility2-comment -->\\n\\\n\\n\\\n\\n\\\n\\n\\\n<label>stderr and stdout</label>\\n\\\n<textarea class=\"resettable\" id=\"outputTextareaStdout1\" readonly></textarea>\\n\\\n<!-- utility2-comment\\n\\\n{{#if isRollup}}\\n\\\n<script src=\"assets.app.js\"></script>\\n\\\n{{#unless isRollup}}\\n\\\nutility2-comment -->\\n\\\n<script src=\"assets.utility2.rollup.js\"></script>\\n\\\n<script src=\"jsonp.utility2._stateInit?callback=window.utility2._stateInit\"></script>\\n\\\n<script src=\"assets.npmtest_limdu.rollup.js\"></script>\\n\\\n<script src=\"assets.example.js\"></script>\\n\\\n<script src=\"assets.test.js\"></script>\\n\\\n<!-- utility2-comment\\n\\\n{{/if isRollup}}\\n\\\nutility2-comment -->\\n\\\n<div class=\"utility2FooterDiv\">\\n\\\n    [ this app was created with\\n\\\n    <a href=\"https://github.com/kaizhu256/node-utility2\" target=\"_blank\">utility2</a>\\n\\\n    ]\\n\\\n</div>\\n\\\n</body>\\n\\\n</html>\\n\\\n';\n        /* jslint-ignore-end */\n        if (local.templateRender) {\n            local.assetsDict['/'] = local.templateRender(\n                local.assetsDict['/assets.index.template.html'],\n                {\n                    env: local.objectSetDefault(local.env, {\n                        npm_package_description: 'the greatest app in the world!',\n                        npm_package_name: 'my-app',\n                        npm_package_nameAlias: 'my_app',\n                        npm_package_version: '0.0.1'\n                    })\n                }\n            );\n        } else {\n            local.assetsDict['/'] = local.assetsDict['/assets.index.template.html']\n                .replace((/\\{\\{env\\.(\\w+?)\\}\\}/g), function (match0, match1) {\n                    // jslint-hack\n                    String(match0);\n                    switch (match1) {\n                    case 'npm_package_description':\n                        return 'the greatest app in the world!';\n                    case 'npm_package_name':\n                        return 'my-app';\n                    case 'npm_package_nameAlias':\n                        return 'my_app';\n                    case 'npm_package_version':\n                        return '0.0.1';\n                    }\n                });\n        }\n        // run the cli\n        if (local.global.utility2_rollup || module !== require.main) {\n            break;\n        }\n        local.assetsDict['/assets.example.js'] =\n            local.assetsDict['/assets.example.js'] ||\n            local.fs.readFileSync(__filename, 'utf8');\n        // bug-workaround - long $npm_package_buildCustomOrg\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.npmtest_limdu.rollup.js'] =\n            local.assetsDict['/assets.npmtest_limdu.rollup.js'] ||\n            local.fs.readFileSync(\n                local.npmtest_limdu.__dirname + '/lib.npmtest_limdu.js',\n                'utf8'\n            ).replace((/^#!/), '//');\n        /* jslint-ignore-end */\n        local.assetsDict['/favicon.ico'] = local.assetsDict['/favicon.ico'] || '';\n        // if $npm_config_timeout_exit exists,\n        // then exit this process after $npm_config_timeout_exit ms\n        if (Number(process.env.npm_config_timeout_exit)) {\n            setTimeout(process.exit, Number(process.env.npm_config_timeout_exit));\n        }\n        // start server\n        if (local.global.utility2_serverHttp1) {\n            break;\n        }\n        process.env.PORT = process.env.PORT || '8081';\n        console.error('server starting on port ' + process.env.PORT);\n        local.http.createServer(function (request, response) {\n            request.urlParsed = local.url.parse(request.url);\n            if (local.assetsDict[request.urlParsed.pathname] !== undefined) {\n                response.end(local.assetsDict[request.urlParsed.pathname]);\n                return;\n            }\n            response.statusCode = 404;\n            response.end();\n        }).listen(process.env.PORT);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/index.js":"module.exports = {\n\tclassifiers: require('./classifiers'),\n\tfeatures: require('./features'),\n\tformats: require('./formats'),\n\tutils: require('./utils'),\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/index.js":"module.exports = {\n\t// basic classifiers:\n\t//NeuralNetwork: require('./brain/lib/neuralnetwork').NeuralNetwork,\n\tNeuralNetwork: require('./neural/NeuralNetwork'),\n\tBayesian: require('./bayesian/bayesian'),\n\n\tkNN: require('./kNN/kNN'),\n\t\n\tSvmJs: require('./svm/SvmJs'),\n\tSvmPerf: require('./svm/SvmPerf'),\n\tSvmLinear: require('./svm/SvmLinear'),\n\t\n\t//BayesClassifier: require('./apparatus/lib/apparatus/classifier/bayes_classifier'),\n\t//LogisticRegressionClassifier: require('./apparatus/lib/apparatus/classifier/logistic_regression_classifier'),\n\tPerceptron: require('./perceptron/PerceptronHash'),\n\tWinnow: require('./winnow/WinnowHash'),\n\n\tDecisionTree: require('./decisiontree/DecisionTree'),\n\n\tmultilabel: require('./multilabel'),\n\t\n\t// meta classifier:\n\tEnhancedClassifier: require('./EnhancedClassifier'),\n}\n\n//Object.defineProperty(Function.prototype, 'where', {\n//\tvalue: function(args) {\treturn this.bind(0,args); }\n//});\n\n\n// add a \"classify and log\" method to all classifiers, for demos:\nfor (var classifierClass in module.exports) {\n\tif (module.exports[classifierClass].prototype && module.exports[classifierClass].prototype.classify)\n\t\tmodule.exports[classifierClass].prototype.classifyAndLog = function(sample) {\n\t\t\tconsole.log(sample+\" is \"+this.classify(sample));\n\t\t}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/neural/NeuralNetwork.js":"/**\n * A wrapper for Heather Arthur's brain.js package: https://github.com/harthur/brain\n *\n * @author Erel Segal-haLevi\n * @since 2013-09-29\n */\n\nvar NeuralNetwork = require('brain').NeuralNetwork;\n\nNeuralNetwork.prototype.trainOnline = function () {throw new Error(\"NeuralNetwork does not support online training\");}; \nNeuralNetwork.prototype.trainBatch  = function(dataset) {\n\tdataset.forEach(function(datum) {\n\t\tif (!Array.isArray(datum.output) && !(datum.output instanceof Object))\n\t\t\tdatum.output = [datum.output];\n\t});\n\tthis.train(dataset); \n};\nNeuralNetwork.prototype.classify  = NeuralNetwork.prototype.run; \n\nmodule.exports = NeuralNetwork;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/bayesian/bayesian.js":"var _ = require(\"underscore\")._;\n\n/**\n * A multi-class single-label Bayes classifier.\n *\n * @author Erel Segal-Halevi based on code by Heather Arthur (https://github.com/harthur/classifier)\n *\n * @param options\n */\nvar Bayesian = function(options) {\n\toptions = options || {}\n\tthis.thresholds = options.thresholds || {};\n\tthis.globalThreshold = options.globalThreshold || 1;\n\tthis.default = options.default || 'unclassified';\n\tthis.weight = options.weight || 1;\n\tthis.assumed = options.assumed || 0.5;\n\tthis.normalizeOutputProbabilities = options.normalizeOutputProbabilities||false;\n\tthis.calculateRelativeProbabilities = options.calculateRelativeProbabilities||false;\n\n\tvar backend = options.backend || { type: 'memory' };\n\tswitch(backend.type.toLowerCase()) {\n\t\tcase 'redis':\n\t\t\t//this.backend = new (require(\"./backends/redis\").RedisBackend)(backend.options);\n\t\t\tthrow new Error(\"Redis backend support was dropped, in order to remove dependencies\");\n\t\t\tbreak;\n\t\tcase 'localstorage':\n\t\t\tthis.backend = new (require(\"./backends/localStorage\")\n\t\t\t\t\t\t\t\t\t\t .LocalStorageBackend)(backend.options);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tthis.backend = new (require(\"./backends/memory\").MemoryBackend)();\n\t}\n}\n\n\nBayesian.prototype = {\n\n\t/**\n\t * Tell the classifier that the given document belongs to the given category.\n\t * @param document [string] a training sample - a feature-value hash: {feature1: value1, feature2: value2, ...}\n\t * @param category [string] the correct category of this sample.\n\t */\n\ttrainOnline: function(document, category) {\n\t\tthis.incDocCounts([{input: document, output: category}]);\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * @param data an array with objects of the format: {input: sample1, output: category1}\n\t * where sample1 is a feature-value hash: {feature1: value1, feature2: value2, ...}\n\t */\n\ttrainBatch: function(data) {\n\t\tthis.incDocCounts(data);\n\t},\n\n\t/**\n\t * Ask the classifier what category the given document belongs to.\n\t * @param document a hash {feature1: value1, feature2: value2, ...}\n\t * @return the most probable category of this sample.\n\t * If explain>0, also return the probability of each category.\n\t */\n\tclassify: function(document, explain) {\n\t\tif (!_.isObject(document)) {\n\t\t\tthrow new Error(\"document should be a feature-value hash, but it is \"+JSON.stringify(document));\n\t\t}\n\t\tvar probs = this.getProbsSync(document);\n\n\t\tvar max = this.bestMatch(probs);\n\t\tif (explain>0) {\n\t\t\treturn {\n\t\t\t\tclasses: max.category,\n\t\t\t\texplanation: probs.map(function(pair) {\n\t\t\t\t\treturn pair[0]+\": \"+pair[1]\n\t\t\t\t})\n\t\t\t};\n\t\t} else {\n\t\t\treturn max.category;\n\t\t}\n\t},\n\n\t/**\n\t * A subroutine used for classification.\n\t * Gets the probabilities of the words in the given sentence.\n\t * @param document a hash {feature1: value1, feature2: value2, ...}\n\t * Values are numeric and represent number of occurences.\n\t */\n\tgetProbsSync: function(document) {\n\t\tvar cats = this.getCats(); // a hash with the possible categories: { 'cat1': 1, 'cat2': 1 }\n\t\tvar counts = this.getWordCounts(Object.keys(document), cats); // For each word encountered during training, the counts of times it occurred in each category.\n\n\t\tvar probs = this.getCatProbs(cats, document, counts); // The probabilities that the given document belongs to each of the categories, i.e.: { 'cat1': 0.1875, 'cat2': 0.0625 }\n\n\t\tif (this.normalizeOutputProbabilities) {\n\t\t\tvar sum = _(probs).reduce(function(memo, num) { return memo + num; }, 0);\n\t\t\tfor (var cat in probs)\n\t\t\t\tprobs[cat] = probs[cat]/sum;\n\t\t}\n\n\t\tvar pairs = _.pairs(probs);   // pairs of [category,probability], for all categories that appeared in the training set.\n\t\t//console.dir(pairs);\n\t\tif (pairs.length==0) {\n\t\t\treturn {category: this.default, probability: 0};\n\t\t}\n\t\tpairs.sort(function(a,b) {   // sort by decreasing prob\n\t\t\treturn b[1]-a[1];\n\t\t});\n\n\t\treturn pairs;\n\t},\n\n\t/**\n\t * Used for classification.\n\t * @param pairs [[category,probability],...]\n\t * @return{category: most-probable-category, probability: its-probability}\n\t */\n\tbestMatch: function(pairs) {\n\t\tvar maxCategory = pairs[0][0];\n\t\tvar maxProbability = pairs[0][1];\n\n\t\tif (pairs.length>1) {\n\t\t\tvar nextProbability = pairs[1][1];\n\t\t\tvar threshold = this.thresholds[maxCategory] || this.globalThreshold;\n\t\t\tif (nextProbability * threshold > maxProbability)\n\t\t\t\tmaxCategory = this.default; // not greater than other category by enough\n\t\t\tif (this.calculateRelativeProbabilities)\n\t\t\t\tmaxProbability /= nextProbability;\n\t\t}\n\n\t\treturn {\n\t\t\tcategory: maxCategory,\n\t\t\tprobability: maxProbability\n\t\t};\n\t},\n\n\n\ttoJSON: function(callback) {\n\t\treturn this.backend.toJSON(callback);\n\t},\n\n\tfromJSON: function(json, callback) {\n\t\tthis.backend.fromJSON(json, callback);\n\t},\n\n\tgetCats: function(callback) {\n\t\t\treturn this.backend.getCats(callback);\n\t},\n\n\n\n\t/*\n\t *\n\t *\tInternal functions (should be private):\n\t *\n\t */\n\n\twordProb: function(word, cat, cats, wordCounts) {\n\t\t// times word appears in a doc in this cat / docs in this cat\n\t\tvar probWordGivenCat = (wordCounts[cat] || 0) / cats[cat];\n\n\t\tvar totalWordCount = _(cats).reduce(function(sum, p, cat) {\n\t\t\treturn sum + (wordCounts[cat] || 0);\n\t\t}, 0, this);\n\t\t// get weighted average with assumed so prob won't be extreme on rare words\n\t\tvar modifiedProbGivenCat = (this.weight * this.assumed + totalWordCount * probWordGivenCat) / (this.weight + totalWordCount);\n\n\t\t//console.log(\"word=\"+word+\" cat=\"+cat+\" probWordGivenCat=\"+probWordGivenCat+\" totalWordCount=\"+totalWordCount+\" modifiedProbGivenCat=\"+modifiedProbGivenCat)\n\t\treturn modifiedProbGivenCat\n\t},\n\n\tgetCatProbs: function(cats, document, counts) {\n\t\tvar numDocs = _(cats).reduce(function(sum, count) {\n\t\t\treturn sum + count;\n\t\t}, 0);  // total number of training samples in all categories\n\n\t\tvar probs = {};\n\t\t_(cats).each(function(catCount, cat) {\n\t\t\tvar catPriorProb = (catCount || 0) / numDocs;\n\n\t\t\t// The probability to see a document is the product\n\t\t\t//     of the probability to see each word in the document.\n\t\t\tvar docProb = _(Object.keys(document)).reduce(function(prob, word) {\n\t\t\t\tvar wordCounts = counts[word] || {};\n\t\t\t\tvar probWordGivenCat = this.wordProb(word, cat, cats, wordCounts);\n\t\t\t\tvar probWordsGivenCat = Math.pow(probWordGivenCat, document[word]);\n\t\t\t\t//console.log(\"probWordGivenCat=\"+probWordGivenCat+\" probWordsGivenCat=\"+probWordsGivenCat+\" document[word]=\"+document[word])\n\t\t\t\treturn prob * probWordsGivenCat;\n\t\t\t}, 1, this);\n\t\t\t//console.log(\"docProb=\"+docProb)\n\n\t\t\t// the probability this doc is in this category\n\t\t\tprobs[cat] = catPriorProb * docProb;\n\t\t}, this);\n\t\treturn probs;\n\t},\n\n\n\tgetWordCounts: function(words, cats, callback) {\n\t\t\treturn this.backend.getWordCounts(words, cats, callback);\n\t},\n\n\t/**\n\t * Increment the feature counts.\n\t * @param data an array with objects of the format: {input: sample1, output: class1}\n\t * where sample1 is a feature-value hash: {feature1: value1, feature2: value2, ...}\n\t */\n\tincDocCounts: function(samples, callback) {\n\t\t\t// accumulate all the pending increments\n\t\t\tvar wordIncs = {};\n\t\t\tvar catIncs = {};\n\t\t\tsamples.forEach(function(sample) {\n\t\t\t\tvar cat = sample.output;\n\t\t\t\t//if (_.isObject(cat))\n\t\t\t\t//\tcat = JSON.stringify(cat);\n\t\t\t\tcatIncs[cat] = catIncs[cat] ? catIncs[cat] + 1: 1;\n\n\t\t\t\tvar features = sample.input;\n\t\t\t\tfor (var feature in features) {\n\t\t\t\t\twordIncs[feature] = wordIncs[feature] || {};\n\t\t\t\t\twordIncs[feature][cat] = wordIncs[feature][cat] || 0;\n\t\t\t\t\twordIncs[feature][cat] += features[feature];\n\t\t\t\t}\n\t\t\t}, this);\n\n\t\t\treturn this.backend.incCounts(catIncs, wordIncs, callback);\n\t},\n\n\tsetThresholds: function(thresholds) {\n\t\t\tthis.thresholds = thresholds;\n\t},\n\n}\n\nmodule.exports = Bayesian;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/kNN/kNN.js":"var _ = require(\"underscore\")._;\nvar fs = require(\"fs\");\n\n/**\n * kNN classifier\n */\n\nvar kNN = function(opts) {\n\tthis.k = opts.k\n\tthis.mode = opts.mode\n\tthis.distanceFunctionList = opts.distanceFunctionList\n\tthis.distanceWeightening = opts.distanceWeightening\n\tthis.labels = []\n}\n\nkNN.prototype = {\n\n\ttrainOnline: function(sample, labels) {\n\t},\n\n\ttrainBatch : function(dataset) {\n\t\tthis.dataset = dataset\n\t},\n\n\tclassify: function(sample, explain) {\n\n\t\tvar trainset = _.map(this.dataset, function(value){ return {\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'input': this.complement(value['input']),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'output': value['output']\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t }, this);\n\n\t\t// is canceled due to \"okay is fine\"\n\t\t// var eq = _.filter(trainset, function(value){ return _.isEqual(value['input'], sample); });\n\t\t\n\t\t// if (eq.length != 0)\n\t\t\t// return { \n\t\t\t\t \t// 'classification': (eq[0]['output'] == 1 ? 1 : -1),\n\t\t\t\t \t// 'explanation': 'same'\n\t\t   \t\t\t// }\n\t\t\n\t\tvar distances = _.map(trainset, function(value){ return {\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'input'   : value['input'],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'output'  : value['output'],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'distance': _.reduce(this.distanceFunctionList, function(memo, df){ return memo + df(sample, value['input']); }, 0),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'score':    _.reduce(this.distanceFunctionList, function(memo, df){ return memo + df(sample, value['input']); }, 0),\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// 'distance': dfmap[this.distanceFunction](sample, value['input']),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// 'score'   : this.distanceWeightening(_.reduce(this.distanceFunctionList, function(memo, df){ return memo + df(sample, value['input']); }, 0))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t}, this);\n\n\t\tvar distances = _.sortBy(distances, function(num){ return num['distance']; })\n\n\t\t// eliminate Infinite and null\n\n\t\tvar distances = _.filter(distances, function(num){ return !isNaN(parseFloat(num['distance'])) && isFinite(num['distance']) });\n\t\t\n\t\tif (distances.length == 0)\n\t\t{\n\t\t\tif (this.mode == 'binary')\n\t\t\t\treturn {'classification': -1, 'explanation': 'not number'}\n\t\t}\n\n\t\tvar metrics = _.unique(_.sortBy(_.pluck(distances, 'distance')))\n\t\tvar margin = metrics[this.k]\n\t\tvar real_k = distances.length - _.find(distances.reverse(), function(num){ return num['distance'] == margin })\n\t\tvar knn = distances.slice(0, real_k)\n\n\t\tvar output = _.groupBy(knn, function(num){ return num['output'] })\n\n\t\tif (this.mode == 'multi')\n\t\t{\n\t\t\treturn { \n\t\t\t\t\t 'classes': Object.keys(output),\n\t\t\t\t\t 'explanation': output\n\t\t\t  \t\t}\n\t\t}\n\n\t\tvar thelabel = {'label': -1, 'score': -1}\n\n\t\t_.each(output, function(value, label, list){ \n\t\t\tvar sum = _.reduce(value, function(memo, num){ return memo + num['score']; }, 0);\n\t\t\tif (sum > thelabel['score'])\n\t\t\t\t{\n\t\t\t\t\tthelabel['score'] = sum\t\n\t\t\t\t\tthelabel['label'] = label\t\n\t\t\t\t}\n\t\t}, this)\n\n\t\t// _.each(knn, function(val, key, list){ \n\t\t\t// console.log(val)\n\t\t\t// console.log(this.translaterow(val['input']))\n\t\t// }, this)\n\n\t\tif (this.mode == 'binary')\n\t\t\treturn { \n\t\t\t\t\t 'classification': (thelabel['label'] == 1 ? thelabel['score'] : (-1) * thelabel['score']),\n\t\t\t\t\t 'explanation': this.translatetrain(knn)\n\t\t\t  \t\t}\n\t\t},\n\n\ttranslatetrain: function(input)\n\t{\n\t\tif (this.featureLookupTable)\n\t\t{\n\t\t\t_.each(input, function(value, key, list){ \n\t\t\t\tinput[key]['input'] = this.translaterow(value['input'])\n\t\t\t}, this)\n\t\t\treturn input\n\t\t}\n\t\telse\n\t\treturn input\n\n\t},\n\n\ttranslaterow: function(row)\n\t{\n\t\tvar output = {}\n\n\t\t_.each(row, function(value, key, list){ \n\t\t\tif (value != 0)\n\t\t\t\toutput[this.featureLookupTable['featureIndexToFeatureName'][key]] = value\n\t\t}, this)\n\n\t\treturn output\n\t},\n\n\tcomplement: function(input) {\n\t\tvar len = this.featureLookupTable['featureIndexToFeatureName'].length\n\t\t_(len - input.length).times(function(n){\n\t\t\tinput.push(0)\n\t\t})\n\t\treturn input\n\t},\n\n\tgetAllClasses: function() {\n\t},\n\n\tstringifyClass: function (aClass) {\n\t\treturn (_(aClass).isString()? aClass: JSON.stringify(aClass));\n\t},\n\n\ttoJSON : function() {\n\t},\n\n\tfromJSON : function(json) {\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tthis.featureLookupTable = featureLookupTable\n\t},\n}\n\n\nmodule.exports = kNN;","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmJs.js":"/**\n * A wrapper for karpathy's SVM.js package: https://github.com/karpathy/svmjs\n *\n *  This is a binary SVM and is trained using the SMO algorithm.\n *  \n *  Reference: \"The Simplified SMO Algorithm\" (http://math.unt.edu/~hsp0009/smo.pdf)\n *\n * @author Erel Segal-haLevi\n * @since 2013-09-09\n */\n\nvar SvmJsBase = require(\"svm\").SVM;\n\nfunction SvmJs(opts) {\n\tthis.base = new SvmJsBase();\n\tthis.opts = opts;  // options for SvmJsBase.train\n}\n\n\nSvmJs.prototype = {\n\ttrainOnline :function(features, label) {\n\t\tthrow new Error(\"svm.js does not support online training\");\n\t},\n\n\ttrainBatch: function(dataset) {\n\t\tvar data = [];\n\t\tvar labels = [];\n\t\tdataset.forEach(function(datum) {\n\t\t\tdata.push(datum.input);\n\t\t\tlabels.push(datum.output>0? 1: -1);\n\t\t});\n\t\treturn this.base.train(data, labels, this.opts);\n\t},\n\n\t/**\n\t * @param features - a feature-value hash.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.  \n\t * @param continuous_output if true, return the net classification score. If false [default], return 0 or 1.\n\t * @return the binary classification - 0 or 1.\n\t */\n    classify: function(features, explain, continuous_output) {\n    \tvar score = this.base.marginOne(features);\n    \tvar classification = continuous_output? score: (score>0? 1: 0);\n    \t\n    \tif (explain>0) {\n            var f = this.base.b;\n\n            // if the linear kernel was used and w was computed and stored,\n            // (i.e. the svm has fully finished training)\n            // the internal class variable usew_ will be set to true.\n            var explanations = [];\n            if(this.base.usew_) {\n              var w = this.base.w;\n              for(var j=0;j<this.base.D;j++) {\n            \texplanations[j] = {\n            \t\tfeature: j,\n            \t\tvalue: features[j],\n            \t\tweight: w[j],\n            \t\trelevance: features[j] * w[j],\n            \t};\n              }\n            } else {\n            \t// explanations not supported.\n                //for(var i=0;i<this.N;i++) {\n                // f += this.alpha[i] * this.labels[i] * this.kernel(inst, this.data[i]);\n                //}\n            }\n            explanations.sort(function(a,b){return b.relevance-a.relevance});\n            return {\n            \tclassification: classification,\n            \texplanation: explanations.slice(0, explain),\n            }\n    \t} else {\n    \t\treturn classification;\n    \t}\n    },\n    \n    toJSON: function() {\n    \treturn this.base.toJSON();\n    },\n    \n    fromJSON: function(json) {\n    \tthis.base.fromJSON(json);\n    },\n};\n\n\nmodule.exports = SvmJs;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmPerf.js":"/**\n * A wrapper for Thorsten Joachims' SVM-perf package.\n *\n * To use this wrapper, the SVM-perf executable (svm_perf_learn) should be in your path.\n *\n * You can download SVM-perf here: http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html\n * subject to the copyright license.\n *\n * @author Erel Segal-haLevi\n * @since 2013-09-02\n *\n * @param opts options: <ul>\n *\t<li>learn_args - a string with arguments for svm_perf_learn  (see http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html )\n *  <li>model_file_prefix - prefix to path to model file (optional; the default is to create a temporary file in the system temp folder).\n *  <li>bias - constant (bias) factor (default: 1).\n */\n\nvar fs   = require('fs')\n  , util  = require('util')\n  , execSync = require('child_process').execSync\n  , svmcommon = require('./svmcommon')\n  , _ = require(\"underscore\")._;\n\n\nfunction SvmPerf(opts) {\n\tif (!SvmPerf.isInstalled()) {\n\t \tvar msg = \"Cannot find the executable 'svm_perf_learn'. Please download it from the SvmPerf website, and put a link to it in your path.\";\n\t \tconsole.error(msg)\n\t \tthrow new Error(msg);\n\t}\n\tthis.learn_args = opts.learn_args || \"\";\n\tthis.learn_args += \" --b 0 \";  // we add the bias here, so we don't need SvmPerf to add it\n\tthis.model_file_prefix = opts.model_file_prefix || null;\n\tthis.bias = 'bias' in opts? opts.bias: 1.0;\n\tthis.debug = opts.debug || false;\n\tthis.ShowFeat = {}\n}\n\nSvmPerf.isInstalled = function() {\n  try {\n    var result = execSync(\"svm_perf_learn\");\n  } catch (err) {\n    return false;\n  }\n  return true\n}\n\nvar FIRST_FEATURE_NUMBER=1;  // in svm perf, feature numbers start with 1, not 0!\n\nSvmPerf.prototype = {\n\t\ttrainOnline: function(features, expected) {\n\t\t\t//throw new Error(\"SVM-perf does not support online training\");\n\t\t\tconsole.error(\"SVM-perf does not support online training\");\n\t\t},\n\n\t\t/**\n\t\t * Send the given dataset to svm_perf_learn.\n\t\t *\n\t\t * @param dataset an array of samples of the form {input: [value1, value2, ...] , output: 0/1}\n\t\t */\n\t\ttrainBatch: function(dataset) {\n\t\t\tif (this.debug) console.log(\"trainBatch start\");\n\n\t\t\tvar timestamp = new Date().getTime()+\"_\"+process.pid\n\t\t\tvar learnFile = svmcommon.writeDatasetToFile(dataset, this.bias, /*binarize=*/true, this.model_file_prefix+\"_\"+timestamp, \"SvmPerf\", FIRST_FEATURE_NUMBER);\n\t\t\tvar modelFile = learnFile.replace(/[.]learn/,\".model\");\n\t\t\tvar command = \"svm_perf_learn \"+this.learn_args+\" \"+learnFile + \" \"+modelFile;\n\t\t\tif (this.debug) console.log(\"running \"+command);\n\t\t\tconsole.log(command)\n\n\t\t\tvar result = execSync(command);\n\t\t\tif (result.code>0) {\n\t\t\t\tconsole.dir(result);\n\t\t\t\tconsole.log(fs.readFileSync(learnFile, 'utf-8'));\n\t\t\t\tthrow new Error(\"Failed to execute: \"+command);\n\t\t\t}\n\n\t\t\tthis.setModel(fs.readFileSync(modelFile, \"utf-8\"));\n\t\t\tif (this.debug) console.log(\"trainBatch end\");\n\t\t},\n\n\t\tsetModel: function(modelString) {\n\t\t\tthis.modelString = modelString;\n\t\t\tthis.mapFeatureToWeight = modelStringToModelMap(modelString);  // weights in modelMap start from 0 (- the bias).\n\t\t\tif (this.debug) console.dir(this.mapFeatureToWeight);\n\t\t\t// console.log(\"maps\"+JSON.stringify(_.keys(this.mapFeatureToWeight).length, null, 4))\n\t\t\t// process.exit(0)\n\t\t},\n\n\t\tgetFeatures: function() {\n\t\t\tvar featlist = []\n\t\t\t_.each(this.mapFeatureToWeight, function(weight, index, list){\n\t\t\t\tif (parseInt(index) == 0)\n\t\t\t\t\tfeatlist.push(['bias', weight])\n\t\t\t\telse\n\t\t\t\t\tfeatlist.push([this.featureLookupTable.numberToFeature(parseInt(index)-1), weight])\n\t\t\t}, this)\n\t\t\tfeatlist = _.sortBy(featlist, function(num){return num[1]})\n\t\t\treturn featlist\n\t\t},\n\n\t\tgetModelWeights: function() {\n\t\t\treturn this.mapFeatureToWeight;\n\t\t},\n\n\t\t/**\n\t\t * @param features - a feature-value hash.\n\t\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t\t * @param continuous_output if true, return the net classification score. If false [default], return 0 or 1.\n\t\t * @return the binary classification - 0 or 1.\n\t\t */\n\t\tclassify: function(features, explain, continuous_output) {\n\t\t\treturn svmcommon.classifyWithModelMap(\n\t\t\t\t\tthis.mapFeatureToWeight, this.bias, features, explain, continuous_output, this.featureLookupTable);\n\t\t},\n\n\t\t/**\n\t\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations.\n\t\t */\n\t\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\t\t//console.log(\"SVMPERF setFeatureLookupTable \"+featureLookupTable);\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t},\n\n\t\ttoJSON: function() {\n\t\t\treturn this.mapFeatureToWeight;\n\t\t},\n\n\t\tfromJSON: function(json) {\n\t\t\tthis.mapFeatureToWeight = json;\n\t\t},\n};\n\n\n/*\n * UTILS\n */\n\nvar SVM_PERF_MODEL_PATTERN = new RegExp(\n\t\t\"[\\\\S\\\\s]*\"+\n\t\t\"^([\\\\S\\\\s]*) # threshold b[\\\\S\\\\s]*\"+  // parse the threshold line\n\t\t\"^([\\\\S\\\\s]*) #[\\\\S\\\\s]*\" + // parse the weights line\n\t\t\"\", \"m\");\n\nvar MIN_WEIGHT = 1e-5; // weights smaller than this are ignored, to save space\n\n/**\n * A utility that converts a model in the SVMPerf format to a map of feature weights.\n * @param modelString a string.\n * @returns a map.\n */\nfunction modelStringToModelMap(modelString) {\n\tvar matches = SVM_PERF_MODEL_PATTERN.exec(modelString);\n\tif (!matches) {\n\t\tconsole.log(modelString);\n\t\tthrow new Error(\"Model does not match SVM-perf format\");\n\t};\n\t//var threshold = parseFloat(matches[1]);  // not needed - we use our own bias\n\tvar featuresAndWeights = matches[2].split(\" \");\n\tvar mapFeatureToWeight = {};\n\t//mapFeatureToWeight.threshold = threshold; // not needed - we use our own bias\n\n\t//String alphaTimesY = featuresAndWeights[0]; // always 1 in svmperf\n\tfor (var i=1; i<featuresAndWeights.length; ++i) {\n\t\tvar featureAndWeight = featuresAndWeights[i];\n\t\tvar featureWeight = featureAndWeight.split(\":\");\n\t\tif (featureWeight.length!=2)\n\t\t\tthrow new Error(\"Model featureAndWeight doesn't match svm-perf pattern: featureAndWeight=\"+featureAndWeight);\n\t\tvar feature = parseInt(featureWeight[0]);\n\t\tif (feature<=0)\n\t\t\tthrow new IllegalArgumentException(\"Non-positive feature id: featureAndWeight=\"+featureAndWeight);\n\t\tvar weight = parseFloat(featureWeight[1]);\n\t\tif (Math.abs(weight)>=MIN_WEIGHT)\n\t\t\tmapFeatureToWeight[feature-FIRST_FEATURE_NUMBER]=weight;   // start feature values from 0.\n\t\t\t// Note: if there is bias, then mapFeatureToWeight[0] is its weight.\n\t}\n\treturn mapFeatureToWeight;\n}\n\n\n\nmodule.exports = SvmPerf;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/svmcommon.js":"/**\n * Utilities common to SVM wrappers\n */\n\nvar temp = require('temp')\n  , fs   = require('fs')\n  , svmlight = require('../../formats/svmlight')\n  , _ = require('underscore')._\n\n/**\n * Writes the given dataset to a file in svm-light format.\n * @return the file name.\n */\nmodule.exports.writeDatasetToFile = function(dataset, bias, binarize, model_file_prefix, default_file_prefix, firstFeatureNumber) {\n\tif (model_file_prefix) {\n\t\tvar learnFile = model_file_prefix+\".learn\";\n\t\tvar fd = fs.openSync(learnFile, 'w');\n\t} else {\n\t\tvar tempFile = temp.openSync({prefix:default_file_prefix+\"-\", suffix:\".learn\"});\n\t\tvar learnFile = tempFile.path;\n\t\tvar fd = tempFile.fd;\n\t}\n\tvar datasetSvmlight = svmlight.toSvmLight(dataset, bias, binarize, firstFeatureNumber);\n\tfs.writeSync(fd, datasetSvmlight);\n\tfs.closeSync(fd);\n\t\n\treturn learnFile;\n}\n\n/**\n * A utility that classifies a given sample (given as a feature-value map) using a model (given as a feature-weight map).\n * @param modelMap a map {feature_i: weight_i, ....} (i >= 0; 0 is the weight of the bias, if exists).\n * @param bias if nonzero, added at the beginning of features.\n * @param features a map {feature_i: value_i, ....} (i >= 1)\n * @param explain (int) if positive, generate explanation about the classification.\n * @param continuous_output (boolean) if true, return a score; if false, return 0 or 1.\n * @param featureLookupTable if not null, used for creating meaningful explanations.\n * @returns a classification value.\n */\nmodule.exports.classifyWithModelMap = function (modelMap, bias, features, explain, continuous_output, featureLookupTable) {\n\tif (explain>0) var explanations = [];\n\tvar result = 0;\n\tif (bias && modelMap[0]) {\n\t\tvar weight = modelMap[0];\n\t\tvar relevance = bias*modelMap[0];\n\t\tresult = relevance;\n\t\tif (explain>0) explanations.push(\n\t\t\t\t{\n\t\t\t\t\tfeature: 'bias',\n\t\t\t\t\tvalue: bias,\n\t\t\t\t\tweight: weight,\n\t\t\t\t\trelevance: relevance,\n\t\t\t\t}\n\t\t);\n\t\t\n\t}\n\t\n\tfor (var feature in features) {\n\t\tvar featureInModelMap = parseInt(feature)+(bias?1:0);\n\t\tif (featureInModelMap in modelMap) {\n\t\t\tvar weight = modelMap[featureInModelMap];\n\t\t\tvar value = features[feature];\n\t\t\tvar relevance = weight*value;\n\t\t\tresult += relevance;\n\n\t\t\tif (explain>0) explanations.push(\n\t\t\t\t\t{\n\t\t\t\t\t\tfeature: featureLookupTable? (featureLookupTable.numberToFeature(feature)||\"?\"): feature,\n\t\t\t\t\t\tvalue: value,\n\t\t\t\t\t\tweight: weight,\n\t\t\t\t\t\trelevance: relevance,\n\t\t\t\t\t}\n\t\t\t);\n\t\t}\n\t}\n\t\n\tif (!continuous_output)\n\t\tresult = (result>0? 1: 0);\n\tif (_.isNaN(result)) {\n\t\tconsole.dir(explanations);\n\t\tthrow new Error(\"result is NaN when classifying \"+features+\" with \"+JSON.stringify(modelMap))\n\t}\n\tif (explain>0) {\n\t\texplanations.sort(function(a,b){return Math.abs(b.relevance)-Math.abs(a.relevance)});\n\t\tvar explanations = _.filter(explanations, function(num){ return num.relevance!=0 });\n\n\t\t// explanations.splice(explain, explanations.length-explain);  // \"explain\" is the max length of explanation.\n\n\t\t\n\t\tif (!this.detailed_explanations) {\n\t\t\t// var sprintf = require('sprintf').sprintf;\n\t\t\texplanations = explanations.map(function(e) {\n\t\t\t\t// return sprintf(\"%s%+1.2f\", e.feature, e.relevance);\n\t\t\t\treturn [e.feature, e.relevance];\n\t\t\t});\n\n\t\t\texplanations = _.sortBy(explanations, function(num){ return num[1] }).reverse()\n\n\t\t}\n\t\treturn {\n\t\t\tclassification: result,\n\t\t\texplanation: explanations\n\t\t};\n\t} else {\n\t\treturn result;\n\t}\n}\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/formats/svmlight.js":"/**\n * Small utility for writing a dataset in SVM-light format.\n *\n * @author Erel Segal-Halevi\n * @since 2013-09\n */\n\n\n/**\n * convert a single dataset to compact JSON format.\n * @param dataset an array of samples in the format {input: [value1, value2, ...], output: (0|1)}\n * @param bias if nonzero, add it at the beginning of the vector.\n * @param binarize if true, change output to -1/1. If false, leave output as it is\n */\nexports.toSvmLight = function(dataset, bias, binarize, firstFeatureNumber) {\n\tvar lines = \"\";\n\tfor (var i=0; i<dataset.length; ++i) {\n\t\tvar line = (i>0? \"\\n\": \"\") + \n\t\t\t(binarize? (dataset[i].output>0? \"1\": \"-1\"): dataset[i].output) +  // in svm-light, the output comes first:\n\t\t\tfeatureArrayToFeatureString(dataset[i].input, bias, firstFeatureNumber)\n\t\t\t;\n\t\tlines += line;\n\t};\n\tlines += \"\\n\";\n\treturn lines;\n}\n\n\n\n/**\n * convert an array of features to a single line in SVM-light format. The line starts with a space.\n */\nfunction featureArrayToFeatureString(features, bias, firstFeatureNumber) {\n\tif (!Array.isArray(features))\n\t\tthrow new Error(\"Expected an array, but got \"+JSON.stringify(features))\n\tvar line = (bias? \" \"+firstFeatureNumber+\":\"+bias: \"\");\n\tfor (var feature=0; feature<features.length; ++feature) {\n\t\tvar value = features[feature];\n\t\tif (value)\n\t\t\tline += (\" \"+(feature+firstFeatureNumber+(bias?1:0))+\":\"+value.toPrecision(5));\n\t}\n\treturn line;\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmLinear.js":"/**\n * A wrapper for the LibLinear package, by Fan, Chang, Hsieh, Wang and Lin.\n *\n * To use this wrapper, the LibLinear executable (liblinear_train) should be in your path.\n *\n * You can download LibLinear here: http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n * subject to the copyright license.\n *\n * @author Erel Segal-haLevi\n * @since 2013-09-09\n *\n * @param opts options: <ul>\n *\t<li>learn_args - a string with arguments for liblinear_train\n *  <li>model_file_prefix - prefix to path to model file (optional; the default is to create a temporary file in the system temp folder).\n *  <li>bias - constant (bias) factor (default: 1).\n *  <li>multiclass - if true, the 'classify' function returns an array [label,score]. If false (default), it returns only a score.\n */\n \n var util  = require('util')\n   , child_process = require('child_process')\n   , exec = require('child_process').exec\n   , fs   = require('fs')\n   , svmcommon = require('./svmcommon')\n   , _ = require('underscore')._\n\n var FIRST_FEATURE_NUMBER=1;  // in lib linear, feature numbers start with 1\n\n\nfunction SvmLinear(opts) {\n\tthis.learn_args = opts.learn_args || \"\";\n\tthis.model_file_prefix = opts.model_file_prefix || null;\n\tthis.bias = opts.bias || 1.0;\n\tthis.multiclass = opts.multiclass || false;\n\tthis.debug = opts.debug||false;\n\tthis.train_command = opts.train_command || 'liblinear_train';\n\tthis.test_command = opts.test_command || 'liblinear_test';\n\tthis.timestamp = \"\"\n\n\tif (!SvmLinear.isInstalled()) {\n                var msg = \"Cannot find the executable 'liblinear_train'. Please download it from the LibLinear website, and put a link to it in your path.\";\n                console.error(msg)\n                throw new Error(msg);\n  }\n}\n\nSvmLinear.isInstalled = function() {\n\ttry {\n\t    var result = child_process.execSync('liblinear_train .');\n\t} catch (err) {\n\t    return false\n\t}\n\treturn true\n};\n\nSvmLinear.prototype = {\n\t\ttrainOnline: function(features, expected) {\n\t\t\tthrow new Error(\"LibLinear does not support online training\");\n\t\t},\n\n\t\t/**\n\t\t * Send the given dataset to liblinear_train.\n\t\t *\n\t\t * @param dataset an array of samples of the form {input: [value1, value2, ...] , output: 0/1}\n\t\t */\n\t\ttrainBatch: function(dataset) {\n\t\t\tthis.timestamp = new Date().getTime()+\"_\"+process.pid\n\n\t\t\t// check for multilabel\n\t\t\t_.each(dataset, function(datum, key, list){\n\t\t\t\tif (_.isArray(datum.output))\n\t\t\t\t\tif (datum.output.length > 1)\n\t\t\t\t\t{\n\t\t\t\t\t\tconsole.log(\"Multi-label is not allowed\")\n\t\t\t\t\t\tconsole.log(JSON.stringify(darum.output, null, 4))\n\t\t\t\t\t\tprocess.exit(0)\n\t\t\t\t\t}\n            }, this)\n\n            //  convert all arraay-like outputs to just values\n\t\t\tdataset = _.map(dataset, function(datum){\n\t\t\t\tif (_.isArray(datum.output))\n\t\t\t\t\tdatum.output = datum.output[0]\n\t\t\t\treturn datum });\n\n\t\t\tthis.allLabels = _(dataset).map(function(datum){return datum.output});\n\t\t\tthis.allLabels = _.uniq(_.flatten(this.allLabels))\n\n\t\t\t// dataset = _.map(dataset, function(datum){\n\t\t\t// \tdatum.output = this.allLabels.indexOf(datum.output)\n\t\t\t// \treturn datum });\n\n\t\t\tif (this.allLabels.length==1) // a single label\n\t\t\t\treturn;\n\t\t\t//console.log(util.inspect(dataset,{depth:1}));\n\t\t\tif (this.debug) console.log(\"trainBatch start\");\n\t\t\tvar learnFile = svmcommon.writeDatasetToFile(\n\t\t\t\t\tdataset, this.bias, /*binarize=*/false, this.model_file_prefix+\"_\"+this.timestamp, \"SvmLinear\", FIRST_FEATURE_NUMBER);\n\t\t\tvar modelFile = learnFile.replace(/[.]learn/,\".model\");\n\n\t\t\tvar command = this.train_command+\" \"+this.learn_args+\" \"+learnFile + \" \"+modelFile;\n\t\t\tconsole.log(\"running \"+command);\n\n\t\t\tvar result = child_process.execSync(command);\n\t\t\tif (result.code>0) {\n\t\t\t\tconsole.dir(result);\n\t\t\t\tconsole.log(fs.readFileSync(learnFile, 'utf-8'));\n\t\t\t\tthrow new Error(\"Failed to execute: \"+command);\n\t\t\t}\n\n\t\t\tthis.modelFileString = modelFile;\n\n\t\t\tif (this.debug) console.log(\"trainBatch end\");\n\t\t},\n\n\t\tsetModel: function(modelFileString) {\n\t\t\t// this.modelFileString = modelFileString;\n\t\t\tthis.modelString = fs.readFileSync(modelFileString, \"utf-8\")\n\t\t\tthis.mapLabelToMapFeatureToWeight = modelStringToModelMap(this.modelString);\n\t\t\tthis.allLabels = Object.keys(this.mapLabelToMapFeatureToWeight);\n\t\t\tif (this.debug) console.dir(this.mapLabelToMapFeatureToWeight);\n\t\t},\n\n\t\tgetModelWeights: function() {\n\t\t\tif (!this.mapLabelToMapFeatureToWeight)\n                                this.setModel(this.modelFileString)\n\t\t\treturn (this.multiclass? this.mapLabelToMapFeatureToWeight: this.mapLabelToMapFeatureToWeight[1]);\n\t\t},\n\n\t\t/**\n\t\t * @param features - a feature-value hash.\n\t\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t\t * @param continuous_output if true, return the net classification score. If false [default], return 0 or 1.\n\t\t * @return the binary classification - 0 or 1.\n\t\t */\n\n\t\tclassifyBatch: function(trainset) {\n\n\t\t\t// console.log(JSON.stringify(this.modelFileString, null, 4))\n\t\t\t_.each(trainset, function(value, key, list){\n\t\t\t\ttrainset[key].output = 0\n\t\t\t}, this)\n\n\t\t\tvar testFile = svmcommon.writeDatasetToFile(\n                                        trainset, this.bias, /*binarize=*/false, \"/tmp/test_\"+this.timestamp, \"SvmLinear\", FIRST_FEATURE_NUMBER);\n\n\t\t\tvar command = this.test_command+\" \"+testFile + \" \" + this.modelFileString + \" /tmp/out_\" + this.timestamp;\n\n\t\t\tvar output = child_process.execSync(command)\n\t\t\tconsole.log(command)\n\n\t\t\tvar result = fs.readFileSync(\"/tmp/out_\" + this.timestamp, \"utf-8\").split(\"\\n\")\n\n\t\t\treturn result\n\t\t},\n\n\t\tclassify: function(features, explain, continuous_output) {\n\n\t\t\tif (!this.mapLabelToMapFeatureToWeight)\n\t\t\t\tthis.setModel(this.modelFileString)\n\n\t\t\tif (this.allLabels.length==1) {  // a single label\n\t\t\t\tvar result = (\n\t\t\t\t\t\t!continuous_output?   this.allLabels[0]:\n\t\t\t\t\t\t\t!this.multiclass? 1.0:\n\t\t\t\t\t\t\t                  [[this.allLabels[0], 1.0]]);\n\t\t\t\treturn (explain>0?\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tclasses: result,\n\t\t\t\t\t\t\texplanation: [\"Single label (\"+result+\") - no classification needed\"],\n\t\t\t\t\t\t}:\n\t\t\t\t\t\tresult);\n\t\t\t}\n\t\t\tvar labels = [];\n\t\t\tif (explain>0) var explanations = [];\n\t\t\tfor (var label in this.mapLabelToMapFeatureToWeight) {\n\t\t\t\tvar mapFeatureToWeight = this.mapLabelToMapFeatureToWeight[label];\n\n\t\t\t\tvar scoreWithExplain = svmcommon.classifyWithModelMap(\n\t\t\t\t\tmapFeatureToWeight, this.bias, features, explain, /*continuous_output=*/true, this.featureLookupTable);\n\n\t\t\t\tvar score = (explain>0? scoreWithExplain.classification: scoreWithExplain);\n\n\t\t\t\tvar labelAndScore = [parseInt(label), score];\n\t\t\t\tif (scoreWithExplain.explanation && explain>0) {\n\t\t\t\t\tlabelAndScore.push(this.multiclass?\n\t\t\t\t\t\tscoreWithExplain.explanation.join(\" \"):\n\t\t\t\t\t\tscoreWithExplain.explanation)\n\t\t\t\t}\n\n\t\t\t\tlabels.push(labelAndScore);\n\t\t\t}\n\n\t\t\tlabels.sort(function(a,b) {return b[1]-a[1]}); // sort by decreasing score\n\n\t\t\tif (explain>0) {\n\t\t\t\tif (this.multiclass) {\n\t\t\t\t\tvar explanations = [];\n\t\t\t\t\tlabels.forEach(function(datum) {\n\t\t\t\t\t\texplanations.push(datum[0]+\": score=\"+JSON.stringify(datum[1])+\" features=\"+datum[2]);\n\t\t\t\t\t\tdatum.pop();\n\t\t\t\t\t});\n\t\t\t\t} else {\n\t\t\t\t\tvar explanations = (labels[0][0]>0? labels[0][2]: labels[1][2])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar result = (\n\t\t\t\t!continuous_output?   labels[0][0]:\n\t\t\t\t\t!this.multiclass? (labels[0][0]>0? labels[0][1]: labels[1][1]):\n\t\t\t\t\t                  labels);\n\t\t\treturn (explain>0?\n\t\t\t\t{\n\t\t\t\t\tclasses: result,\n\t\t\t\t\tclassification: result,\n\t\t\t\t\texplanation: explanations,\n\t\t\t\t}:\n\t\t\t\tresult);\n\t\t},\n\n\t\t/**\n\t\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations.\n\t\t */\n\t\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t},\n\n\t\ttoJSON: function() {\n\t\t\treturn this.mapFeatureToWeight;\n\t\t},\n\n\t\tfromJSON: function(json) {\n\t\t\tthis.mapFeatureToWeight = json;\n\t\t},\n};\n\n\n/*\n * UTILS\n */\n\nvar NEWLINE = require('os').EOL;\n\nvar LIB_LINEAR_MODEL_PATTERN = new RegExp(\n\t\t\"[\\\\S\\\\s]*\"+    // skip the beginning of string\n\t\t\"^label (.*)\"+NEWLINE+  // parse the label-list line\n\t\t\"^nr_feature .*\"+NEWLINE+  // parse the feature-count line (not used)\n\t\t\"^bias (.*)\"+NEWLINE+  // parse the bias line (not used - we use our own bias)\n\t\t\"^w\"+NEWLINE+                // start of weight matrix\n\t\t\"([\\\\S\\\\s]*)\" + // parse the weights\n\t\t\"\", \"m\");\n\nvar MIN_WEIGHT = 1e-5; // weights smaller than this are ignored, to save space\n\n/**\n * A utility that converts a model in the SvmLinear format to a matrix of feature weights per label.\n * @param modelString a string.\n * @returns mapLabelToMapFeatureToWeight.\n */\nfunction modelStringToModelMap(modelString) {\n\tvar matches = LIB_LINEAR_MODEL_PATTERN.exec(modelString);\n\tif (!matches) {\n\t\tconsole.log(modelString);\n\t\tthrow new Error(\"Model does not match SVM-Linear format\");\n\t};\n\tvar labels = matches[1].split(/\\s+/);\n\tvar mapLabelToMapFeatureToWeight = {};\n\tfor (var iLabel in labels) {\n\t\tvar label = labels[iLabel];\n\t\tmapLabelToMapFeatureToWeight[label]={};\n\t}\n\n\tvar weightsMatrix = matches[3];\n\t// each line represents a feature; each column represents a label:\n\n\tvar weightsLines = weightsMatrix.split(NEWLINE);\n\tfor (var feature in weightsLines) {\n\t\tvar weights = weightsLines[feature].split(/\\s+/);\n\t\tweights.pop(); // ignore lal]st weight, which is empty (-space)\n\t\tif (weights.length==0)\n\t\t\tcontinue; // ignore empty lines\n//\t\tif (isNaN(parseFloat(weights[weights.length-1])))\n//\t\t\tweights.pop();\n\t\tif (weights.length==1 && labels.length==2)\n\t\t\tweights[1] = -weights[0];\n\t\tif (weights.length!=labels.length)\n\t\t\tthrow new Error(\"Model does not match SVM-Linear format: there are \"+labels.length+\" labels (\"+labels+\") and \"+weights.length+\" weights (\"+weights+\")\");\n\t\tfor (var iLabel in labels) {\n\t\t\tvar label = labels[iLabel];\n\t\t\tvar weight = parseFloat(weights[iLabel]);\n\t\t\tif (Math.abs(weight)>=MIN_WEIGHT)\n\t\t\t\tmapLabelToMapFeatureToWeight[label][feature]=weight;\n\t\t}\n\t}\n\n\treturn mapLabelToMapFeatureToWeight;\n}\n\n\n\nmodule.exports = SvmLinear;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/perceptron/PerceptronHash.js":"/**\n * A simple Perceptron implementation.\n *\n * Based on code by John Chesley:  https://github.com/chesles/perceptron\n *\n * the weights vector is a hash (not a numeric array), so the features can be any objects (not just nubmers).\n *\n * @author Erel Segal-haLevi\n * @since 2013-05-27\n * \n * @param opts optional parameters: <ul>\n *\t<li>debug \n *  <li>default_weight: default weight for a newly discovered feature (default = 0).\n *  <li>do_averaging: boolean, see http://ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf . But count only weight vectors with successful predictions (Carvalho and Cohen, 2006).\n *  <li>do_normalization: boolean, normalize sum of features to 1.\n *\t<li>learning_rate: defaults to 0.1.\n *  <li>retrain_count: in batch training mode, how many times to retrain. 0=no retrain. Default=1.\n * \n */\n\nvar hash = require(\"../../utils/hash\");\n\nvar PerceptronHash = function(opts) {\n\tif (!opts) opts = {};\n\t\n\tthis.debug = opts.debug||false;\n\tthis.default_weight = opts.default_weight||0;\n\tthis.do_averaging = opts.do_averaging||false;\n\tthis.do_normalization = opts.do_normalization||false;\n\tthis.learning_rate = opts.learning_rate||0.1;\n\tthis.retrain_count = 'retrain_count' in opts? opts.retrain_count: 1;\n\t\n\tthis.weights = {};\n\tif (this.do_averaging) this.weights_sum = {};   // for averaging; see http://ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf . But count only weight vectors with successful predictions (Carvalho and Cohen, 2006).\n}\n\n\nPerceptronHash.prototype = {\n\n\ttoJSON: function() {\n\t\treturn {\n\t\t\tweights: this.weights,\n\t\t\tweights_sum: this.weights_sum\n\t\t}\n\t},\n\n\tfromJSON: function(json) {\n\t\tthis.weights = json.weights;\n\t\tthis.weights_sum = json.weights_sum;\n\t},\n\n\tnormalized_features: function (features, remove_unknown_features) {\n\t\tfeatures['bias'] = 1;\n\t\tif (remove_unknown_features) {\n\t\t\tfor (var feature in features)\n\t\t\t\tif (!(feature in this.weights))\n\t\t\t\t\tdelete features[feature];\n\t\t} \n\t\tif (this.do_normalization) \n\t\t\thash.normalize_sum_of_values_to_1(features);\n\t\treturn features;\n\t},\n\n\t/**\n\t * @param features a SINGLE training sample; a hash (feature => value).\n\t * @param expected the classification value for that sample (0 or 1)\n\t * @return true if the input sample got its correct classification (i.e. no change made).\n\t */\n\ttrain_features: function(features, expected) {\n\t\tfor (feature in features) \n\t\t\tif (!(feature in this.weights)) \n\t\t\t\tthis.weights[feature] = this.default_weight;\n\n\t\tvar result = this.perceive_features(features, /*net=*/false, this.weights); // always use the running 'weights' vector for training, and NOT the weights_sum!\n\n\t\tif (this.debug) console.log('> training ',features,', expecting: ',expected, ' got: ', result);\n\n\t\tif (result != expected) {\n\t\t\t// Current model is incorrect - adjustment needed!\n\t\t\tif (this.debug) console.log('> adjusting weights...', this.weights, features);\n\t\t\tfor (var feature in features) \n\t\t\t\tthis.adjust(result, expected, features[feature], feature);\n\t\t\tif (this.debug) console.log(' -> weights:', this.weights)\n\t\t} else {\n\t\t\tif (this.do_averaging) hash.add(this.weights_sum, this.weights);\n\t\t}\n\t\t\n\t\treturn (result == expected);\n\t},\n\n\t/**\n\t * Online training (a single sample).\n\t *\n\t * @param features a SINGLE training sample; a hash (feature => value).\n\t * @param expected the classification value for that sample (0 or 1)\n\t * @return true if the input sample got its correct classification (i.e. no change made).\n\t */\n\ttrainOnline: function(features, expected) {\n\t\treturn this.train_features(\n\t\t\tthis.normalized_features(features, /*remove_unknown_features=*/false), expected);\n\t},\n\t\n\n\t/**\n\t * Batch training (a set of samples). Uses the option this.retrain_count.\n\t *\n\t * @param dataset an array of samples of the form {input: {feature1: value1...} , output: 0/1} \n\t */\n\ttrainBatch: function(dataset) {\n\t\tvar normalized_features = [];\n\t\tfor (var i=0; i<dataset.length; ++i)\n\t\t\tnormalized_features[i] = this.normalized_features(dataset[i].input, /*remove_unknown_features=*/false);\n\n\t\tfor (var r=0; r<=this.retrain_count; ++r)\n\t\t\tfor (var i=0; i<normalized_features.length; ++i) \n\t\t\t\tthis.train_features(normalized_features[i], dataset[i].output);\n\t},\n\n\n\tadjust: function(result, expected, input, feature) {\n\t\tvar delta = (expected - result) * this.learning_rate * input;\n\t\tif (isNaN(delta)) throw new Error('delta is NaN!! result='+result+\" expected=\"+expected+\" input=\"+input+\" feature=\"+feature);\n\t\tthis.weights[feature] += delta;\n\t\tif (isNaN(this.weights[feature])) throw new Error('weights['+feature+'] went to NaN!! delta='+d);\n\t},\n\t\t\n\n\t/**\n\t * @param features a SINGLE sample; a hash (feature => value).\n\t * @param weights_for_classification the weights vector to use (either the running 'weights' or 'weights_sum').  \n\t * @param continuous_output if true, return the net classification value. If false [default], return 0 or 1.\n\t * @return the classification of the sample.\n\t */\n\tperceive_features: function(features, continuous_output, weights_for_classification, explain) {\n\t\tvar score = hash.inner_product(features, weights_for_classification);\n\t\tif (this.debug) console.log(\"> perceive_features \",features,\" = \",score);\n\t\tvar result = (continuous_output? score: (score > 0 ? 1 : 0));\n\t\tif (explain) {\n\t\t\tresult = {\n\t\t\t\tclassification: result,\n\t\t\t\texplanation: [\"Perceptron does not support explanations yet\"],\n\t\t\t\tnet_score: score, \n\t\t\t}\n\t\t}\n\t\treturn result;\n\t},\n\n\t/**\n\t * @param features a SINGLE sample, as a hash {feature:value}\n\t * @param explain (int) if positive, add an explanation (currently ignored).\n\t * @param continuous_output if true, return the net classification score. If false [default], return 0 or 1.\n\t * @return the classification of the sample.\n\t */\n\tclassify: function(features, explain, continuous_output) {\n\t\treturn this.perceive_features(\n\t\t\tthis.normalized_features(features, /*remove_unknown_features=*/true), \n\t\t\tcontinuous_output,\n\t\t\t(this.do_averaging? this.weights_sum: this.weights),\n\t\t\texplain);\n\t},\n}\n\n\nmodule.exports = PerceptronHash;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/hash.js":"/**\n * Static utilities for hashes (= associative arrays = Javascript objects).\n * \n * @author Erel Segal-Halevi\n * @since 2013-06\n * @note see performance tests of adding hashes versus arrays here: http://jsperf.com/adding-sparse-feature-vectors\n */\n\n\n/**\n * create a hash from a string in the format:\n \n * key1 / value1\n * key2 / value2\n * ...\n *\n * Comments start with '#' and end with end-of-line.\n */\nexports.fromString = function(string) {\n\tvar lines = string.split(/[\\n\\r]/g);\n\tvar hash = {};\n\tfor (var i=0; i<lines.length; ++i) {\n\t\tvar line = lines[i].trim();\n\t\tline = line.replace(/\\s*#.*?$/, \"\");  // remove comments\n\t\tif (line.length<1) continue; // skip empty lines\n\n\t\tvar parts = line.split(/\\s*\\/\\s*/);\n\t\tif (parts.length<2 || !parts[0] || !parts[1]) {\n\t\t\tconsole.dir(parts);\n\t\t\tthrow new Error(\"empty key or value\");\n\t\t}\n\t\tvar key = parts[0];\n\t\tvar value = parts[1];\n\t\tif (key in hash) {\n\t\t\tconsole.warn(\"key \"+key+\" already exists. Old value=\"+hash[key]+\", new value=\"+value);\n\t\t}\n\t\thash[key]=value;\n\t}\n\treturn hash;\n};\n\n/**\n * add one hash to another (target += source)\n * @param target [input and output]\n * @param source [input]: will be added to target.\n */\nexports.add  = function(target, source) {\n\tfor (var feature in source) {\n\t\tif (!(feature in target))\n\t\t\ttarget[feature]=0;\n\t\tif (toString.call(target[feature]) != '[object Number]') continue;\n\t\ttarget[feature] += source[feature];\n\t}\n\treturn target;\n};\n \n/**\n * add one hash to another (target += scalar * source)\n * @param target [input and output]\n * @param source [input]\n * @param scalar [input]\n */\nexports.addtimes  = function(target, scalar, source) {\n\tfor (var feature in source) {\n\t\tif (!(feature in target))\n\t\t\ttarget[feature]=0;\n\t\tif (toString.call(target[feature]) != '[object Number]') continue;\n\t\ttarget[feature] += scalar*source[feature];\n\t}\n\treturn target;\n};\n\n/**\n * multiply one hash by another (elementwise multiplication).\n * @param target [input and output]\n * @param source [input]: target will be multiplied by it.\n */\nexports.multiply  = function(target, source) {\n\tfor (var feature in source) {\n\t\tif (!(feature in target))\n\t\t\ttarget[feature]=1;\n\t\tif (toString.call(target[feature]) != '[object Number]') continue;\n\t\ttarget[feature] *= source[feature];\n\t}\n\treturn target;\n};\n\n/**\n * multiply a hash by a scalar.\n * @param target [input and output]\n * @param source [input]: target will be multiplied by it.\n */\nexports.multiply_scalar  = function(target, source) {\n\tfor (var feature in target) {\n\t\tif (toString.call(target[feature]) != '[object Number]') continue;\n\t\ttarget[feature] *= source;\n\t}\n\treturn target;\n};\n\n/**\n * calculate the scalar product (dot product) of the given two hashes.\n * @param features [input] - a hash representing a 1-dimensional vector.\n * @param weights [input] - a hash representing a 1-dimensional vector.\n * @return a scalar - the sum of elementwise products.\n * @note Usually, there are much less features than weights.\n */\nexports.inner_product = function(features, weights) {\n\tvar result = 0;\n\tfor (var feature in features) {\n\t\t\tif (feature in weights) {\n\t\t\t\tresult += features[feature] * weights[feature];\n\t\t\t} else {\n\t\t\t\t\t/* the sample contains a feature that was never seen in training - ignore it for now */ \n\t\t\t}\n\t}\n\treturn result;\n};\n\n/**\n * calculate the vector dot product of the given two hashes.\n * @param features [input] - a hash representing a 1-dimensional vector.\n * @param weights [input] - a hash of hashes, representing a 2-dimensional matrix.\n * @return a hash - for each key of weights, return the dot product of the given row with the features vector.\n * @note Usually, there are much less features than weights.\n */\nexports.inner_product_matrix = function(features, weights) {\n\tvar result = {};\n\tfor (category in weights) {\n\t\tresult[category] = exports.inner_product(features, weights[category]);\n\t}\n\treturn result;\n};\n\nexports.sum_of_values = function(weights) {\n\tvar result = 0;\n\tfor (var feature in weights)\n\t\tresult += weights[feature];\n\treturn result;\n};\n\nexports.sum_of_absolute_values = function(weights) {\n\tvar result = 0;\n\tfor (var feature in weights)\n\t\tresult += Math.abs(weights[feature]);\n\treturn result;\n};\n\nexports.sum_of_square_values = function(weights) {\n\tvar result = 0;\n\tfor (var feature in weights)\n\t\tresult += Math.pow(weights[feature],2);\n\treturn result;\n};\n\n/**\n * Normalize the given hash, such that the sum of values is 1.\n * Unless, of course, the current sum is 0, in which case, nothing is done. \n */\nexports.normalize_sum_of_values_to_1 = function(features) {\n\tvar sum = exports.sum_of_absolute_values(features);\n\tif (sum!=0)\n\t\texports.multiply_scalar(features, 1/sum);\n};\n\n/**\n * Normalize the given hash, such that the sum of squares of the values is 1.\n * Unless, of course, the current sum is 0, in which case, nothing is done. \n */\nexports.normalize_sum_of_squares_to_1 = function(features) {\n\tvar sum = exports.sum_of_square_values(features);\n\tif (sum!=0)\n\t\texports.multiply_scalar(features, 1/Math.sqrt(sum));\n};\n\n\n/**\n * @param array [input]\n * @return a string of the given hash, sorted by keys.\n */\nexports.stringify_sorted = function(weights, separator) {\n\tvar result = \"{\" + separator;\n\tvar keys = Object.keys(weights);\n\tkeys.sort();\n\tvar last = keys.length-1;\n\tfor (var i=0; i <= last; i++) {\n\t\tvar key = keys[i];\n\t\tvar weight = weights[key]; \n\t\tresult += '\"'+key+'\": '+weight;\n\t\tif (i<last) result+=\",\";\n\t\tresult += separator;\n\t}\n\tresult += \"}\";\n\treturn result;\t\n};\n\n\n/**\n * Convert any object to a hash (representing a set):\n *\n * - an array ['a', 'b', 'c'..] to a hash {'a': true, 'b': true, 'c': true};\n * - a string 'a' to a hash {'a': true}.\n */\nexports.normalized = function(object) {\n\tif (Array.isArray(object)) {\n\t\tvar result = {}; \n\t\tfor (var i=0; i<object.length; ++i) \n\t\t\tresult[stringifyIfNeeded(object[i])]=true;\n\t\treturn result;\n\t} else if (object instanceof Object) {\n\t\treturn object;\n\t} else {\n\t\tvar result = {};\n\t\tresult[stringifyIfNeeded(object)]=true; \n\t\treturn result;\n\t}\n};\n\n\nvar stringifyIfNeeded = function (label) {\n\treturn (typeof(label)==='string'? label: JSON.stringify(label));\n};\n\n/*\nvar toStringOrStringArray = function (classes) {\n\tif (classes instanceof Array)\n\t\tclasses = classes.map(stringifyIfNeeded);\n\telse \n\t\tclasses = stringifyIfNeeded(classes);\n\treturn hash.normalized(classes);\n}\n*/\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/winnow/WinnowHash.js":"/**\n * A version of Modified Balanced Winnow (Carvalho and Cohen, 2006)\n *    where the weights vector is a hash (not a numeric array), \n *    so the features can be any objects (not just nubmers).\n * @author Erel Segal-haLevi\n * @since 2013-06-03\n * \n * @param opts optional parameters: <ul>\n *\t<li>debug \n *  <li>default_positive_weight, default_negative_weight: default weight for a newly discovered feature (default = 2, 1).\n *  <li>promotion, demotion, threshold, margin - explained in the paper.\n *  <li>retrain_count - number of times to retrain in batch mode. Default = 0 (no retrain).\n *  <li>bias - constant (bias) factor (default: 1).\n */\n \nvar hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;  // for explanations\n\nfunction WinnowHash(opts) {\n\tif (!opts) opts = {}\n\n\tthis.debug = opts.debug || false; \n\t\t\n\t// Default values are based on Carvalho and Cohen, 2006, section 4.2:\t\n\tthis.default_positive_weight = opts.default_positive_weight || 2.0;\n\tthis.default_negative_weight = opts.default_negative_weight || 1.0;\n\tthis.do_averaging = opts.do_averaging || false;\n\tthis.threshold = ('threshold' in opts? opts.threshold: 1);\n\tthis.promotion = opts.promotion || 1.5;\n\tthis.demotion = opts.demotion || 0.5;\n\tthis.margin = ('margin' in opts? opts.margin: 1.0);\n\tthis.retrain_count = opts.retrain_count || 0;\n\tthis.detailed_explanations = opts.detailed_explanations || false;\n\t\n\tthis.bias = ('bias' in opts? opts.bias: 1.0);\n\n\tthis.positive_weights = {};\n\tthis.negative_weights = {};\n\tthis.positive_weights_sum = {};   // for averaging; count only weight vectors with successful predictions (Carvalho and Cohen, 2006).\n\tthis.negative_weights_sum = {};   // for averaging; count only weight vectors with successful predictions (Carvalho and Cohen, 2006).\n}\n\n\nWinnowHash.prototype = {\n\t\t\n\t\ttoJSON: function(folder) {\n\t\t\treturn {\n\t\t\t\tpositive_weights: this.positive_weights,\n\t\t\t\tnegative_weights: this.negative_weights,\n\t\t\t\tpositive_weights_sum: this.positive_weights_sum,\n\t\t\t\tnegative_weights_sum: this.negative_weights_sum,\n\t\t\t}\n\t\t},\n\n\t\tfromJSON: function(json) {\n\t\t\tif (!json.positive_weights) throw new Error(\"No positive weights in json: \"+JSON.stringify(json));\n\t\t\tthis.positive_weights = json.positive_weights;\n\t\t\tthis.positive_weights_sum = json.positive_weights_sum;\n\t\t\tif (!json.negative_weights) throw new Error(\"No negative weights in json: \"+JSON.stringify(json));\n\t\t\tthis.negative_weights = json.negative_weights;\n\t\t\tthis.negative_weights_sum = json.negative_weights_sum;\n\t\t},\n\t\t\n\t\teditFeatureValues: function (features, remove_unknown_features) {\n\t\t\tif (this.bias && !('bias' in features))\n\t\t\t\tfeatures['bias'] = 1;\n\t\t\tif (remove_unknown_features) {\n\t\t\t\tfor (var feature in features)\n\t\t\t\t\tif (!(feature in this.positive_weights))\n\t\t\t\t\t\tdelete features[feature];\n\t\t\t}\n\t\t\thash.normalize_sum_of_values_to_1(features);\n\t\t},\n\n\t\t/**\n\t\t * @param inputs a SINGLE training sample; a hash (feature => value).\n\t\t * @param expected the classification value for that sample (0 or 1)\n\t\t * @return true if the input sample got its correct classification (i.e. no change made).\n\t\t */\n\t\ttrain_features: function(features, expected) {\n\t\t\tif (this.debug) console.log(\"train_features \"+JSON.stringify(features)+\" , \"+expected);\n\t\t\tfor (feature in features) {\n\t\t\t\tif (!(feature in this.positive_weights)) \n\t\t\t\t\tthis.positive_weights[feature] = this.default_positive_weight;\n\t\t\t\tif (!(feature in this.negative_weights)) \n\t\t\t\t\tthis.negative_weights[feature] = this.default_negative_weight;\n\t\t\t}\n\n\t\t\tif (this.debug) console.log('> this.positive_weights  ',JSON.stringify(this.positive_weights),', this.negative_weights: ',JSON.stringify(this.negative_weights));\n\n\t\t\tvar score = this.perceive_features(features, /*continuous_output=*/true, this.positive_weights, this.negative_weights);\n\t\t\t\t // always use the running 'weights' vector for training, and NOT the weights_sum!\n\n\t\t\t//if (this.debug) console.log('> training ',features,', expecting: ',expected, ' got score=', score);\n\t\t\t\n\t\t\tif ((expected && score<=this.margin) || (!expected && score>=-this.margin)) {\n\t\t\t\t// Current model is incorrect - adjustment needed!\n\t\t\t\tif (expected) {\n\t\t\t\t\tfor (var feature in features) {\n\t\t\t\t\t\tvar value = features[feature]; \n\t\t\t\t\t\tthis.positive_weights[feature] *= (this.promotion * (1 + value));\n\t\t\t\t\t\tthis.negative_weights[feature] *= (this.demotion * (1 - value));\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfor (var feature in features) { \n\t\t\t\t\t\tvar value = features[feature]; \n\t\t\t\t\t\tthis.positive_weights[feature] *= (this.demotion * (1 - value));\n\t\t\t\t\t\tthis.negative_weights[feature] *= (this.promotion * (1 + value));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (this.debug) console.log('--> this.positive_weights',JSON.stringify(this.positive_weights),', this.negative_weights: ',JSON.stringify(this.negative_weights));\n\t\t\t\treturn false;\n\t\t\t} else {\n\t\t\t\tif (this.do_averaging) {\n\t\t\t\t\thash.add(this.positive_weights_sum, this.positive_weights);\n\t\t\t\t\thash.add(this.negative_weights_sum, this.negative_weights);\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t}\n\t\t},\n\n\t\t/**\n\t\t * train online (a single instance).\n\t\t *\n\t\t * @param features a SINGLE training sample (a hash of feature-value pairs).\n\t\t * @param expected the classification value for that sample (0 or 1).\n\t\t * @return true if the input sample got its correct classification (i.e. no change made).\n\t\t */\n\t\ttrainOnline: function(features, expected) {\n\t\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\t\treturn this.train_features(features, expected);\n\t\t\t\t//this.normalized_features(features, /*remove_unknown_features=*/false), expected);\n\t\t},\n\n\t\t/**\n\t\t * Batch training (a set of samples). Uses the option this.retrain_count.\n\t\t *\n\t\t * @param dataset an array of samples of the form {input: {feature1: value1...} , output: 0/1} \n\t\t */\n\t\ttrainBatch: function(dataset) {\n//\t\t\tvar normalized_inputs = [];\n\t\t\tfor (var i=0; i<dataset.length; ++i)\n\t\t\t\tthis.editFeatureValues(dataset[i].input, /*remove_unknown_features=*/false);\n//\t\t\t\tnormalized_inputs[i] = this.normalized_features(dataset[i].input, /*remove_unknown_features=*/false);\n\t\n\t\t\tfor (var r=0; r<=this.retrain_count; ++r)\n\t\t\t\tfor (var i=0; i<dataset.length; ++i) \n\t\t\t\t\tthis.train_features(dataset[i].input, dataset[i].output);\n\t\t},\n\t\t\n\n\t\t/**\n\t\t * @param inputs a SINGLE sample; a hash (feature => value).\n\t\t * @param continuous_output if true, return the net classification score. If false [default], return 0 or 1.\n\t\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.  \n\t\t * @param positive_weights_for_classification, negative_weights_for_classification -\n\t\t  the weights vector to use (either the running 'weights' or 'weights_sum').  \n\t\t * @return the classification of the sample.\n\t\t */\n\t\tperceive_features: function(features, continuous_output, positive_weights_for_classification, negative_weights_for_classification, explain) {\n\t\t\tvar score = 0;\n\t\t\tvar explanations = [];\n\t\t\tfor (var feature in features) {\n\t\t\t\tif (feature in positive_weights_for_classification) {\n\t\t\t\t\tvar positive_weight = positive_weights_for_classification[feature];\n\t\t\t\t\tif (!isFinite(positive_weight)) {\n\t\t\t\t\t\tconsole.dir(positive_weights_for_classification);\n\t\t\t\t\t\tthrow new Error(\"positive_weight[\"+feature+\"]=\"+positive_weight);\n\t\t\t\t\t}\n\t\t\t\t\tvar negative_weight = negative_weights_for_classification[feature];\n\t\t\t\t\tif (!isFinite(negative_weight)) {\n\t\t\t\t\t\tconsole.dir(negative_weights_for_classification);\n\t\t\t\t\t\tthrow new Error(\"negative_weight[\"+feature+\"]=\"+negative_weight);\n\t\t\t\t\t}\n\t\t\t\t\tvar net_weight = positive_weight-negative_weight;\n\t\t\t\t\tvar value = features[feature];\n\t\t\t\t\tif (isNaN(value)) {\n\t\t\t\t\t\tconsole.dir(features);\n\t\t\t\t\t\tthrow new Error(\"score is NaN! features[\"+feature+\"]=\"+value+\" net_weight=\"+positive_weight+\"-\"+negative_weight+\"=\"+net_weight);\n\t\t\t\t\t}\n\t\t\t\t\tvar relevance = value * net_weight;\n\t\t\t\t\tscore += relevance;\n\t\t\t\t\tif (isNaN(score)) \n\t\t\t\t\t\tthrow new Error(\"score is NaN! features[\"+feature+\"]=\"+value+\" net_weight=\"+positive_weight+\"-\"+negative_weight+\"=\"+net_weight);\n\t\t\t\t\tif (explain>0) explanations.push(\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tfeature: feature,\n\t\t\t\t\t\t\t\tvalue: value,\n\t\t\t\t\t\t\t\tweight: sprintf(\"+%1.3f-%1.3f=%1.3f\",positive_weight,negative_weight,net_weight),\n\t\t\t\t\t\t\t\trelevance: relevance,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (isNaN(score)) \n\t\t\t\tthrow new Error(\"score is NaN! features=\"+JSON.stringify(features));\n\t\t\tscore -= this.threshold;\n\n\t\t\tif (this.debug) console.log(\"> perceive_features \",JSON.stringify(features),\" = \",score);\n\t\t\tvar result = (continuous_output? score: (score > 0 ? 1 : 0));\n\t\t\tif (explain>0) {\n\t\t\t\texplanations.sort(function(a,b){return Math.abs(b.relevance)-Math.abs(a.relevance); });\n\t\t\t\texplanations.splice(explain, explanations.length-explain);  // \"explain\" is the max length of explanation.\n\t\t\t\t\n\t\t\t\tif (!this.detailed_explanations) {\n\t\t\t\t\texplanations = explanations.map(function(e) {\n\t\t\t\t\t\treturn sprintf(\"%s%+1.2f\", e.feature, e.relevance);\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tresult = {\n\t\t\t\t\tclassification: result,\n\t\t\t\t\texplanation: explanations,\n\t\t\t\t};\n\t\t\t}\n\t\t\treturn result;\n\t\t},\n\n\t\t/**\n\t\t * @param inputs a SINGLE sample (a hash of feature-value pairs).\n\t\t * @param continuous_output if true, return the net classification value. If false [default], return 0 or 1.\n\t\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.  \n\t\t * @return the classification of the sample.\n\t\t */\n\t\tclassify: function(features, explain, continuous_output) {\n\t\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/true);\n\t\t\treturn this.perceive_features(\n\t\t\t\t//this.normalized_features(features, /*remove_unknown_features=*/true),\n\t\t\t\tfeatures, \n\t\t\t\tcontinuous_output,\n\t\t\t\t(this.do_averaging? this.positive_weights_sum: this.positive_weights),\n\t\t\t\t(this.do_averaging? this.negative_weights_sum: this.negative_weights),\n\t\t\t\texplain );\n\t\t},\n}\n\nmodule.exports = WinnowHash;\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/decisiontree/DecisionTree.js":"/* Implementation of Decision Tree classifier, ID3 implementation\n   the code based on https://github.com/bugless/nodejs-decision-tree-id3/blob/master/lib/decision-tree.js\n */\nvar _ = require('underscore');\nvar hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;  // for explanations\n\nfunction DecisionTree(opts) {\n\tif (!opts) opts = {}\n\t// this.debug = opts.debug || false; \n}\n\nDecisionTree.prototype = {\n\t\t\n\t\ttoJSON: function(folder) {\n\t\t\treturn this.root\n\t\t},\n\n\t\tfromJSON: function(json) {\n\t\t\tthis.root = json\n\t\t},\n\n\t\tcreateTree: function(dataset, features) {\n\t\t\tvar targets = _.unique(_.pluck(dataset, 'output'));\n\t\t\tif (targets.length == 1){\n                // console.log(\"end node! \"+targets[0]);\n                return {type:\"result\", val: targets[0], name: targets[0], alias:targets[0]+this.randomTag() }; \n        \t}\n        \t if(features.length == 0){\n                // console.log(\"returning the most dominate feature!!!\");\n                var topTarget = this.mostCommon(targets);\n                return {type:\"result\", val: topTarget, name: topTarget, alias: topTarget+this.randomTag()};\n        \t}\n        \tvar bestFeature = this.maxGain(dataset, features);\n\t\t\tvar remainingFeatures = _.without(features,bestFeature);\n        \tvar possibleValues = _.unique(_.pluck(_.pluck(dataset, 'input'), bestFeature));\n        \tvar node = {name: bestFeature,alias: bestFeature+this.randomTag()};\n        \tnode.type = \"feature\";\n        \tnode.vals = _.map(possibleValues,function(v){\n                var _newS = dataset.filter(function(x) {return x['input'][bestFeature] == v});\n                var child_node = {name:v,alias:v+this.randomTag(),type: \"feature_value\"};\n                child_node.child =  this.createTree(_newS,remainingFeatures);\n                return child_node;\n        }, this);\n        \n        return node;\n\t\t},\n\n\t\tmostCommon: function(l) {\n        \treturn  _.sortBy(l,function(a){ return this.count(a,l); },this).reverse()[0];\n\t\t},\n\n\t\tcount: function(a, l) {\n        \treturn _.filter(l,function(b) { return b === a}).length\n\t\t},\n\n\t\trandomTag: function() {\n        \treturn \"_r\"+Math.round(Math.random()*1000000).toString();\n\t\t},\n\n\t\textractFeatures: function(dataset) {\n\t\t\tvar features = []\n\t\t\tfor (record in dataset)\n\t\t\t{\n\t\t\t\tfor (key in dataset[record]['input'])\n\t\t\t\t{\n\t\t\t\t\tfeatures.push(key)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn features\n\t\t},\n\n\t\tgain: function(dataset, feature){\n        \tvar attrVals = _.unique(_.pluck(_.pluck(dataset, 'input'), feature));\n            var setEntropy = this.entropy(_.pluck(dataset, 'output'));\n        \tvar setSize = _.size(dataset);\n        \tvar entropies = attrVals.map(function(n){\n            \tvar subset = dataset.filter(function(x){return x['input'][feature] === n});\n                return (subset.length/setSize)*this.entropy(_.pluck(subset,'output'));\n        \t }, this);\n        \tvar sumOfEntropies =  entropies.reduce(function(a,b){return a+b},0);\n        \treturn setEntropy - sumOfEntropies;\n\t\t},\n\t\t\n\t\tentropy: function(vals){\n\t        var uniqueVals = _.unique(vals);\n\t        var probs = uniqueVals.map(function(x){return this.prob(x,vals)}, this);\n\t        var logVals = probs.map(function(p){return -p*this.log2(p) }, this);\n\t        return logVals.reduce(function(a,b){return a+b},0);\n\t\t},\n\n\t\tprob: function(val,vals){\n\t        var instances = _.filter(vals,function(x) {return x === val}).length;\n\t        var total = vals.length;\n\t        return instances/total;\n\t\t},\n\n\t\tlog2: function(n){\n\t        return Math.log(n)/Math.log(2);\n\t\t},\n\n\t\tmaxGain: function(dataset, features) {\n\t        return _.max(features,function(e){return this.gain(dataset,e)}, this)\n\t\t},\n\n\t\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t},\n\t\t\n  \t\t/**\n\t\t * Batch training (a set of samples). Uses the option this.retrain_count.\n\t\t *\n\t\t * @param dataset an array of samples of the form {input: {feature1: value1...} , output: 0/1} \n\t\t */\n\t\ttrainBatch: function(dataset) {\n\t\t\tfeatures = this.extractFeatures(dataset)\n\t\t\tthis.root = this.createTree(dataset, features)\n\t\t},\n\t\t\n\t\t/**\n\t\t * @param inputs a SINGLE sample (a hash of feature-value pairs).\n\t\t * @param continuous_output if true, return the net classification value. If false [default], return 0 or 1.\n\t\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.  \n\t\t * @return the classification of the sample.\n\t\t */\n\t\tclassify: function(features, explain, continuous_output) {\n\t\t    root = this.root;\n            while (root.type !== \"result\") {\n                var attr = root.name;\n                var sampleVal = features[attr];\n                        var childNode = _.detect(root.vals,function(x) { return x.name == sampleVal });\n                        if (childNode) {\n                                root = childNode.child;\n                        } else {\n                                root = root.vals[0].child;\n                        }\n                }\n            return root.val;\n\t\t},\n}\n\nmodule.exports = DecisionTree;\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/index.js":"module.exports = {\n\tBinaryRelevance:        require('./BinaryRelevance'),\n\tPassiveAggressive:      require('./PassiveAggressiveHash'),\n\tBinarySegmentation:     require('./BinarySegmentation'),\n\tMulticlassSegmentation: require('./MulticlassSegmentation'),\n\tHomer:                  require('./Homer'),\n\tMetaLabeler:            require('./MetaLabeler'),\n\tCrossLanguageModel:     require('./CrossLangaugeModelClassifier'),\n\tThresholdClassifier:    require('./ThresholdClassifier'),\n\tAdaboost:  \t\t\t\trequire('./Adaboost'),\n\tPartialClassification:  require('./PartialClassification'),\n}\n\n// add a \"classify and log\" method to all classifiers, for demos:\nfor (var classifierClass in module.exports) {\n\tif (module.exports[classifierClass].prototype && module.exports[classifierClass].prototype.classify)\n\t\tmodule.exports[classifierClass].prototype.classifyAndLog = function(sample) {\n\t\t\tconsole.log(sample+\" is \"+this.classify(sample));\n\t\t}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/BinaryRelevance.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar multilabelutils = require('./multilabelutils');\n// var fs = require('fs');\n\n/**\n * BinaryRelevance - Multi-label classifier, based on a collection of binary classifiers. \n * Also known as: One-vs-All.\n * \n * @param opts\n *            binaryClassifierType (mandatory) - the type of the base binary classifier. There is one such classifier per label. \n */\nvar BinaryRelevance = function(opts) {\n\tif (!opts.binaryClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.binaryClassifierType not found\");\n\t}\n\tthis.binaryClassifierType = opts.binaryClassifierType;\n\tthis.debug = opts.debug || false\n\tthis.mapClassnameToClassifier = {};\n}\n\nBinaryRelevance.prototype = {\n\n\t/**\n\t * Tell the classifier that the given sample belongs to the given labels.\n\t * \n\t * @param sample\n\t *            a document.\n\t * @param labels\n\t *            an object whose KEYS are labels, or an array whose VALUES are labels.\n\t */\n\ttrainOnline: function(sample, labels) {\n\t\tlabels = multilabelutils.normalizeOutputLabels(labels);\n\t\tfor (var l in labels) {\n\t\t\tvar positiveLabel = labels[l];\n\t\t\tthis.makeSureClassifierExists(positiveLabel);\n\t\t\tthis.mapClassnameToClassifier[positiveLabel].trainOnline(sample, 1);\n\t\t}\n\t\tfor (var negativeLabel in this.mapClassnameToClassifier) {\n\t\t\tif (labels.indexOf(negativeLabel)<0)\n\t\t\t\tthis.mapClassnameToClassifier[negativeLabel].trainOnline(sample, 0);\n\t\t}\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\t// this variable will hold a dataset for each binary classifier:\n\t\tvar mapClassnameToDataset = {}; \n\n\t\t// create positive samples for each class:\n\t\tfor (var d in dataset) {\n\t\t\tvar sample = dataset[d].input;\n\t\t\tdataset[d].output = multilabelutils.normalizeOutputLabels(dataset[d].output);\n\t\t\tvar labels = dataset[d].output;\n\n\t\t\tfor (var l in labels) {\n\t\t\t\tvar positiveLabel  = labels[l];\n\t\t\t\tthis.makeSureClassifierExists(positiveLabel);\n\t\t\t\tif (!(positiveLabel in mapClassnameToDataset)) // make sure dataset for this class exists\n\t\t\t\t\tmapClassnameToDataset[positiveLabel] = [];\n\t\t\t\tmapClassnameToDataset[positiveLabel].push({\n\t\t\t\t\tinput : sample,\n\t\t\t\t\toutput : 1\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\n\t\t// create negative samples for each class (after all labels are in the array):\n\t\tfor (var d in dataset) {\n\t\t\tvar sample = dataset[d].input;\n\t\t\tvar labels = dataset[d].output;\n\t\t\tfor (var negativeLabel in this.mapClassnameToClassifier) {\n\t\t\t\tif (!(negativeLabel in mapClassnameToDataset)) // make sure dataset for this class exists\n\t\t\t\t\tmapClassnameToDataset[negativeLabel] = [];\n\t\t\t\tif (labels.indexOf(negativeLabel)<0)\n\t\t\t\t\tmapClassnameToDataset[negativeLabel].push({\n\t\t\t\t\t\tinput : sample,\n\t\t\t\t\t\toutput : 0\n\t\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\t// train all classifiers:\n\t\tfor (var label in mapClassnameToDataset) {\n\t\t\tif (this.debug) console.dir(\"TRAIN class=\"+label);\n\t\t\tthis.mapClassnameToClassifier[label]\n\t\t\t\t\t.trainBatch(mapClassnameToDataset[label]);\n\t\t}\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param sample a document.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t * @param withScores - boolean - if true, return an array of [class,score], ordered by decreasing order of score.\n\t *  \n\t * @return an array whose VALUES are the labels.\n\t * @output\n\t * scores [hash] - the scores of each binary classifier in the class\n\t * explanations [hash] positive - features of the classifier with positive labels\n\t *\t\t\t\t\t   negative - features of classifiers with negative labels\n\t * classes [list] the list of given labels\n\t */\n\n\tclassify: function(sample, explain, withScores) {\n\t\tvar labels = []\n\t\tvar scores = []\n\t\tvar explanations = [];\n\t\tvar positive_explanations = {};\n\t\tvar negative_explanations = []\n\n\t\tfor (var label in this.mapClassnameToClassifier) {\n\t\t\tvar classifier = this.mapClassnameToClassifier[label];\n\n\t\t\tif (this.debug) console.dir(\"Classify for class=\"+label)\n\t\t\t\n\t\t\t// fs.writeFileSync('/tmp/labels/'+label, JSON.stringify(classifier.getFeatures(), null, 4), 'utf8');\n\n\t\t\tvar scoreWithExplain = classifier.classify(sample, explain, withScores);\n\t\t\tif (this.debug) console.log(JSON.stringify(scoreWithExplain, null, 4))\n\n\t\t\tvar score = scoreWithExplain.explanation?  scoreWithExplain.classification: scoreWithExplain;\n\t\t\tif (this.debug) console.dir(\"score=\"+score)\n\n\t\t\texplanations_string = scoreWithExplain.explanation\n\n\t\t\t// if (score>0.5)\n\t\t\tif (score>0)\n\t\t\t\t{\n\t\t\t\tlabels.push([label, score])\n\t\t\t\tif (explanations_string) positive_explanations[label]=explanations_string;\n\t\t\t\t}\n\t\t\telse\n\t\t\t\t{\n\t\t\t\tif (explanations_string) negative_explanations.push([label, score, explanations_string])\n\t\t\t\t}\n\n\t\t\tscores.push([label,score])\n\t\t}\n\n\t\tif (this.debug) console.dir(scores)\n\n\t\tif (explain>0)\n\t\t{\n\t\t\tscores = _.sortBy(scores, function(num){ return num[1] }).reverse()\n\t\t\tvar scores_hash = _.object(scores)\n\n\t\t\tnegative_explanations = _.sortBy(negative_explanations, function(num){ return num[1] }).reverse()\n\t\t\tnegative_explanations = _.map(negative_explanations, function(num){ return [num[0],num[2]] });\n\n\t\t\tvar negative_explanations_hash = _.object(negative_explanations)\n\t\t}\n\n\t\tlabels = _.sortBy(labels, function(num){ return num[1] });\n\t\tlabels = _.map(labels.reverse(), function(num){ return num[0] });\n\n\t\treturn (explain>0?\n\t\t\t{\n\t\t\t\tclasses: labels, \n\t\t\t\tscores: scores_hash,\n\t\t\t\texplanation: {\n\t\t\t\t\tpositive: positive_explanations, \n\t\t\t\t\tnegative: negative_explanations_hash,\n\t\t\t\t}\n\t\t\t}:\n\t\t\tlabels);\n\t},\n\n\tclassifyBatch: function(testSet) {\n\t\tvar labels = []\n\t\tvar results = {}\n\t\tvar output = []\n\n\t\tfor (var label in this.mapClassnameToClassifier) {\n\t\t\tvar classifier = this.mapClassnameToClassifier[label]\n\t\t\tvar scoreWithExplain = classifier.classifyBatch(testSet)\n\t\t\tresults[label] = scoreWithExplain\n\t\t}\n\n\t\t_.each(testSet, function(value, key, list){\n\t\t\ttestSet[key]['output'] = []\n\t\t\t_.each(results, function(ar, label, list){\n\t\t\t\tif (ar[key]!=0)\n\t\t\t\t\ttestSet[key]['output'].push(label)\n\t\t\t}, this)\n\t\t}, this)\n\n\t\treturn testSet\n\t},\n\t\n\tgetAllClasses: function() {\n\t\treturn Object.keys(this.mapClassnameToClassifier);\n\t},\n\t\n\t/**\n\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations. \n\t */\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\t//console.log(\"BR setFeatureLookupTable \"+featureLookupTable);\n\t\tthis.featureLookupTable = featureLookupTable;\n\t\tfor (var label in this.mapClassnameToClassifier)\n\t\t\tif (featureLookupTable && this.mapClassnameToClassifier[label].setFeatureLookupTable)\n\t\t\t\tthis.mapClassnameToClassifier[label].setFeatureLookupTable(featureLookupTable);\n\t},\n\n\ttoJSON : function() {\n\t\tvar result = {};\n\t\tfor (var label in this.mapClassnameToClassifier) {\n\t\t\tvar binaryClassifier = this.mapClassnameToClassifier[label];\n\t\t\tif (!binaryClassifier.toJSON) {\n\t\t\t\tconsole.dir(binaryClassifier);\n\t\t\t\tconsole.log(\"prototype: \");\n\t\t\t\tconsole.dir(binaryClassifier.__proto__);\n\t\t\t\tthrow new Error(\"this binary classifier does not have a toJSON function\");\n\t\t\t}\n\t\t\tresult[label] = binaryClassifier.toJSON();\n\t\t}\n\t\treturn result;\n\t},\n\n\tfromJSON : function(json) {\n\t\tfor (var label in json) {\n\t\t\tthis.mapClassnameToClassifier[label] = new this.binaryClassifierType();\n\t\t\tthis.mapClassnameToClassifier[label].fromJSON(json[label]);\n\t\t}\n\t},\n\t\n\t// private function: \n\tmakeSureClassifierExists: function(label) {\n\t\tif (!this.mapClassnameToClassifier[label]) { // make sure classifier exists\n\t\t\tthis.mapClassnameToClassifier[label] = new this.binaryClassifierType();\n\t\t\tif (this.featureLookupTable && this.mapClassnameToClassifier[label].setFeatureLookupTable)\n\t\t\t\tthis.mapClassnameToClassifier[label].setFeatureLookupTable(this.featureLookupTable);\n\t\t\t\n\t\t}\n\t},\n}\n\n\nmodule.exports = BinaryRelevance;\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/multilabelutils.js":"var _ = require(\"underscore\")._;\n/**\n * A utility function, used by several multi-label classifiers.\n * \n * @param scoresVector [[label1,score1],[label2,score2],...]\n * @param explain (int) if >0, return explanation.\n * @param withScores (boolean) if true, return the original scores vector.\n * @param threshold if withScores is false, all labels with scores above this threshold will be returned.\n */\nmodule.exports = {\n\t\t// iterate the list and collect the second item from the every element of the list\n\t\tgetvalue: function (list) {\n\t                  val = []\n                          for (elem in list)\n                               { val.push(list[elem][1]) }\n\t                  return val\n                },\n\n                normalizeClasses: function (expectedClasses) {\n\t\t        if (!_(expectedClasses).isArray())\n\t\t\t        expectedClasses = [expectedClasses];\n\n\t\t        expectedClasses = expectedClasses.map(this.stringifyClass);\n\t\t        expectedClasses.sort();\n\t\t        return expectedClasses;\n\t        },\n\n\t\tstringifyClass: function (aClass) {\n\t\t        return (_(aClass).isString()? aClass: JSON.stringify(aClass));\n\t\t},\n\t\t\n\t\tstringifyIfNeeded: function (label) {\n\t\t\treturn (typeof(label)==='string'? label: JSON.stringify(label));\n\t\t},\n\n\t\tnormalizeOutputLabels: function(labels) {\n\t\t\tif (!Array.isArray(labels))\n\t\t\t\tlabels = [labels];\n\t\t\treturn labels.map(module.exports.stringifyIfNeeded);\n\t\t},\n\t\t\n\t\tmapScoresVectorToMultilabelResult: function(scoresVector, explain, withScores, threshold) {\n\t\t\tvar results;\n\t\t\tif (withScores) {\n\t\t\t\tresults = scoresVector;\n\t\t\t} else {\n\t\t\t\tresults = [];\n\t\t\t\tscoresVector.forEach(function(pair) {\n\t\t\t\t\tif (pair[1]>=threshold)\n\t\t\t\t\t\tresults.push(pair[0]);\n\t\t\t\t});\n\t\t\t}\n\t\t\treturn explain>0? \t{\n\t\t\t\tclasses: results, \n\t\t\t\texplanation: scoresVector.map(function(pair) {return pair[0]+\": \"+pair[1];})\n\t\t\t}: \n\t\t\tresults; \n\t\t}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/PassiveAggressiveHash.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar util = require(\"util\");\nvar multilabelutils = require('./multilabelutils');\n\n\n\n/**\n * Multilabel online classifier based on Perceptron and Passive-Aggressive.\n *\n * features and categories are strings; samples are hashes.\n *\n * Based on Python code Copyright (c) 2013 Joseph Keshet.\n *\n * Conversion to Node.js started by Erel Segal-haLevi, but not finished yet.\n *\n * @since 2013-08\n * \n * @param opts\n *\t\t\tConstant (optional) - tradeoff constant (default=5.0)\n *\t\t\tretrain_count (optional) - number of epoches to run in trainBatch mode (default=10)  \n */\nvar MultiLabelPassiveAggressive = function(opts) {\n\tthis.retrain_count = opts.retrain_count || 10;\n\tthis.Constant = opts.Constant || 5.0;\n\tthis.weights = {\n\t\t//DUMMY_CLASS:{}\n\t};\n\tthis.weights_sum = {\n\t\t//DUMMY_CLASS:{}\n\t};\n\tthis.seenFeatures = {};\n\tthis.num_iterations = 0\n}\n\nMultiLabelPassiveAggressive.prototype = {\n\n\t/** \n\t * @param sample a hash of feature-values (string features)\n\t * @param averaging boolean \n\t * @return an array of pairs [category,score], sorted by decreasing score\n\t */\n\tpredict: function(features, averaging, explain) {\n\t\tvar weights_for_classification = (averaging? this.weights_sum: this.weights);\n\t\tvar scores = {};\n\t\tif (explain>0) var explanations = [];\n\t\t\n//\t\tfor (var feature in features) {\n//\t\t\tif (feature in weights_for_classification) {\n//\t\t\t\tvar weight = weights_for_classification[feature];\n//\t\t\t\tvar value = features[feature];\n//\t\t\t\tvar relevance = value * weight;\n//\t\t\t\tscore += relevance;\n//\t\t\t\tif (explain>0) explanations.push(this.detailed_explanations?\n//\t\t\t\t\t\t{\n//\t\t\t\t\t\t\tfeature: feature,\n//\t\t\t\t\t\t\tvalue: value,\n//\t\t\t\t\t\t\tweight: weight,\n//\t\t\t\t\t\t\trelevance: relevance,\n//\t\t\t\t\t\t}:\n//\t\t\t\t\t\tsprintf(\"%s%+1.2f*%+1.2f=%+1.2f\", feature, value, weight, relevance);\n//\t\t\t\t);\n//\t\t\t}\n//\t\t}\n\t\t\n\t\tscores = hash.inner_product_matrix(features, weights_for_classification); // scores is a map: category=>score\n\t\tvar scoresVector = _.pairs(scores); // scoresVector is an array of pairs [category,score]\n\t\tscoresVector.sort(function(a,b) {return b[1]-a[1]}); // sort by decreasing score\n\t\treturn scoresVector; \n\t},\n\t\n\t/**\n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * \n\t * @param sample a hash of feature-values (string features)\n\t * @param classes an array whose VALUES are classes.\n\t */\n\tupdate: function(sample, classes) {\n\t\tvar classesSet = hash.normalized(classes);\n\n\t\tvar ranks = this.predict(sample, /*averaging=*/false);  // pairs of [class,score] sorted by decreasing score\n\n\t\t// find the lowest ranked relevant label r:\n\t\tvar r = 0\n\t\tvar r_score = Number.MAX_VALUE\n\t\tranks.forEach(function(labelAndScore) {\n\t\t\tvar label = labelAndScore[0];\n\t\t\tvar score = labelAndScore[1];\n\t\t\tif ((label in classesSet) && score < r_score) {\n\t\t\t\tr = label\n\t\t\t\tr_score = score\n\t\t\t}\n\t\t});\n\n\t\t// find the highest ranked irrelevant label s\n\t\tvar s = 0\n\t\tvar s_score = -Number.MAX_VALUE\n\t\tranks.reverse();\n\t\tranks.forEach(function(labelAndScore) {\n\t\t\tvar label = labelAndScore[0];\n\t\t\tvar score = labelAndScore[1];\n\t\t\tif (!(label in classesSet) && score > s_score) {\n\t\t\t\ts = label;\n\t\t\t\ts_score = score;\n\t\t\t}\n\t\t});\n\n\t\tvar loss = Math.max(1.0 - r_score, 0.0) + Math.max(1.0 + s_score, 0.0);\n\t\tif (loss > 0) {\n\t\t\tvar sample_norm2 = hash.sum_of_square_values(sample);\n\t\t\tvar tau = Math.min(this.Constant, loss / sample_norm2);\n\n\t\t\tif (r_score < Number.MAX_VALUE)\n\t\t\t\thash.addtimes(this.weights[r], tau, sample);  // weights[r] += tau*sample\n\t\t\tif (s_score > -Number.MAX_VALUE)\n\t\t\t\thash.addtimes(this.weights[s], -tau, sample); // weights[s] -= tau*sample\n\t\t}\n\t\t// this.weights_sum = (this.weights + this.weights_sum);\n\t\tfor (category in this.weights)\n\t\t\thash.add(this.weights_sum[category], this.weights[category]);\n\t\t\n\t\thash.add(this.seenFeatures, sample);\n\t\tthis.num_iterations += 1;\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *\t\t\tan array with objects of the format: \n\t *\t\t\t{input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\t// preprocessing: add all the classes in the dataset to the weights vector;\n\t\tdataset.forEach(function(datum) {\n\t\t\tthis.addClasses(datum.output);\n\t\t\tthis.editFeatureValues(datum.input, /*remove_unknown_features=*/false);\n\t\t}, this);\n\n\t\tfor (var i=0; i<this.retrain_count; ++i)\n\t\t\tdataset.forEach(function(datum) {\n\t\t\t\tthis.update(datum.input, datum.output);\n\t\t\t}, this);\n\t},\n\t\n\ttrainOnline: function(features, classes) {\n\t\tthis.addClasses(classes);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\tthis.update(features, classes);\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param sample a hash {feature1: value1, feature2: value2, ...}.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, should be added to the result.\n\t * @param withScores - boolean - if true, return an array of [class,score], ordered by decreasing order of score.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\tclassify : function(features, explain, withScores) {\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/true);\n\t\tvar scoresVector = this.predict(features, /*averaging=*/true, explain);\n\t\treturn multilabelutils.mapScoresVectorToMultilabelResult(scoresVector, explain, withScores, /*threshold=*/0);\n\t},\n\n\t\n\t/**\n\t * Copied from Modified Balanced Winnow (see winnow/winnow_hash.js).\n\t * Commented out, because it is unuseful here.\n\t */\n\teditFeatureValues: function (features, remove_unknown_features) {\n//\t\tconsole.log(\"before: \"+util.inspect(features));\n//\t\tif (!('bias' in features))\n//\t\t\tfeatures['bias'] = 1.0;\n//\t\tif (remove_unknown_features) {\n//\t\t\tfor (var feature in features)\n//\t\t\t\tif (!(feature in this.seenFeatures))\n//\t\t\t\t\tdelete features[feature];\n//\t\t}\n//\t\thash.normalize_sum_of_values_to_1(features);\n//\t\tconsole.log(\"after: \"+util.inspect(features));\n\t},\n\n\t\n\t/**\n\t * Tell the classifier that the given classes will be used for the following\n\t * samples, so that it will know to add negative samples to classes that do\n\t * not appear.\n\t * \n\t * @param classes an object whose KEYS are classes, or an array whose VALUES are classes.\n\t */\n\taddClasses: function(classes) {\n\t\tclasses = hash.normalized(classes);\n\t\tfor (var aClass in classes) {\n\t\t\tif (!(aClass in this.weights)) {\n\t\t\t\tthis.weights[aClass]={};\n\t\t\t\tthis.weights_sum[aClass]={};\n\t\t\t}\n\t\t}\n\t},\n\n\tgetAllClasses: function() {\n\t\treturn Object.keys(this.weights);\n\t},\n\n\ttoJSON : function(callback) {\n\t\treturn {\n\t\t\tweights_sum: this.weights_sum,\n\t\t\tweights: this.weights,\n\t\t\tnum_iterations: this.num_iterations,\n\t\t}\n\t},\n\n\tfromJSON : function(json, callback) {\n\t\tthis.weights_sum = json.weights_sum;\n\t\tthis.weights = json.weights;\n\t\tthis.num_iterations = json.num_iterations;\n\t},\n}\n\n\nmodule.exports = MultiLabelPassiveAggressive;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/BinarySegmentation.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar ftrs = require('../../features');\n\n/**\n * BinarySegmentation - Multi-label text classifier, based on a segmentation scheme using base binary classifiers.\n *\n * Inspired by:\n *\n * Morbini Fabrizio, Sagae Kenji. Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems. ACL-HLT 2011\n * http://www.citeulike.org/user/erelsegal-halevi/article/10259046\n *\n * @author Erel Segal-haLevi\n * \n * @param opts\n *            binaryClassifierType (mandatory) - the type of the base binary classifier. \n *            featureExtractor (optional) - a single feature-extractor (see the \"features\" folder), or an array of extractors, for extracting features from the text segments during classification.\n */\nvar BinarySegmentation = function(opts) {\n\tif (!('binaryClassifierType' in opts)) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain binaryClassifierType\");\n\t}\n\tif (!opts.binaryClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.binaryClassifierType is null\");\n\t}\n\n\tthis.binaryClassifierType = opts.binaryClassifierType;\n\tthis.classifier = new this.binaryClassifierType();\n\n\tswitch (opts.segmentSplitStrategy) {\n\tcase 'shortestSegment': this.segmentSplitStrategy = this.shortestSegmentSplitStrategy; break;\n\tcase 'longestSegment':  this.segmentSplitStrategy = this.longestSegmentSplitStrategy;  break;\n\tcase 'cheapestSegment':  this.segmentSplitStrategy = this.cheapestSegmentSplitStrategy;  break;\n\tdefault: this.segmentSplitStrategy = null;\n\t}\n\t\n\tthis.mapClassnameToClassifier = {};\n}\n\nBinarySegmentation.prototype = {\n\t\n\t/* Tell the classifier that the given sample belongs to the given classes.\n\t * \n\t * @param sample\n\t *            a document.\n\t * @param classes\n\t *            an object whose KEYS are classes, or an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, classes) {\n\t\tsample = this.sampleToFeatures(sample, this.featureExtractors);\n\t\tclasses = hash.normalized(classes);\n\t\tfor (var positiveClass in classes) {\n\t\t\tthis.makeSureClassifierExists(positiveClass);\n\t\t\tthis.mapClassnameToClassifier[positiveClass].trainOnline(sample, 1);\n\t\t}\n\t\tfor (var negativeClass in this.mapClassnameToClassifier) {\n\t\t\tif (!classes[negativeClass])\n\t\t\t\tthis.mapClassnameToClassifier[negativeClass].trainOnline(sample, 0);\n\t\t}\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\t// add ['start'] and ['end'] as a try to resolve Append:previous FP\n\t\t_.map(dataset, function(num){ \n\t\t\tnum['input'] = \"['start'] \"+ num['input'] + \" ['end']\"\n\t\t\treturn num });\n\n\t\tthis.classifier.trainBatch(dataset)\n\t},\n\n\t/**\n\t * Internal function - use the model trained so far to classify a single segment of a sentence.\n\t * \n\t * @param segment a part of a text sentence.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\n\n\tclassifySegment: function(segment, explain) {\n\t\t\tvar classification = this.classifier.classify(segment, explain, true);\n\t\t\t// console.log(classification)\n\t\t\t// if (classification.classes.length != 0)\n\t\t\t{\n\t\t\t\t// HERE\n\t\t\t\t// console.log(segment)\n\t\t\t\t// console.log(classification['classes'])\n\t\t\t\t// console.log(classification['scores'])\n\t\t\t\t// console.log(classification['explanation']['positive'])\n\t\t\t\t// console.log(classification['explanation']['negative'])\n\t\t\t\t// // console.log()\n\t\t\t\t// process.exit(0)\n\t\t\t\t// console.log(classification['explanation']['negative']['Query'])\n\t\t\t\t// console.log(classification['explanation']['negative']['10%'])\n\t\t\t}\n\n\n\t\t\t// if ((segment == \"Leased car\")&&('With leased car' in classification['explanation']['negative']))\n\t\t\t\t{\n\t\t\t\t// console.log(classification['explanation']['negative']['With leased car'])\n\t\t\t\t// console.log(\"______________________________________\")\n\t\t\t\t// process.exit(0)\n\n\t\t\t\t}\n\t\t\treturn classification\n\t},\n\n\t/**\n\t * Internal function - use the model trained so far to classify a single segment of a sentence.\n\t * \n\t * @param segment a part of a text sentence.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array [the_best_class, and_its_probability].\n\t */\n\tbestClassOfSegment: function(segment, explain) {\n\t\tvar classes = this.classifySegment(segment, explain);\n\t\tif (classes.classes.length==0) {\n\t\t\t// FEATURES\n\t\t\treturn ['', 0, ''];\n\t\t\t// return ['', 0];\n\t\t} else {\n\t\t\t// HERE\n\t\t\t// console.log([classes.classes[0], classes.scores[classes.classes[0]]])\n\t\t\t// FEATURES\n\t\t\treturn [classes.classes[0], classes.scores[classes.classes[0]], classes['explanation']['positive']];\n\t\t\t// return [classes.classes[0], classes.scores[classes.classes[0]]];\n\t\t}\n\t},\n\t\n\t\n\t/**\n\t * protected function:\n\t * Strategy of finding the cheapest segmentation (- most probable segmentation), using a dynamic programming algorithm.\n\t * Based on: \n\t * Morbini Fabrizio, Sagae Kenji. Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems. ACL-HLT 2011\n \t * http://www.citeulike.org/user/erelsegal-halevi/article/10259046\n\t */\n\tcheapestSegmentSplitStrategy: function(words, accumulatedClasses, explain, explanations) {\n\n\t\t//for (var start=0; start<=words.length; ++start) {\n\t\t//\tfor (var end=start+1; end<=words.length; ++end) {\n\t\t//\t\tvar segment = words.slice(start,end).join(\" \");\n\n\t\t//\t\tvar bestClassAndProbability = this.bestClassOfSegment(segment, explain);\n\t\t//\t\tif (bestClassAndProbability[1] != Infinity)\n\t\t//\t\t{\n\t\t//\t\t\tvar bestClass = bestClassAndProbability[0];\n\t\t//\t\t\tvar bestClassProbability = bestClassAndProbability[1];\n\t\t\t//\t\tdigraph.add(start, end, -bestClassProbability);\n\t\t//\t\t}\n\t\t//\t}\n\t\t//}\n\n\t\tvar cheapest_paths = require(\"graph-paths\").cheapest_paths;\n\n                var mini = Infinity\n                _(words.length).times(function(nn){\n                   cheapestSegmentClassificationCosts = cheapest_paths(segmentClassificationCosts, nn);\n                       _.each(cheapestSegmentClassificationCosts, function(value, key, list){\n                             if (value.cost<mini)\n                               {\n                                mini = value.cost\n\t                            cheapestSentenceClassificationCost = value\n                                 }\n                         }, this)\n                 }, this)\n                   \n     cheapestSegmentClassificationCosts = cheapest_paths(segmentClassificationCosts, 0);\n     cheapestSentenceClassificationCost = cheapestSegmentClassificationCosts[words.length];\n\n\n        var path = cheapestSentenceClassificationCost.path;\n\n\n\t\tfor (var i=0; i<path.length-1; ++i) {\n\t\t\t// var segment = words.slice(cheapestClassificationPath[i],cheapestClassificationPath[i+1]).join(\" \");\n\t\t\tvar segment = words.slice(path[i],path[i+1]).join(\" \");\n\t\t\t//HERE\n\t\t\tvar segmentClassesWithExplain = this.classifySegment(segment, explain);\n\t\t\tvar segmentClasses = (segmentClassesWithExplain.classes? segmentClassesWithExplain.classes: segmentClassesWithExplain);\n\t\t\t\n\t\t\tif (segmentClasses.length>0)\n\t\t\t\taccumulatedClasses[segmentClasses[0]] = true;\n\n\t\t\t// explanations = []\n\t\t\tif (explain>0) {\n\t\t\t\t\tif (segmentClasses.length>0)\n\t\t\t\t\t\texplanations.push([segmentClasses[0], segment, [path[i], path[i+1]],segmentClassesWithExplain['explanation']['positive']])\n\n\t\t\t};\n\t\t\t\n\t\t}\n\t},\n\n\t/**\n\t * protected function:\n\t * Strategy of classifying the shortest segments with a single class.\n\t */\n\tshortestSegmentSplitStrategy: function(words, accumulatedClasses, explain, explanations) {\n\t\tvar currentStart = 0;\n\t\tfor (var currentEnd=1; currentEnd<=words.length; ++currentEnd) {\n\t\t\tvar segment = words.slice(currentStart,currentEnd).join(\" \");\n\t\t\tvar segmentClassesWithExplain = this.classifySegment(segment, explain);\n\t\t\tvar segmentClasses = (segmentClassesWithExplain.classes? segmentClassesWithExplain.classes: segmentClassesWithExplain);\n\n\t\t\tif (segmentClasses.length==1) {\n\t\t\t\t// greedy algorithm: found a section with a single class - cut it and go on\n\t\t\t\taccumulatedClasses[segmentClasses[0]]=true;\n\t\t\t\tcurrentStart = currentEnd;\n\t\t\t\tif (explain>0) {\n\t\t\t\t\texplanations.push(segment);\n\t\t\t\t\texplanations.push(segmentClassesWithExplain.explanation);\n\t\t\t\t};\n\t\t\t}\n\t\t}\n\t},\n\n\t\n\t/**\n\t * protected function:\n\t * Strategy of classifying the longest segments with a single class.\n\t */\n\tlongestSegmentSplitStrategy: function(words, accumulatedClasses, explain, explanations) {\n\t\tvar currentStart = 0;\n\t\tvar segment = null;\n\t\tvar segmentClassesWithExplain = null;\n\t\tvar segmentClasses = null;\n\t\tfor (var currentEnd=1; currentEnd<=words.length; ++currentEnd) {\n\t\t\tvar nextSegment = words.slice(currentStart,currentEnd).join(\" \");\n\t\t\tvar nextSegmentClassesWithExplain = this.classifySegment(nextSegment, explain);\n\t\t\tvar nextSegmentClasses = (nextSegmentClassesWithExplain.classes? nextSegmentClassesWithExplain.classes: nextSegmentClassesWithExplain);\n\t\t\t//console.log(\"\\t\"+JSON.stringify(nextSegment) +\" -> \"+nextSegmentClasses)\n\t\t\tnextSegmentClasses.sort();\n\n\t\t\tif (segmentClasses && segmentClasses.length==1 && (nextSegmentClasses.length>1 || !_(nextSegmentClasses).isEqual(segmentClasses))) {\n\t\t\t\t// greedy algorithm: found a section with a single class - cut it and go on\n\t\t\t\taccumulatedClasses[segmentClasses[0]]=true;\n\t\t\t\tcurrentStart = currentEnd-1;\n\t\t\t\tif (explain>0) {\n\t\t\t\t\texplanations.push(segment);\n\t\t\t\t\texplanations.push(segmentClassesWithExplain.explanation);\n\t\t\t\t};\n\t\t\t}\n\n\t\t\tsegment = nextSegment;\n\t\t\tsegmentClassesWithExplain = nextSegmentClassesWithExplain;\n\t\t\tsegmentClasses = nextSegmentClasses;\n\t\t}\n\t\t\n\t\t// add the classes of the last section:\n\t\tfor (var i in segmentClasses) \n\t\t\taccumulatedClasses[segmentClasses[i]]=true;\n\t\tif (explain>0) {\n\t\t\texplanations.push(segment);\n\t\t\texplanations.push(segmentClassesWithExplain.explanation);\n\t\t};\n\t\t/*if (words.length>20)  {\n\t\t\tconsole.dir(explanations);\n\t\t\tprocess.exit(1);\n\t\t}*/\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param segment a part of a text sentence.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\tclassify: function(sentence, explain) {\n\n\t\t// sentence = \"['start'] \" + sentence + \" ['end']\"\n\t\tvar minWordsToSplit = 2;\n\t\tvar words = sentence.split(/ /);\n\t\t// var words = tokenizer.tokenize(sentence);\n\t\tif (this.segmentSplitStrategy && words.length>=minWordsToSplit) {\n\t\t\tvar accumulatedClasses = {};\n\t\t\tvar explanations = [];\n\t\t\tthis.segmentSplitStrategy(words, accumulatedClasses, explain, explanations); \n\t\t\t\n\t\t\tvar classes = Object.keys(accumulatedClasses);\n\n\t\t\treturn (explain>0?\t{\n\t\t\t\tclasses: classes, \n\t\t\t\texplanation: explanations\n\t\t\t}: \n\t\t\tclasses);\n\t\t} else {\n\t\t\tclassification = this.bestClassOfSegment(sentence, explain)\n\t\t\t// classification = this.classifySegment(sentence, explain);\n\t\t\t//HERER\n\n\t\t\t// console.log(sentence)\n\t\t\t// console.log(classification)\n\t\t\t// process.exit(0)\n\t\t\t// process.exit(0)\n\n\t\t\treturn (explain>0?\t{\n\t\t\t\tclasses: classification[0], \n\t\t\t\t// FEATURES\n\t\t\t\texplanation: [[classification[0], sentence, [0,sentence.length-1], classification[2]]]\n\t\t\t\t// explanation: [[classification[0], sentence, [0,sentence.length-1]]]\n\n\t\t\t}: \n\t\t\tclassification[0]);\n\t\t\t// return {classes: classification[0],\n\t\t\t\t\t// explanation: [[classification[0], sentence, [0,sentence.length-1]]]}\n\t\t\t// return {classes: classification.classes[0]}\n\t\t}\n\t},\n\n\ttoJSON : function(callback) {\n\t\tvar result = {};\n\t\tfor ( var aClass in this.mapClassnameToClassifier) {\n\t\t\tvar binaryClassifier = this.mapClassnameToClassifier[aClass];\n\t\t\tif (!binaryClassifier.toJSON) {\n\t\t\t\tconsole.dir(binaryClassifier);\n\t\t\t\tconsole.log(\"prototype: \");\n\t\t\t\tconsole.dir(binaryClassifier.__proto__);\n\t\t\t\tthrow new Error(\"this binary classifier does not have a toJSON function\");\n\t\t\t}\n\t\t\tresult[aClass] = binaryClassifier.toJSON(callback);\n\t\t}\n\t\treturn result;\n\t},\n\n\tfromJSON : function(json, callback) {\n\t\tfor ( var aClass in json) {\n\t\t\tthis.mapClassnameToClassifier[aClass] = new this.binaryClassifierType();\n\t\t\tthis.mapClassnameToClassifier[aClass].fromJSON(json[aClass]);\n\t\t}\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (featureLookupTable) \n\t\t\t// this.featureLookupTable = featureLookupTable\n\t\t\tif (this.classifier.setFeatureLookupTable)\n\t\t\t\tthis.classifier.setFeatureLookupTable(featureLookupTable);  // for generating clearer explanations only\n\t\t// }\n \t},\n}\n\nmodule.exports = BinarySegmentation;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/index.js":"module.exports = {\n\tNGramsFromArray: require(\"./NGramsFromArray\"),\n\tNGramsOfWords:  require(\"./NGramsOfWords\"),\n\tNGramsOfLetters: require(\"./NGramsOfLetters\"),\n\n\tHypernyms: require(\"./HypernymExtractor\"),\n\tCollectionOfExtractors: require(\"./CollectionOfExtractors\"),\n\tFeatureLookupTable: require(\"./FeatureLookupTable\"),\n\t\n\tLowerCaseNormalizer: require(\"./LowerCaseNormalizer\"),\n\tRegexpNormalizer: require(\"./RegexpNormalizer\"),\n\n\tRegexpSplitter: require(\"./RegexpSplitter\"),\n};\n\n/**\n * Call the given featureExtractor on the given sample, and return the result.\n * Used for testing.\n */\nmodule.exports.call = function(featureExtractor, sample) {\n\tvar features = {};\n\tfeatureExtractor(sample, features);\n\treturn features;\n} \n\n/**\n * If the input is a featureExtractor, return it as is.\n *\n * If it is an array of featureExtractors, convert it to a CollectionOfExtractors.\n *\n */\nmodule.exports.normalize = function(featureExtractorOrArray) {\n\treturn (!featureExtractorOrArray? \n\t\t\t\tfeatureExtractorOrArray:\n\t\t\tArray.isArray(featureExtractorOrArray)? \n\t\t\t\tnew module.exports.CollectionOfExtractors(featureExtractorOrArray):\n\t\t\t\tfeatureExtractorOrArray);\t\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/NGramsFromArray.js":"/**\n * Convert an array of words/tokens to a set of n-grams, for a given n, possibly with a gap:\n */\nmodule.exports = function(numOfWords, gap, grams, features) {\n\t\t\t\tfor (var i=0; i<numOfWords-1-(gap?1:0); ++i) {\n\t\t\t\t\tgrams.unshift(\"[start]\");\n\t\t\t\t\tgrams.push(\"[end]\");\n\t\t\t\t}\n\t\t\t\tfor (var i=0; i<=grams.length-numOfWords; ++i) {\n\t\t\t\t\tsliceOfWords = grams.slice(i, i+numOfWords);\n\t\t\t\t\tif (gap) sliceOfWords[1]=\"-\";\n\t\t\t\t\tvar feature = sliceOfWords.join(\" \");\n\t\t\t\t\tfeatures[feature.trim()]=1;\n\t\t\t\t}\n\t\t\t\tfor (var i=0; i<numOfWords-1-(gap?1:0); ++i) {\n\t\t\t\t\tgrams.pop();\n\t\t\t\t\tgrams.shift();\n\t\t\t\t}\n};\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/NGramsOfWords.js":"/**\n * NGramExtractor - extracts sequences of words in a text as its features.\n */\n\nvar NGramsFromArray = require('./NGramsFromArray');\nmodule.exports = function(numOfWords, gap) {\n\t\t\treturn function(sample, features) {\n\t\t\t\tvar words = sample.split(/[ \\t,;:.!?]/).filter(function(a){return !!a}); // all non-empty words\n\t\t\t\tNGramsFromArray(numOfWords, gap, words, features);\n\t\t\t}\n};\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/NGramsOfLetters.js":"/**\n * Extracts substrings of letters of a given size.\n */\n\nvar PAD_CHAR = '#';\n\n/**\n * Add letter n-gram features to the given feature-vector.\n *\n * @param numOfLetters - a positive integer.\n * @param caseSensitive - boolean. if false, convert all to lower case.\n * @param sample - a string.\n * @param features an initial hash of features (optional).\n * @return a hash with all the different letter n-grams contained in the given sentence.\n */\nmodule.exports = function(numOfLetters, caseSensitive) {\n\treturn function(sample, features) {\n\t\tif (!caseSensitive) sample=sample.toLowerCase();\n\t\tfor (var i=0; i<numOfLetters-1; ++i)\n\t\t\tsample = PAD_CHAR+sample+PAD_CHAR;\n\t\tfor (var firstLetter=0; firstLetter<sample.length-numOfLetters+1; ++firstLetter) {\n\t\t\tvar feature = sample.substr(firstLetter, numOfLetters);\n\t\t\tfeatures[feature]=1;\n\t\t}\n\t}\n}\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/HypernymExtractor.js":"/**\n * HypernymExtractor - extracts hypernyms - words and phrases that are entailed by the given text.\n *\n * A hypernym is described by a regular expression, a feature string, and a confidence score.\n * For example: if regexp=/no (.*)/ and feature=\"without $1\", then, \n *   if the sample contains \"no car\", the extractor will add the feature \"without car\", with the given confidence score (usually a number from 0 to 1).\n *\n * @author Erel Segal-haLevi\n * @since 2013-07\n */\n\n/**\n * Adds hypernym features to the given feature-vector.\n * \n * @param hypernyms - an array of objects {regexp: /regexp/g, feature: \"feature\", confidence: confidence}\n * @param sample - a string.\n * @param features an initial hash of features (optional). The hypernym features will be added to that array.\n */\nmodule.exports = function(hypernyms) {\n\treturn function(sample, features) {\n\t\thypernyms.forEach(function(hypernym) {\n\t\t\tvar matches = null;\n\t\t\tif (!(hypernym.regexp instanceof RegExp)) \n\t\t\t\thypernym.regexp = new RegExp(hypernym.regexp, \"gi\");\n\t\t\tif (hypernym.regexp.test(sample))\n\t\t\t\tfeatures[hypernym.feature]=hypernym.confidence;\n\t\t});\n\t}\n}\n/*\nmodule.exports = function(hypernyms) {\n        return function(sample, features) {\n                hypernyms.forEach(function(hypernym) {\n                        var matches = null;\n                        if (hypernym.regexp instanceof RegExp) {\n                                if (!hypernym.regexp.global) {\n                                        console.warn(\"hypernym regexp, \"+hypernym.regexp+\", is not global - skipping\");\n                                        return;\n                                }\n                        } else {\n                                hypernym.regexp = new RegExp(hypernym.regexp,\"gi\");\n                        }\n                        while ((matches = hypernym.regexp.exec(sample)) !== null) {\n                                var feature = matches[0].replace(hypernym.regexp, hypernym.feature);\n                                features[feature]=hypernym.confidence;\n                        }\n                });\n        }\n}*/\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/CollectionOfExtractors.js":"/**\n * CollectionOfExtractors - combines the features from several feature extractors. \n */\n\n/**\n * @param extractors - an array of other feature extractors. \n * @param sample - a string.\n * @param features an initial hash of features (optional).\n * @return a hash with all features generated from the sample by the different extractors\n */\nmodule.exports = function(extractors) {\n\treturn function(sample, features) {\n\t\tfor (var i=0; i<extractors.length; ++i)\n\t\t\textractors[i](sample, features);\n\t}\n}\n\n\n//var async = require('async');\n/*\nmodule.exports = function(extractors) {\n        return function(sample, features, stopwords, callback){\n                async.eachSeries(extractors, function(extractor, callback1){\n                extractor(sample, features, stopwords, function(err, result){\n                callback1()\n            })\n        }, function(err){\n                callback()\n        })\n    }\n}\n*/\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/FeatureLookupTable.js":"/**\n * FeatureLookupTable - a table for converting features to numbers and vice versa \n */\n\nvar FeatureLookupTable = function() {\n\tthis.featureIndexToFeatureName = [undefined];\n\tthis.featureNameToFeatureIndex = {undefined: 0};\n}\n\nFeatureLookupTable.prototype = {\n\t\n\t// add a single feature, if it does not exist\n\taddFeature: function(feature) {\n\t\tif (!(feature in this.featureNameToFeatureIndex)) {\n\t\t\tvar newIndex = this.featureIndexToFeatureName.length;\n\t\t\tthis.featureIndexToFeatureName.push(feature);\n\t\t\tthis.featureNameToFeatureIndex[feature] = newIndex;\n\t\t}\n\t},\n\t\n\t// add all features in the given hash or array\n\taddFeatures: function(hash) {\n\t\tif (hash instanceof Array) {\n\t\t\tfor (var index in hash)\n\t\t\t\tthis.addFeature(hash[index]);\n\t\t} else if (hash instanceof Object) {\n\t\t\tfor (var feature in hash)\n\t\t\t\tthis.addFeature(feature);\n\t\t} \n\t\telse throw new Error(\"FeatureLookupTable.addFeatures expects a hash or an array, but got: \"+JSON.stringify(hash));\n\t},\n\n\t// add all features in all hashes in the given array\n\taddFeaturess: function(hashes) {\n\t\tfor (var i=0; i<hashes.length; ++i)\n\t\t\tthis.addFeatures(hashes[i]);\n\t},\n\t\n\t/**\n\t * Convert the given feature to a numeric index.\n\t */\n\tfeatureToNumber: function(feature) {\n\t\tthis.addFeature(feature);\n\t\treturn this.featureNameToFeatureIndex[feature];\n\t},\n\t\n\tnumberToFeature: function(number) {\n\t\treturn this.featureIndexToFeatureName[number];\n\t},\n\t\t\n\t/**\n\t * Convert the given hash of features to a numeric array, using 0 for padding.\n\t * If some features in the hash do not exist - they will be added.\n\t * @param hash any hash, for example, {a: 111, b: 222, c: 333}\n\t * @return a matching array, based on the current feature table. For example: [0, 111, 222, 0, 333]\n\t * @note some code borrowed from Heather Arthur: https://github.com/harthur/brain/blob/master/lib/lookup.js\n\t */\n\thashToArray: function(hash) {\n\t\tthis.addFeatures(hash);\n\t\tvar array = [];\n\t\tfor (var featureIndex=0; featureIndex<this.featureIndexToFeatureName.length; ++featureIndex)\n\t\t\tarray[featureIndex]=0;\n\t\tif (hash instanceof Array) {\n\t\t\tfor (var i in hash)\n\t\t\t\tarray[this.featureNameToFeatureIndex[hash[i]]] = true;\n\t\t} else if (hash instanceof Object) {\n\t\t\tfor (var feature in hash)\n\t\t\t\tarray[this.featureNameToFeatureIndex[feature]] = hash[feature];\n\t\t}\n\t\telse throw new Error(\"Unsupported type: \"+JSON.stringify(hash));\n\t\treturn array;\n\t},\n\t\n\t/**\n\t * Convert all the given hashes of features to numeric arrays, using 0 for padding.\n\t * If some features in some of the hashes do not exist - they will be added.\n\t * @param hashes an array of hashes, for example, [{a: 111, b: 222}, {a: 11, c: 33}, ...] \n\t * @return an array of matching arrays, based on the current feature table. For example: [[111, 222], [11, 0, 33]]\n\t */\n\thashesToArrays: function(hashes) {\n\t\tthis.addFeaturess(hashes);\n\t  \n\t\tvar arrays = [];\n\t\tfor (var i=0; i<hashes.length; ++i) {\n\t\t\tarrays[i] = [];\n\t\t\tfor (var feature in this.featureNameToFeatureIndex)\n\t\t\t\tarrays[i][this.featureNameToFeatureIndex[feature]] = hashes[i][feature] || 0;\n\t\t}\n\t\treturn arrays;\n\t},\n\n\t/**\n\t * Convert the given numeric array to a hash of features, ignoring zero values.\n\t * @note some code borrowed from Heather Arthur: https://github.com/harthur/brain/blob/master/lib/lookup.js\n\t */\n\tarrayToHash: function(array) {\n\t\tvar hash = {};\n\t\tfor (var feature in this.featureNameToFeatureIndex) {\n\t\t\tif (array[this.featureNameToFeatureIndex[feature]])\n\t\t\t\thash[feature] = array[this.featureNameToFeatureIndex[feature]];\n\t\t}\n\t\treturn hash;\n\t},\n\t\n\t/**\n\t * Convert the given numeric arrays to array of hashes of features, ignoring zero values.\n\t */\n\tarraysToHashes: function(arrays) {\n\t\tvar hashes = [];\n\t\tfor (var i=0; i<arrays.length; ++i)\n\t\t\thashes[i] = this.arrayToHash(arrays[i]);\n\t\treturn hashes;\n\t},\n\t\n\t\n\ttoJSON: function() {\n\t\treturn {\n\t\t\tfeatureIndexToFeatureName: this.featureIndexToFeatureName,\n\t\t\tfeatureNameToFeatureIndex: this.featureNameToFeatureIndex,\n\t\t}\n\t},\n\t\n\tfromJSON: function(json) {\n\t\tthis.featureIndexToFeatureName = json.featureIndexToFeatureName;\n\t\tthis.featureNameToFeatureIndex = json.featureNameToFeatureIndex;\n\t},\n}\n\nmodule.exports = FeatureLookupTable;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/LowerCaseNormalizer.js":"/**\n * LowerCaseNormalizer - normalizes sentences by converting them to lower-case.\n *\n * @author Erel Segal-haLevi\n * @since 2013-08\n */\n\n/**\n * Normalizes a sentence by converting it to lower case.\n */\nmodule.exports = function(sample) {\n\treturn sample.toLowerCase();\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/RegexpNormalizer.js":"/**\n * RegexpNormalizer - normalizes sentences using a custom regular expression file.\n *\n * A normalization rule is described by two regular expressions: 'source' and 'target'.\n *\n * @author Erel Segal-haLevi\n * @since 2013-07\n */\n\n/**\n * normalizes a sentence based on a list of regular expressions.\n * @param normalizations - an array of objects {source: /regexp/g, target: \"target\"}\n * @param sample - a string.\n * @return a new string, with all normalizations carried out.\n */\nmodule.exports = function(normalizations) {\n\treturn function(sample) {\n\t\tnormalizations.forEach(function(normalization) {\n\t\t\tvar matches = null;\n\t\t\tif (normalization.source instanceof RegExp) {\n\t\t\t\tif (!normalization.source.global) {\n\t\t\t\t\tconsole.warn(\"normalization source, \"+normalization.source+\", is not global - skipping\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tnormalization.source = new RegExp(normalization.source,\"gi\");\n\t\t\t}\n\t\t\tsample = sample.replace(normalization.source, normalization.target);\n\t\t\t//console.log(sample);\n\t\t});\n\t\treturn sample;\n\t}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/features/RegexpSplitter.js":"/**\n * RegexpSplitter - splits sentences using a custom regular expression.\n *\n * @author Erel Segal-haLevi\n * @since 2013-08\n */\n\n/**\n * splits sentences using a custom regular expression.\n * @param regexpString - a string\n * @param delimitersToInclude - a hash (set) of delimiters that will be added to the end of the previous sentence.\n * @param text - a string.\n * @return an array of parts (sentences). \n */\nmodule.exports = function(regexpString, delimitersToInclude) {\n\tregexpString = \"(\"+regexpString+\")\";  // to capture the delimiters\n\tvar regexp = new RegExp(regexpString, \"i\");\n\tif (!delimitersToInclude) delimitersToInclude = {};\n\treturn function(text) {\n\t\tvar parts = text.split(regexp);\n\t\tvar normalizedParts = [];\n\t\tfor (var i=0; i<parts.length; i+=2) {\n\t\t\tparts[i] = parts[i].trim();\n\t\t\tvar part = parts[i];\n\t\t\tif (i+1<parts.length) {\n\t\t\t\tvar delimiter = parts[i+1];\n\t\t\t\tif (delimitersToInclude[delimiter])\n\t\t\t\t\tpart += \" \" + delimiter;\n\t\t\t}\n\t\t\tif (part.length>0)\n\t\t\t\tnormalizedParts.push(part);\n\t\t}\n\t\t//console.log(text);\n\t\t//console.dir(normalizedParts);\n\t\treturn normalizedParts;\n\t}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/MulticlassSegmentation.js":"var hash = require(\"../../utils/hash\");\nvar FeaturesUnit = require(\"../../features\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\n\n/**\n * MulticlassSegmentation - Multi-label text classifier, based on a segmentation scheme using a base multi-class classifier.\n *\n * Inspired by:\n *\n * Morbini Fabrizio, Sagae Kenji. Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems. ACL-HLT 2011\n * http://www.citeulike.org/user/erelsegal-halevi/article/10259046\n *\n * @author Erel Segal-haLevi\n * \n * @param opts\n *            binaryClassifierType (mandatory) - the type of the base binary classifier. \n *            featureExtractor (optional) - a single feature-extractor (see the \"features\" folder), or an array of extractors, for extracting features from the text segments during classification.\n */\nvar MulticlassSegmentation = function(opts) {\n\tif (!opts.multiclassClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.multiclassClassifierType not found\");\n\t}\n\tthis.multiclassClassifierType = opts.multiclassClassifierType;\n\tthis.featureExtractor = FeaturesUnit.normalize(opts.featureExtractor);\n\t\n\tthis.multiclassClassifier = new this.multiclassClassifierType();\n}\n\nMulticlassSegmentation.prototype = {\n\n\t/**\n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * \n\t * @param sample\n\t *            a document.\n\t * @param classes\n\t *            an object whose KEYS are classes, or an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, classes) {\n\t\tsample = this.sampleToFeatures(sample, this.featureExtractor);\n\t\tvar category = (Array.isArray(classes)? classes[0]: classes);\n\t\tthis.multiclassClassifier.trainOnline(sample, category);\n\t\t/*for (var positiveClass in classes) {\n\t\t\tthis.makeSureClassifierExists(positiveClass);\n\t\t\tthis.mapClassnameToClassifier[positiveClass].trainOnline(sample, 1);\n\t\t}\n\t\tfor (var negativeClass in this.mapClassnameToClassifier) {\n\t\t\tif (!classes[negativeClass])\n\t\t\t\tthis.mapClassnameToClassifier[negativeClass].trainOnline(sample, 0);\n\t\t}*/\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\tfor ( var i = 0; i < dataset.length; ++i) {\n\t\t\tdataset[i] = {\n\t\t\t\tinput: this.sampleToFeatures(dataset[i].input, this.featureExtractor),\n\t\t\t\toutput: (Array.isArray(dataset[i].output)? dataset[i].output[0]: dataset[i].output)\n\t\t\t};\n\t\t}\n\t\t\n\t\tthis.multiclassClassifier.trainBatch(dataset);\n\t},\n\n\t/**\n\t * Internal function - use the model trained so far to classify a single segment of a sentence.\n\t * \n\t * @param segment a part of a text sentence.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return {category: xxx, probability: yyy}\n\t */\n\tclassifySegment: function(segment, explain) {\n\t\tvar sample = this.sampleToFeatures(segment, this.featureExtractor);\n\t\treturn this.multiclassClassifier.classify(sample, explain);\n\t},\n\n\t/**\n\t * @param segment a part of a text sentence.\n\t * @return {category: best_category_of_segment, probability: its_probability}\n\t */\n\tbestClassOfSegment: function(segment) {\n\t\tvar sample = this.sampleToFeatures(segment, this.featureExtractor);\n\t\tvar classification = this.multiclassClassifier.classify(sample, 1);\n\t\t//console.log(segment+\": \");\tconsole.dir(classification)\n\t\treturn classification;\n\t},\n\t\n\t\n\t/**\n\t * protected function:\n\t * Strategy of finding the cheapest segmentation (- most probable segmentation), using a dynamic programming algorithm.\n\t * Based on: \n\t * Morbini Fabrizio, Sagae Kenji. Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems. ACL-HLT 2011\n \t * http://www.citeulike.org/user/erelsegal-halevi/article/10259046\n\t */\n\tcheapestSegmentSplitStrategy: function(words, accumulatedClasses, explain, explanations) {\n\t\t\n\t\t// Calculate the cost of classification of the segment from i to j.\n\t\t// (Cost = - log probability).\n\t\tvar segmentClassificationCosts = [];  // best cost to classify segment [i,j]\n\t\tfor (var start=0; start<=words.length; ++start) {\n\t\t\tsegmentClassificationCosts[start] = [];\n\t\t\tfor (var end=0; end<start; ++end)\n\t\t\t\tsegmentClassificationCosts[start][end]=Infinity;\n\t\t\tsegmentClassificationCosts[start][start]=0;\n\t\t\tfor (var end=start+1; end<=words.length; ++end) {\n\t\t\t\tvar segment = words.slice(start,end).join(\" \");\n\t\t\t\tvar classification = this.bestClassOfSegment(segment);\n\t\t\t\tsegmentClassificationCosts[start][end] = -Math.log(classification.probability);\n\t\t\t}\n\t\t}\n\t\t//console.log(words+\":  \");\t\tconsole.log(\"segmentClassificationCosts\");\t\tconsole.dir(segmentClassificationCosts);\n\t\tvar cheapest_paths = require(\"graph-paths\").cheapest_paths;\n\t\tcheapestSegmentClassificationCosts = cheapest_paths(segmentClassificationCosts, 0);\n\t\tcheapestSentenceClassificationCost = cheapestSegmentClassificationCosts[words.length];\n\t\tif (!cheapestSentenceClassificationCost)\n\t\t\tthrow new Error(\"cheapestSegmentClassificationCosts[\"+words.length+\"] is empty\");\n\t\t//console.log(\"cheapestSentenceClassificationCost\");\t\tconsole.dir(cheapestSentenceClassificationCost);\n\t\t\n\t\tvar cheapestClassificationPath = cheapestSentenceClassificationCost.path;\n\t\texplanations.push(cheapestSentenceClassificationCost);\n\t\tfor (var i=0; i<cheapestClassificationPath.length-1; ++i) {\n\t\t\tvar segment = words.slice(cheapestClassificationPath[i],cheapestClassificationPath[i+1]).join(\" \");\n\t\t\t//console.log(segment+\":  \");\n\t\t\tvar segmentCategoryWithExplain = this.classifySegment(segment, explain);\n\t\t\t//console.dir(segmentCategoryWithExplain);\n\t\t\tvar segmentCategory = (segmentCategoryWithExplain.category? segmentCategoryWithExplain.category: segmentCategoryWithExplain);\n\t\t\taccumulatedClasses[segmentCategory]=true;\n\t\t\tif (explain>0) {\n\t\t\t\texplanations.push(segment);\n\t\t\t\texplanations.push(segmentCategoryWithExplain.explanation);\n\t\t\t};\n\t\t}\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param segment a part of a text sentence.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\tclassify: function(sentence, explain) {\n\t\tvar minWordsToSplit = 2;\n\t\tvar words = sentence.split(/ /);\n\t\tif (words.length>=minWordsToSplit) {\n\t\t\tvar accumulatedClasses = {};\n\t\t\tvar explanations = [];\n\t\t\tthis.cheapestSegmentSplitStrategy(words, accumulatedClasses, explain, explanations); \n\t\t\t\n\t\t\tvar classes = Object.keys(accumulatedClasses);\n\t\t\treturn (explain>0?\t{\n\t\t\t\tclasses: classes, \n\t\t\t\texplanation: explanations\n\t\t\t}: \n\t\t\tclasses);\n\t\t} else {\n\t\t\treturn this.classifySegment(sentence, explain);\n\t\t}\n\t},\n\n\ttoJSON : function() {\n\t\treturn this.multiclassClassifier.toJSON();\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.multiclassClassifier.fromJSON(json);\n\t},\n\t\n\t// private function: \n\tsampleToFeatures: function(sample, featureExtractor) {\n\t\tvar features = sample;\n\t\tif (featureExtractor) {\n\t\t\ttry {\n\t\t\t\tfeatures = {};\n\t\t\t\tfeatureExtractor(sample, features);\n\t\t\t} catch (err) {\n\t\t\t\tthrow new Error(\"Cannot extract features from '\"+JSON.stringify(sample)+\"': \"+JSON.stringify(err));\n\t\t\t}\n\t\t}\n\t\treturn features;\n\t},\n}\n\nmodule.exports = MulticlassSegmentation;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/Homer.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar util = require('util');\nvar multilabelutils = require('./multilabelutils');\n\n\n/**\n * HOMER - Hierarchy Of Multilabel classifiERs. See:\n * \n * Tsoumakas Grigorios, Katakis Ioannis, Vlahavas Ioannis. Effective and Efficient Multilabel Classification in Domains with Large Number of Labels in Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD'08):XX 2008.\n * http://www.citeulike.org/user/erelsegal-halevi/tag/homer\n * \n * @param opts\n *            multilabelClassifierType (mandatory) - the type of the multilabel classifier used in each level of the hierarchy.\n *            splitLabel (optional) - a function that splits a label to a array of sub-labels, from root to leaves. DEFAULT: split around the \"@\" char. \n *            joinLabel (optional) - a function that joins an array of sub-labels, from root to leaves, to create a full label. DEFAULT: join with the \"@\" char.\n *  \n * @note The original HOMER paper used a clustering algorithm to create a hierarchy of labels.\n * This clustering algorithm is not implemented here.\n * Instead, we use a custom function that converts a label to a path in the hierarchy, and another custom function that converts a path back to a label.\n */\nvar Homer = function(opts) {\n\topts = opts || {};\n\tif (!opts.multilabelClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.multilabelClassifierType is null\");\n\t}\n\tthis.multilabelClassifierType = opts.multilabelClassifierType;\n\t\n\tthis.splitLabel = opts.splitLabel || function(label)      {return label.split(/@/);}\n\tthis.joinLabel  = opts.joinLabel  || function(superlabel) {return superlabel.join(\"@\");}\n\t\n\tthis.root = {\n\t\tsuperlabelClassifier: this.newMultilabelClassifier(),\n\t\tmapSuperlabelToBranch: {}\n\t}\n\t\n\tthis.allClasses = {};\n}\n\nHomer.prototype = {\n\n\t/**\n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * \n\t * @param sample\n\t *            a document.\n\t * @param classes\n\t *            an object whose KEYS are classes, or an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, labels) {\n\t\tlabels = multilabelutils.normalizeOutputLabels(labels);\n\t\t\n\t\tfor (var i in labels)\n\t\t\tthis.allClasses[labels[i]]=true;\n\t\t\n\t\treturn this.trainOnlineRecursive(\n\t\t\t\tsample, \n\t\t\t\tlabels.map(this.splitLabel), \n\t\t\t\tthis.root);\n\t},\n\n\t\n\t/**\n\t *  Recursive internal subroutine of trainOnline.\n\t *  @param splitLabels an array of arrays: each internal array represents the parts of a single label.\n\t */\n\ttrainOnlineRecursive: function(sample, splitLabels, treeNode) {\n\t\tvar superlabels = {}; // the first parts of each of the splitLabels\n\t\tvar mapSuperlabelToRest = {};   // each value is a list of continuations of the key. \n\t\tfor (var i in splitLabels) {\n\t\t\tvar splitLabel = splitLabels[i];\n\t\t\tvar superlabel = splitLabel[0];\n\t\t\tsuperlabels[superlabel] = true;\n\t\t\tif (splitLabel.length>1) {\n\t\t\t\tif (!mapSuperlabelToRest[superlabel]) \n\t\t\t\t\tmapSuperlabelToRest[superlabel] = [];\n\t\t\t\tmapSuperlabelToRest[superlabel].push(splitLabel.slice(1));\n\t\t\t}\n\t\t}\n\n\t\ttreeNode.superlabelClassifier.trainOnline(sample, Object.keys(superlabels));\n\t\tfor (var superlabel in mapSuperlabelToRest) {\n\t\t\tif (!(superlabel in treeNode.mapSuperlabelToBranch)) {\n\t\t\t\ttreeNode.mapSuperlabelToBranch[superlabel] = {\n\t\t\t\t\tsuperlabelClassifier: this.newMultilabelClassifier(),\n\t\t\t\t\tmapSuperlabelToBranch: {}\n\t\t\t\t}\n\t\t\t}\n\t\t\tthis.trainOnlineRecursive(sample, mapSuperlabelToRest[superlabel], treeNode.mapSuperlabelToBranch[superlabel]);\n\t\t}\n\t},\n\t\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\tdataset = dataset.map(function(datum) {\n\t\t\tvar normalizedLabels = multilabelutils.normalizeOutputLabels(datum.output);\n\t\t\tfor (var i in normalizedLabels)\n\t\t\t\tthis.allClasses[normalizedLabels[i]]=true;\n\t\t\treturn {\n\t\t\t\tinput: datum.input,\n\t\t\t\toutput: normalizedLabels.map(this.splitLabel)\n\t\t\t}\n\t\t}, this);\n\t\t\n\t\t// [ [ 'Offer', 'Leased Car', 'Without leased car' ], [ 'Offer', 'Working Hours', '9 hours' ] ]\n\t\t\n\t\treturn this.trainBatchRecursive(dataset, this.root);\n\t},\n\t\n\t/**\n\t *  Recursive internal subroutine of trainBatch.\n\t */\n\ttrainBatchRecursive: function(dataset, treeNode) {\n\t\tvar superlabelsDataset = [];\n\t\tvar mapSuperlabelToRestDataset = {};\n\t\tdataset.forEach(function(datum) { \n\t\t\tvar splitLabels = datum.output;\t// [ [ 'Offer', 'Leased Car', 'Without leased car' ], [ 'Offer', 'Working Hours', '9 hours' ] ]\n\t\t\tvar superlabels = {};           // the first parts of each of the splitLabels\n\t\t\tvar mapSuperlabelToRest = {};   // each value is a list of continuations of the key. \n\t\t\tfor (var i in splitLabels) { \n\t\t\t\tvar splitLabel = splitLabels[i];//[ 'Offer', 'Leased Car', 'Without leased car' ]\n\t\t\t\tvar superlabel = splitLabel[0];\n\t\t\t\tsuperlabels[superlabel] = true; //superlabels['Offer'] = true\n\t\t\t\tif (splitLabel.length>1) { \t\t// if it have more than one label (superlabel)\n\t\t\t\t\tif (!mapSuperlabelToRest[superlabel]) \n\t\t\t\t\t\tmapSuperlabelToRest[superlabel] = [];\n\t\t\t\t\tmapSuperlabelToRest[superlabel].push(splitLabel.slice(1));//['Leased Car', 'Without leased car']\n\t\t\t\t}\n\t\t\t}\n\n/*\t\t\tSample of mapSuperlabelToRest\n\t\t\t{ Offer: \n\t\t\t[ [ 'Leased Car', 'Without leased car' ],\n   \t\t\t  [ 'Working Hours', '9 hours' ] ] }\n\n\t\t\tSample of superlabelsDataset, initial dataset with superlabel instead of entire output\n\t\t\t'. [end]': 0.965080896043587 },\n\t\t\toutput: [ 'Offer' ] } ]\n*/\n\t\t\tsuperlabelsDataset.push({\n\t\t\t\tinput: datum.input,\n\t\t\t\toutput: Object.keys(superlabels)\n\t\t\t});\t\t\n\n\t\t\tfor (var superlabel in mapSuperlabelToRest) {\n\t\t\t\tif (!(superlabel in mapSuperlabelToRestDataset)) \n\t\t\t\t\tmapSuperlabelToRestDataset[superlabel] = [];\n\t\t\t\tmapSuperlabelToRestDataset[superlabel].push({\n\t\t\t\t\tinput: datum.input,\n\t\t\t\t\toutput: mapSuperlabelToRest[superlabel]\n\t\t\t\t});\n\t\t\t}\n\t\t}, this);\n\t\t\n/*\t\tSample of mapSuperlabelToRestDataset\n\t\t{ Offer: [ { input: [Object], output: [[\"Leased Car\",\"Without leased car\"],[\"Working Hours\",\"9 hours\"]] } ] }\n*/\n\n\t\t// train the classifier only on superlabels\n\t\ttreeNode.superlabelClassifier.trainBatch(superlabelsDataset);\n\n\t\tfor (var superlabel in mapSuperlabelToRestDataset) {\n\t\t\tif (!(superlabel in treeNode.mapSuperlabelToBranch)) {\n\t\t\t\ttreeNode.mapSuperlabelToBranch[superlabel] = {\n\t\t\t\t\tsuperlabelClassifier: this.newMultilabelClassifier(),\n\t\t\t\t\tmapSuperlabelToBranch: {}\n\t\t\t\t}\n\t\t\t}\n/*\t\t\ttrain the next level classifier for a give superlabel classifier superlabel (from loop)\n\t\t\twith the dataset from new structure mapSuperlabelToRestDataset (see above)\n*/\t\t\tthis.trainBatchRecursive(mapSuperlabelToRestDataset[superlabel], treeNode.mapSuperlabelToBranch[superlabel]);\n\t\t}\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param sample a document.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\tclassify: function(sample, explain) {\n\t\tvar splitLabels = this.classifyRecursive(sample, explain, this.root);\n\t\t//console.log(\"splitLabels:\"+JSON.stringify(splitLabels));\n\t\tif (explain>0) {\n\t\t\tsplitLabels.classes = splitLabels.classes.map(this.joinLabel);\n\t\t} else {\n\t\t\tsplitLabels = splitLabels.map(this.joinLabel);\n\t\t}\n\t\treturn splitLabels;\n\t},\n\t\n\t\n\t/**\n\t *  Recursive internal subroutine of classify.\n\t *  @return an array of arrays, where each internal array represents a split label.\n\t */\n\tclassifyRecursive: function(sample, explain, treeNode, depth) {\n\t\tif (!depth) depth = 1;\n\t\t// classify the superlabel \n\t\tvar superlabelsWithExplain = treeNode.superlabelClassifier.classify(sample, explain);\n\t\tvar superlabels = (explain>0? superlabelsWithExplain.classes: superlabelsWithExplain);\n\t\t\n\t\tvar splitLabels = [];\n\t\tif (explain>0) {\n\t\t\tvar explanations = [\"depth=\"+depth+\": \"+superlabels, superlabelsWithExplain.explanation];\n\t\t}\n\n\t\t// for all superlabels that were classified, may be there are more than one that were classified with it\n\t\tfor (var i in superlabels) {\n\t\t\tvar superlabel = superlabels[i];\n\t\t\tvar splitLabel = [superlabel];\n\t\t\t\n\t\t\t// classifier of [Offer] types / second level / classifies Offer's parameters\n\t\t\tvar branch = treeNode.mapSuperlabelToBranch[superlabel];\n\t\t\t\n\t\t\tif (branch) {\n\t\t\t\t\n\t\t\t\t// [ [ 'Without leased car' ] ]\n\t\t\t\tvar branchLabelsWithExplain = this.classifyRecursive(sample, explain, branch, depth+1);\n\t\t\t\t\n\t\t\t\tvar branchLabels = (explain>0? branchLabelsWithExplain.classes: branchLabelsWithExplain);\n\t\t\t\t\t\n\t\t\t\tfor (var j in branchLabels)\n\t\t\t\t\tsplitLabels.push(splitLabel.concat(branchLabels[j]));\n\t\t\t\tif (explain>0) \n\t\t\t\t\texplanations = explanations.concat(branchLabelsWithExplain.explanation);\n\t\t\t} else {\n\t\t\t\tsplitLabels.push(splitLabel);\n\t\t\t}\n\t\t}\n\t\treturn (explain>0? \n\t\t\t\t{classes: splitLabels, explanation: explanations}:\n\t\t\t\tsplitLabels);\n\t},\n\n\n\ttoJSON: function() {\n\t\tvar json = this.toJSONRecursive(this.root);\n\t\tjson.allClasses = this.allClasses;\n\t\treturn json;\n\t},\n\t\n\ttoJSONRecursive: function(treeNode) {\n\t\tvar treeNodeJson = { \n\t\t\tsuperlabelClassifier: treeNode.superlabelClassifier.toJSON(),\n\t\t\tmapSuperlabelToBranch: {}\n\t\t};\n\t\tfor (var superlabel in treeNode.mapSuperlabelToBranch) {\n\t\t\ttreeNodeJson.mapSuperlabelToBranch[superlabel] = this.toJSONRecursive(treeNode.mapSuperlabelToBranch[superlabel]);\n\t\t}\n\t\treturn treeNodeJson;\n\t},\n\n\tfromJSON: function(json) {\n\t\tthis.allClasses = json.allClasses;\n\t\tthis.root = this.fromJSONRecursive(json);\n\t},\n\t\n\tfromJSONRecursive: function(treeNodeJson) {\n\t\tvar treeNode = {\n\t\t\tmapSuperlabelToBranch: {}\n\t\t}; \n\t\ttreeNode.superlabelClassifier =  this.newMultilabelClassifier();\n\t\ttreeNode.superlabelClassifier.fromJSON(treeNodeJson.superlabelClassifier);\n\t\tfor (var superlabel in treeNodeJson.mapSuperlabelToBranch) \n\t\t\ttreeNode.mapSuperlabelToBranch[superlabel] = \n\t\t\t\tthis.fromJSONRecursive(treeNodeJson.mapSuperlabelToBranch[superlabel]);\n\t\treturn treeNode;\n\t},\n\n\tgetAllClasses: function() {\n\t\treturn Object.keys(this.allClasses);\n\t},\n\t\n\t/**\n\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations. \n\t */\n\tsetFeatureLookupTableRecursive: function(featureLookupTable, treeNode) {\n\t\tif (treeNode.superlabelClassifier && treeNode.superlabelClassifier.setFeatureLookupTable)\n\t\t\ttreeNode.superlabelClassifier.setFeatureLookupTable(featureLookupTable);\n\t\tfor (var superlabel in treeNode.mapSuperlabelToBranch)\n\t\t\tthis.setFeatureLookupTableRecursive(featureLookupTable, treeNode.mapSuperlabelToBranch[superlabel]);\n\t},\n\t\n\t/**\n\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations. \n\t */\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\t//console.log(\"HOMER setFeatureLookupTable \"+featureLookupTable);\n\t\tthis.featureLookupTable = featureLookupTable;\n\t\tthis.setFeatureLookupTableRecursive(featureLookupTable, this.root);\n\t},\n\t\n\t\n\tnewMultilabelClassifier: function() {\n\t\tvar classifier = new this.multilabelClassifierType();\n\t\tif (this.featureLookupTable && classifier.setFeatureLookupTable)\n\t\t\tclassifier.setFeatureLookupTable(this.featureLookupTable);\n\t\treturn classifier;\n\t}\n}\n\n\n/*\n * UTILS\n */\n\nmodule.exports = Homer;","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/MetaLabeler.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\n\n/**\n * MetaLabeler - Multi-label classifier, based on:\n * \n * Tang Lei, Rajan Suju, Narayanan Vijay K.. Large scale multi-label classification via metalabeler in Proceedings of the 18th international conference on World wide webWWW '09(New York, NY, USA):211-220ACM 2009.\n * http://www.citeulike.org/user/erelsegal-halevi/article/4860265\n * \n * A MetaLabeler uses two multi-class classifiers to create a single multi-label classifier. One is called \"ranker\" and the other is called \"counter\".\n * \n * The MetaLabeler assigns labels to a sample in the following two stages:\n *  - Stage 1: Ranking. The sample is sent to the \"ranker\", which returns all available labels ordered from the most relevant to the least relevant.\n *  - Stage 2: Counting. The sample is sent to the \"counter\", which returns integer C >= 0 which represents a number of labels.\n * The MetaLabeler returns the C most relevant labels from the list returned by the ranker.   \n * \n * @param opts\n *            rankerType (mandatory) - the type of the multi-class classifier used for ranking the labels. \n *            counterType (mandatory) - the type of the multi-class classifier used for selecting the number of labels. \n */\nvar MetaLabeler = function(opts) {\n\tif (!opts.rankerType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.rankerType not found\");\n\t}\n\tif (!opts.counterType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.counterType not found\");\n\t}\n\tthis.ranker = new opts.rankerType();\n\tthis.counter = new opts.counterType();\n}\n\nMetaLabeler.prototype = {\n\n\t/**\n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * \n\t * @param sample  a document.\n\t * @param labels an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, labels) {\n\t\t// The ranker is just trained by the given set of relevant labels:\n\t\tthis.ranker.trainOnline(sample, labels);\n\n\t\t// The counter is trained by the *number* of relevant labels:\n\t\tvar labelCount = (Array.isArray(labels)? labels: Object.keys(labels)).length;\n\t\tthis.counter.trainOnline(sample, labelCount);\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\t// The ranker is just trained by the given set of labels relevant to each sample:\n\t\tthis.ranker.trainBatch(dataset);\n\n\t\t// The counter is trained by the *number* of labels relevant to each sample:\n\t\tvar labelCountDataset = dataset.map(function(datum) {\n\t\t\tvar labelCount = (Array.isArray(datum.output)? datum.output.length: 1);\n\t\t\treturn {\n\t\t\t\tinput: datum.input,\n\t\t\t\toutput: labelCount\n\t\t\t};\n\t\t});\n\t\tthis.counter.trainBatch(labelCountDataset);\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * \n\t * @param sample a document.\n\t * @param explain - int - if positive, an \"explanation\" field, with the given length, will be added to the result.\n\t *  \n\t * @return an array whose VALUES are classes.\n\t */\n\tclassify: function(sample, explain) {\n\t\tvar rankedLabelsWithExplain = this.ranker.classify(sample, explain, /*withScores=*/true);\n\t\tvar rankedLabels = (explain>0? rankedLabelsWithExplain.classes: rankedLabelsWithExplain);\n\t\tvar labelCountWithExplain = this.counter.classify(sample, explain, /*withScores=*/true);\n\t\tvar labelCount = (explain>0? labelCountWithExplain.classes[0][0]: labelCountWithExplain[0][0]);\n\t\tif (_.isString(labelCount)) labelCount = parseInt(labelCount);\n\t\t\n\t\t// Pick the labelCount most relevant labels from the list returned by the ranker:   \n\t\tvar positiveLabelsWithScores = rankedLabels.slice(0, labelCount);\n\n\t\tvar positiveLabels = positiveLabelsWithScores\n\n\t\tif (positiveLabelsWithScores.length != 0)\n\t\t\tif (_.isArray(positiveLabelsWithScores[0]))\n\t\t\t\tvar positiveLabels = positiveLabelsWithScores.map(function(labelWithScore) {return labelWithScore[0]});\n\t\t\n\t\treturn (explain>0? {\n\t\t\tclasses: positiveLabels,\n\t\t\texplanation: {\n\t\t\t\tranking: rankedLabelsWithExplain.explanation,\n\t\t\t\tcounting: labelCountWithExplain.explanation\n\t\t\t}\n\t\t}:\n\t\tpositiveLabels)\n\t},\n\t\n\tgetAllClasses: function() {\n\t\treturn this.ranker.getAllClasses();\n\t},\n\n\ttoJSON : function() {\n\t},\n\n\tfromJSON : function(json) {\n\t},\n\t\n\t/**\n\t * Link to a FeatureLookupTable from a higher level in the hierarchy (typically from an EnhancedClassifier), used ONLY for generating meaningful explanations. \n\t */\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (this.ranker.setFeatureLookupTable)\n\t\t\tthis.ranker.setFeatureLookupTable(featureLookupTable);\n\t\tif (this.counter.setFeatureLookupTable)\n\t\t\tthis.counter.setFeatureLookupTable(featureLookupTable);\n\t},\n}\n\n\nmodule.exports = MetaLabeler;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/CrossLangaugeModelClassifier.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar util = require(\"util\");\nvar multilabelutils = require('./multilabelutils');\n\nvar CrossLanguageModel = require('languagemodel').CrossLanguageModel;\n\n\n\n/**\n * Multilabel classifier based on cross-language model.\n * \n * See https://github.com/erelsgl/languagemodel .\n *\n * @param opts\n *\t\t\tsmoothingCoefficient (lamda of the model)\n *          threshold (optional; default 0) - for selecting relevant/irrelevant classes.\n *          labelFeatureExtractor (optional) - function that extracts features from the output labels. \n *\n */\nvar CrossLanguageModelClassifier = function(opts) {\n\tthis.model = new CrossLanguageModel(opts);\n\tthis.labelFeatureExtractor = opts.labelFeatureExtractor;\n\tthis.threshold = opts.threshold || 0;\n\tthis.allLabels = {};   \n\tthis.allLabelsFeatures = {};\n}\n\nCrossLanguageModelClassifier.prototype = {\n\n\t/**\n\t * Train the classifier with the given input features and the given array of output labels.\n\t * \n\t * @note In the original paper, training was apparently done with a single output label per training instance. \n\t * It is not clear how to train when there are multiple   output labels per training instance.\n\t * I asked: \"suppose there is an input sentence \"Where is the robot and how do I use it?\" and it is labeled with two different output sentences: \"The robot is there\" and \"Read the instructions\".   What exactly do you put in the training set in this case?\"\n\t * And Anton Leusky replied: \"in your example, both labels are in the training set. The goal is to have the classifier to rank these labels above all others for question \"Where is the robot and how do I use it?\" The order in which the correct labels are ranked is irrelevant. \"\n\t */\n\ttrainOnline: function(features, labels) {\n\t\tthis.model.trainOnline(\n\t\t\tfeatures, // input features\n\t\t\tthis.labelsToFeatures(labels)); // output features\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents.\n\t * \n\t * @param dataset\n\t *\t\t\tan array with objects of the format: \n\t *\t\t\t{input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch : function(dataset) {\n\t\tdataset = dataset.map(function(datum) {\n\t\t\treturn {\n\t\t\t\tinput: datum.input,\n\t\t\t\toutput: this.labelsToFeatures(datum.output),\n\t\t\t}\n\t\t}, this);\n\t\tthis.model.trainBatch(dataset);\n\t},\n\n\tclassify : function(features, explain, withScores) {\n\t\tvar scoresVector = [];\n\t\tfor (var labelString in this.allLabels) {\n\t\t\tvar label = this.allLabels[labelString];\n\t\t\tvar labelFeatures = this.allLabelsFeatures[labelString];\n\t\t\tif (!labelFeatures)\n\t\t\t\tthrow new Error(\"label features for \"+labelString+\" are undefined\");\n\t\t\tvar similarity = -this.model.divergence(features, labelFeatures);\n\t\t\tscoresVector.push([label, similarity]);\n\t\t}\n\t\tscoresVector.sort(function(a,b) {return b[1]-a[1]}); // sort by decreasing score\n\t\t\n\t\treturn multilabelutils.mapScoresVectorToMultilabelResult(scoresVector, explain, withScores, this.threshold);\n\t},\n\n\n\t/**\n\t * @return an array with all possible output labels.\n\t */\n\tgetAllClasses: function() {\n\t\treturn Object.keys(this.allLabels);\n\t},\n\t\n\t/**\n\t * Internal function.\n\t * \n\t * Converts an array of output labels to a hash of features.\n\t */\n\tlabelsToFeatures: function(labels) {\n\t\tif (!Array.isArray(labels))  labels = [labels];\n\t\tvar features = {};\n\t\tfor (var i in labels) {\n\t\t\tvar label = labels[i];\n\t\t\tvar labelString = multilabelutils.stringifyIfNeeded(label);\n\t\t\tvar labelFeatures = {};\n\t\t\tif (this.labelFeatureExtractor) {\n\t\t\t\tthis.labelFeatureExtractor(label, labelFeatures);\n\t\t\t} else if (_.isObject(label)) {\n\t\t\t\tlabelFeatures = label;\n\t\t\t} else {\n\t\t\t\tlabelFeatures[label] = true;\n\t\t\t}\n\t\t\tthis.allLabels[labelString] = label;\n\t\t\tthis.allLabelsFeatures[labelString] = labelFeatures;\n\t\t\tfeatures = util._extend(features, labelFeatures);\n\t\t}\n\t\treturn features;\n\t},\n\n\ttoJSON : function() {\n\t\treturn {\n\t\t\tallLabels: this.allLabels,\n\t\t\tmodel: this.model.toJSON(),\n\t\t}\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.allLabels = json.allLabels;\n\t\tthis.model.fromJSON(json);\n\t},\n}\n\n\nmodule.exports = CrossLanguageModelClassifier;\n\n\nif (process.argv[1] === __filename) {\n\tconsole.log(\"CrossLanguageModelClassifier demo start\");\n\t\n\tvar classifier = new CrossLanguageModelClassifier({\n\t\tsmoothingCoefficient : 0.9,\n\t\tlabelFeatureExtractor: null,\n\t\tthreshold: -0.5,\n\t});\n\n\tclassifier.trainBatch([\n\t                       {input: {i:1, want:1, aa:1}, output: {a:1}},\n\t                       {input: {i:1, want:1, bb:1}, output: {b:1}},\n\t                       {input: {i:1, want:1, cc:1}, output: {c:1}},\n\t\t]);\n\n\t//console.log(util.inspect(classifier, {depth:10}));\n\t\n\tconsole.log(\"classify:\");\n\tconsole.dir(classifier.classify({i:1, want:1, aa:1, and:1, bb:1}));\t\n\n\tconsole.log(\"classify with explain:\");\n\tconsole.dir(classifier.classify({i:1, want:1, aa:1, and:1, bb:1}, 3));\t\n\n\tconsole.log(\"classify with scores:\");\n\tconsole.dir(classifier.classify({i:1, want:1, aa:1, and:1, bb:1}, 0, true));\t\n\n\tconsole.log(\"classify with scores and explain:\");\n\tconsole.dir(classifier.classify({i:1, want:1, aa:1, and:1, bb:1}, 3, true));\t\n\n\tconsole.log(\"CrossLanguageModelClassifier demo end\");\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/ThresholdClassifier.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar multilabelutils = require('./multilabelutils');\nvar _ = require(\"underscore\")._;\nvar PrecisionRecall = require('../../utils/PrecisionRecall');\nvar partitions = require('../../utils/partitions');\nvar ulist = require('../../utils/list');\n\n\n/* ThresholdClassifier - classifier that converts multi-class classifier to multi-label classifier by finding\n * the best appropriate threshold. \n * @param opts\n *            numOfFoldsForThresholdCalculation - =1 the threshold is approximated on validation set of size 10% of training set\n \t\t\t\t\t\t\t\t\t\t\t\t  >1 n - fold cross - validation is applied to approximate the threshold\n *            evaluateMeasureToMaximize (['Accuracy','F1']) - string of the measure that operate the improvement of threshold\n *\t\t\t  multiclassClassifier - multi-class classifier used for classification.\n * @author Vasily Konovalov\n */\n\nvar ThresholdClassifier = function(opts) {\n\t\n\topts = opts || {};\n\n\tif (!('multiclassClassifierType' in opts)) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain multiclassClassifierType\");\n\t}\n\tif (!opts.multiclassClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.multiclassClassifierType is null\");\n\t}\n\n\tif (!('evaluateMeasureToMaximize' in opts)) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain evaluateMeasureToMaximize\");\n\t}\n\tif (!opts.evaluateMeasureToMaximize) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.evaluateMeasureToMaximize is null\");\n\t}\n\tif (!opts.numOfFoldsForThresholdCalculation) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.numOfFoldsForThresholdCalculation is null\");\n\t}\n\t\n\tthis.multiclassClassifier = new opts.multiclassClassifierType();\n\n\t// [F1, Accuracy]\t\n\tthis.evaluateMeasureToMaximize = opts.evaluateMeasureToMaximize;\n\n\t// constant size of validation set\n\tthis.devsetsize = 0.1\n\n\t// > 1, n - fold cross - validation, otherwise validation set\n\tthis.numOfFoldsForThresholdCalculation = opts.numOfFoldsForThresholdCalculation\n}\n\nThresholdClassifier.prototype = {\n\n\ttrainOnline: function(sample, labels) {\n\t\tthrow new Error(\"ThresholdClassifier does not support online training\");\n\t},\n\n\t/**\n\t * Train the classifier with all the given documents and identify the best possible threshold\n\t * simply by running over all relevant scores and determining the value of feedback function\n\t * (F1 by default)\n\t * \n\t * @param dataset\n\t *            an array with objects of the format: \n\t *            {input: sample1, output: [class11, class12...]}\n\t * @author Vasily Konovalov\n\t * \n\t */\n\ttrainBatch : function(dataset) {\n\n\t\t_.times(3, function(n) {dataset = _.shuffle(dataset)})\n\n\t\tif (this.numOfFoldsForThresholdCalculation > 1) {\n\t\t\tthresholds=[]\n\t\t\tbest_performances=[]\n\t\t\taverage_performances = []\n\t\t\tmedian_performances = []\n\t\t\tpartitions.partitions_consistent(dataset, this.numOfFoldsForThresholdCalculation, (function(trainSet, testSet, index) { \t \n\t\t\t\tthis.multiclassClassifier.trainBatch(trainSet);\n\t\t\t\tresult = this.receiveScores(testSet)\n\t\t\t\tperformance = this.CalculatePerformance(result[0], testSet, result[1])\n\t\t\t\tbest_performances.push(performance)\n\t\t\t}).bind(this))\n\n\t\t\tthis.stats = best_performances\n\t\t\t\t\t\n\t\t\tthreshold_average = ulist.average(_.pluck(best_performances, 'Threshold'))\n\t\t\tthreshold_median = ulist.median(_.pluck(best_performances, 'Threshold'))\n\n\t\t\tThreshold = threshold_median\n\t\t}\n\t\telse\n\t\t{\n\t\t\tdataset = partitions.partition(dataset, 1, Math.round(dataset.length*this.devsetsize))\n\t\t\ttrainSet = dataset['train']\n\t\t\ttestSet = dataset['test']\n\t\t\tthis.multiclassClassifier.trainBatch(trainSet);\n\t\t\tresult = this.receiveScores(testSet)\n\t\t\tperformance = this.CalculatePerformance(result[0], testSet, result[1])\n\t\t\tThreshold = performance['Threshold']\t\n\t\t}\n\n\t\tthis.multiclassClassifier.threshold = Threshold\n\t},\n\t/*\n\t* Classify dataset and return the scored result in sorted list\n\t*/\n\treceiveScores: function(dataset) {\n\t\tlist_of_scores = [];\n\t\tFN=0\n\t\tfor (var i=0; i<dataset.length; ++i) \n\t\t{\n \t\t\tvar scoresVector = this.multiclassClassifier.classify(dataset[i].input, false, true);\n\n \t\t\tfor (score in scoresVector)\n \t\t\t{\n \t\t\t\tif (dataset[i].output.indexOf(scoresVector[score][0])>-1)\n \t\t\t\t{\n \t\t\t\t\tscoresVector[score].push(\"+\")\n \t\t\t\t\tFN+=1\n \t\t\t\t}\n \t\t\t\telse {scoresVector[score].push(\"-\")}\n\n \t\t\t\tscoresVector[score].push(i)\t\n\t\t\t}\n\n \t\t\tlist_of_scores = list_of_scores.concat(scoresVector)\n  \t\t}\t\n\n  \t\t// list_of_scores = [['d',4],['b',2],['a',1],['c',3]]\n\n\t\tlist_of_scores.sort((function(index){\n\t\t    return function(a, b){\n\t        return (a[index] === b[index] ? 0 : (a[index] < b[index] ? 1 : -1));\n\t\t    };\n\t\t})(1))\n\n\t\treturn [list_of_scores, FN]\n\t},\n\t\n\t/*\n\tCalculate the bst threshold with the highest evaluateMeasureToMaximize\n\t@param  list_of_scores list of scores\n\t@param  testSet test set\n\t@param FN false negative\n\t*/\n\tCalculatePerformance: function(list_of_scores, testSet, FN){\n\n\t\tcurrent_set=[]\n\n\t\tTRUE = 0\n\t\tFP = 0\n\t\tTP = 0\n\n\t\tresult = []\n\t\t\n\t\tfor (var th=0; th<list_of_scores.length; ++th) {\n\n\t\t\tif (list_of_scores[th][2]==\"+\") {TP+=1; FN-=1}\n\t\t\tif (list_of_scores[th][2]==\"-\") {FP+=1;}\n\n\t\t\t// console.log(list_of_scores[th])\n\t\t\t// console.log(\"TP \"+TP+\" FP \"+FP+\" FN \"+FN)\n\n\t\t\tindex_in_testSet = list_of_scores[th][3]\n\n\t\t\tif (_.isEqual(current_set[index_in_testSet], testSet[index_in_testSet]['output'])) \n\t\t\t{TRUE-=1}\n\t\t\t\n\t\t\tif (!current_set[index_in_testSet])\n\t\t\t{current_set[index_in_testSet] = [list_of_scores[th][0]]}\n\t\t\telse\n\t\t\t{current_set[index_in_testSet].push(list_of_scores[th][0])}\n\n \t\t\tif (_.isEqual(current_set[index_in_testSet], testSet[index_in_testSet]['output'])) \n\t\t\t{TRUE+=1 }\n\t\t\t\n \t\t\tPRF = calculate_PRF(TP, FP, FN)\n \t\t\tPRF['Accuracy'] = TRUE/testSet.length\n \t\t\tPRF['Threshold'] = list_of_scores[th][1]\n \n \t\t\tresult[list_of_scores[th][1]] = PRF\n \t\t\t}\n\n\t\t\toptial_measure=0\n\t\t\tindex=Object.keys(result)[0]\n\t\t\tfor (i in result)\n\t\t\t{\n\t\t\t\tif (result[i][this.evaluateMeasureToMaximize] >= optial_measure)\n\t\t\t\t{\n\t\t\t\t\tindex = i\n\t\t\t\t\toptial_measure = result[i][this.evaluateMeasureToMaximize]\n\t\t\t\t}\n\t\t\t}\n\n \t\t\treturn result[index]\n \t\t\n\t},\n\t\n\tclassify: function(sample, explain) {\n\t\treturn this.multiclassClassifier.classify(sample, explain, /*withScores=*/false);\n\t},\n\t\n\tgetAllClasses: function() {\n\t\treturn this.multiclassClassifier.getAllClasses();\n\t},\n\n\ttoJSON : function() {\n\t\treturn this.multiclassClassifier.toJSON();\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.multiclassClassifier.fromJSON(json);\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (this.multiclassClassifier.setFeatureLookupTable)\n\t\t\tthis.multiclassClassifier.setFeatureLookupTable(featureLookupTable);\n\t},\n}\n\nfunction calculate_PRF(TP, FP, FN)\n\t{\n\tstats = {}\n\tstats['TP']=TP\n\tstats['FP']=FP\n\tstats['FN']=FN\n\tstats['Precision'] = (TP + FP == 0? 0: TP/(TP+FP))\n\tstats['Recall'] = (TP + FN == 0? 0: TP/(TP+FN))\n\tstats['F1'] = (stats['Precision'] + stats['Recall'] == 0? 0: 2*stats['Precision']*stats['Recall']/(stats['Precision'] + stats['Recall']))\n\treturn stats\n\t}\n\nmodule.exports = ThresholdClassifier;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/PrecisionRecall.js":"var hash = require(\"./hash\");\nvar sprintf = require('sprintf').sprintf;\nvar _ = require('underscore')._;\n\n/**\n * PrecisionRecall - an object for tracking results of experiments: precision, recall, f1, and execution time.\n * \n * @author Erel Segal-haLevi\n * @since 2013-06\n */\nvar PrecisionRecall = function() {\n\tthis.labels = {}\n\tthis.confusion = {} // only in single label case\n\t\n\tthis.count = 0;\n\tthis.TRUE = 0;\n\tthis.startTime = new Date();\n}\n\nPrecisionRecall.prototype = {\n\t\t\n\t/**\n\t * Record the result of a new binary experiment.\n\t * \n\t * @param expected - the expected result (true/false).\n\t * @param actual   - the actual   result (true/false).\n\t */\n\taddCase: function(expected, actual) {\n\t\tthis.count++;\n\t\tif (expected && actual) this.TP++;\n\t\tif (!expected && actual) this.FP++;\n\t\tif (expected && !actual) this.FN++;\n\t\tif (!expected && !actual) this.TN++;\n\t\tif (expected==actual) this.TRUE++;\n\t},\n\n\t/**\n\t * Record the result of a new classes experiment.\n\t *\n\t * @param expectedClasses - the expected set of classes (as an array or a hash).\n\t * @param actualClasses   - the actual   set of classes (as an array or a hash).\n\t * @param logTruePositives- if true, log the true positives. \n\t * @return an array of explanations \"FALSE POSITIVE\", \"FALSE NEGATIVE\", and maybe also \"TRUE POSITIVE\"\n\t */\n\taddCases: function (expectedClasses, actualClasses, logTruePositives) {\n\t\tvar explanations = [];\n\t\tactualClasses = hash.normalized(actualClasses);\n\t\texpectedClasses = hash.normalized(expectedClasses);\n\n\t\tvar allTrue = true;\n\t\tfor (var actualClass in actualClasses) {\n\t\t\tif (actualClass in expectedClasses) { \n\t\t\t\tif (logTruePositives) explanations.push(\"\\t\\t+++ TRUE POSITIVE: \"+actualClass);\n\t\t\t\tthis.TP++;\n\t\t\t} else {\n\t\t\t\texplanations.push(\"\\t\\t--- FALSE POSITIVE: \"+actualClass);\n\t\t\t\tthis.FP++;\n\t\t\t\tallTrue = false;\n\t\t\t}\n\t\t}\n\t\tfor (var expectedClass in expectedClasses) {\n\t\t\tif (!(expectedClass in actualClasses)) {\n\t\t\t\texplanations.push(\"\\t\\t--- FALSE NEGATIVE: \"+expectedClass);\n\t\t\t\tthis.FN++;\n\t\t\t\tallTrue = false;\n\t\t\t}\n\t\t}\n\t\tif (allTrue) {\n\t\t\tif (logTruePositives) explanations.push(\"\\t\\t*** ALL TRUE!\");\n\t\t\tthis.TRUE++;\n\t\t}\n\t\tthis.count++;\n\t\treturn explanations;\n\t},\n\n/**\n\t * Record the result of a new classes experiment in a hash manner.\n\t * Doesn't allowed to do a inner output, all stats are put in hash\n\t * @param expectedClasses - the expected set of classes (as an array or a hash).\n\t * @param actualClasses   - the actual   set of classes (as an array or a hash).\n\t * @param logTruePositives- if true, log the true positives. \n\t * @return an array of explanations \"FALSE POSITIVE\", \"FALSE NEGATIVE\", and maybe also \"TRUE POSITIVE\"\n     * @author Vasily Konovalov\n\t */\n\n\taddCasesHash: function (expectedClasses, actualClasses, logTruePositives ) {\n\t\tvar explanations = {};\n\t\texplanations['TP'] = []; explanations['FP'] = []; explanations['FN'] = [];\n\n\t\tif (expectedClasses.length == 1)\n\t\t{\n\t\t\tvar expected = expectedClasses[0]\n\t\t\tif (!(expected in this.confusion))\n\t\t\t\t\tthis.confusion[expected] = {}\n\t\t\t_.each(actualClasses, function(actualClass, key, list){\n\t\t\t\tif (!(actualClass in this.confusion[expected]))\n\t\t\t\t\tthis.confusion[expected][actualClass] = 0\n\t\t\t\tthis.confusion[expected][actualClass] +=1\n\t\t\t}, this)\n\t\t}\n\n\t\tactualClasses = hash.normalized(actualClasses);\n\t\texpectedClasses = hash.normalized(expectedClasses);\n\n\t\tvar allTrue = true;\n\t\tfor (var actualClass in actualClasses) {\n\n\t\t\tif (!(actualClass in this.labels)) {\n\t\t\t\tthis.labels[actualClass]={}\n\t\t\t\tthis.labels[actualClass]['TP']=0\n\t\t\t\tthis.labels[actualClass]['FP']=0\n\t\t\t\tthis.labels[actualClass]['FN']=0\n\t\t\t\t}\n\n\t\t\tif (actualClass in expectedClasses) { \n\t\t\t\tif (logTruePositives) explanations['TP'].push(actualClass);\n\t\t\t\tthis.labels[actualClass]['TP'] += 1 \n\t\t\t\t// this.TP++;\n\t\t\t} else {\n\t\t\t\texplanations['FP'].push(actualClass);\n\t\t\t\tthis.labels[actualClass]['FP'] += 1\n\t\t\t\t// this.FP++;\n\t\t\t\tallTrue = false;\n\t\t\t}\n\t\t}\n\t\tfor (var expectedClass in expectedClasses) {\n\n\t\t\tif (!(expectedClass in this.labels)) {\n\t\t\t\tthis.labels[expectedClass]={}\n\t\t\t\tthis.labels[expectedClass]['TP']=0\n\t\t\t\tthis.labels[expectedClass]['FP']=0\n\t\t\t\tthis.labels[expectedClass]['FN']=0\n\t\t\t\t}\n\n\t\t\tif (!(expectedClass in actualClasses)) {\n\t\t\t\texplanations['FN'].push(expectedClass);\n\t\t\t\tthis.labels[expectedClass]['FN'] += 1 \n\t\t\t\t// this.FN++;\n\t\t\t\tallTrue = false;\n\t\t\t}\n\t\t}\n\t\tif (allTrue) {\n\t\t\t// if ((logTruePositives)&& (!only_false_cases)) explanations.push(\"\\t\\t*** ALL TRUE!\");\n\t\t\tthis.TRUE++;\n\t\t}\n\t\tthis.count++;\n\n\t\t_.each(explanations, function(value, key, list){ \n\t\t\t// explanations[key] = _.sortBy(explanations[key], function(num){ num });\n\t\t\texplanations[key].sort()\n\t\t}, this)\n\n\t\tif (explanations['FP'].length == 0)\n\t\t\tdelete explanations['FP']\n\n\t\tif (explanations['FN'].length == 0)\n\t\t\tdelete explanations['FN']\n\n\t\treturn explanations;\n\t},\n\n\tretrieveLabels: function()\n\t{\n\t\t_.each(Object.keys(this.labels), function(label, key, list){ \n\t\t\t\n\t\t\tthis.labels[label]['Recall'] = this.labels[label]['TP'] / (this.labels[label]['TP'] + this.labels[label]['FN']);\n\t\t\tthis.labels[label]['Precision'] = this.labels[label]['TP'] / (this.labels[label]['TP'] + this.labels[label]['FP']);\n\t\t\tthis.labels[label]['F1'] = 2 / (1/this.labels[label]['Recall'] + 1/this.labels[label]['Precision'])\n\n\t\t\tif (!this.labels[label]['F1']) this.labels[label]['F1'] = -1\n\t\t}, this)\n\n\t\tvar arlabels = _.pairs(this.labels) \n\t\tarlabels = _.sortBy(arlabels, function(num){ return arlabels[0] })\n\t\tthis.labels = _.object(arlabels)\n\n\t\treturn this.labels\n\t},\n\n\tcalculateStats: function()\n\t{\n\t\tvar stats = {}\n\t\tvar temp_stats = {}\n\t\t\n\t\tthis.retrieveLabels()\n\n\t\tvar labelsstats = _.values(this.labels)\n\n\t\t_.each(['Precision', 'Recall', 'F1'], function(param, key, list){ \n\t\t\ttemp_stats[param] = _.pluck(labelsstats, param)\n\t\t\t// temp_stats[param] = _.filter(temp_stats[param], function(elem){ return (!_.isNaN(elem) && !_.isNull(elem) && elem>-1)  })\n\t\t\ttemp_stats[param] = _.reduce(temp_stats[param], function(memo, num){ if (!_.isNaN(num) && !_.isNull(num) && num>-1) {return (memo + num)} else return memo }) / temp_stats[param].length\n\t\t})\n\n\t\t_.each(['TP', 'FP', 'FN'], function(param, key, list){ \n\t\t\tstats[param] = _.pluck(labelsstats, param)\n\t\t\tstats[param] = _.reduce(stats[param], function(memo, num){ return memo + num })\n\t\t})\n\n\t\tthis.endTime = new Date();\n\t\tthis.timeMillis = this.endTime-this.startTime;\n\t\tthis.timePerSampleMillis = this.timeMillis / this.count;\n\t\tthis.TP = stats.TP\n\t\tthis.FP = stats.FP\n\t\tthis.FN = stats.FN\n\t\tthis.Accuracy = (this.TRUE) / (this.count);\n\t\tthis.macroPrecision = temp_stats['Precision']\n\t\tthis.macroRecall = temp_stats['Recall']\n\t\tthis.macroF1 = temp_stats['F1']\n\t\tthis.microPrecision = stats.TP / (stats.TP+stats.FP);\n\t\tthis.microRecall = stats.TP / (stats.TP+stats.FN);\n\t\tthis.microF1 = 2 / (1/this.microRecall + 1/this.microPrecision);\n\t\tthis.HammingLoss = (stats.FN+stats.FP) / (stats.FN+stats.TP); // \"the percentage of the wrong labels to the total number of labels\"\n\t\tthis.HammingGain = 1-this.HammingLoss;\n\t\t\n\t\t// this.shortStatsString = sprintf(\"Accuracy=%d/%d=%1.0f%% HammingGain=1-%d/%d=%1.0f%% Precision=%1.0f%% Recall=%1.0f%% F1=%1.0f%% timePerSample=%1.0f[ms]\",\n\t\t\t\t// this.TRUE, this.count, this.Accuracy*100, (this.FN+this.FP), (this.FN+this.TP), this.HammingGain*100, this.Precision*100, this.Recall*100, this.F1*100, this.timePerSampleMillis);\n\t\t\n\t\t// _.each(this.labels, function(st, lab, list){ \n\t\t// \t_.each(st, function(val, par, list){ \n\t\t// \t\tstats[lab+\"_\"+par] = val\n\t\t// \t}, this)\n\t\t// }, this)\n\n\t\t// return stats\n\t}\n}\n\nmodule.exports = PrecisionRecall;\n\n\n// example of usage see in test\n\t// addCasesHashSeq: function (expectedClasses, actualClasses, logTruePositives ) {\n\n\t// \tvar ex = []\n\t// \tvar ac = []\n\t// \tvar matchlist = []\n\n\t// \t// clean up expected list\n\t// \t_.each(expectedClasses, function(expected, key, list){ \n\t// \t\tif ((expected.length == 2) || (expected.length == 3))\n\t// \t\t\tex.push(expected)\n\t// \t}, this)\n\n\t// \t// ac = actualClasses\n\t// \t// // filtering actual classes\t\t\n\t// \t// _.each(actualClasses, function(actual, key, list){ \n\t// \t// \tvar found = _.filter(ac, function(num){ return ((num[0] == actual[0]) && (this.intersection(num[1], actual[1]) == true)) }, this);\n\t// \t// \tif (found.length == 0)\n\t// \t// \t\tac.push(actual)\n\t// \t// }, this)\n\n\t// \t// console.log(JSON.stringify(actualClasses, null, 4))\n\n\t// \t// var ac = this.uniquecandidate(this.uniqueaggregate(actualClasses))\n\t// \tvar ac = actualClasses\n\n\n\t// \t// filling interdependencies between labels \n\t// \t// for every candidate (actual) it looks for intersection between actual labels with different \n\t// \t// intents, intersection means that different intents came to the common substring, then arrange \n\t// \t// all the data in the hash, and mention only keyphrases.\n\n\t// \t_.each(ac, function(actual, key, list){\n\t// \tif (actual.length > 3)\n\t// \t\t{\t \n\t// \t\tlabel = actual[0]\n\t// \t\t// keyphrase\n\t// \t\tstr = actual[2]\n\t// \t\tif (!(label in this.dep))\n\t// \t\t\t{\n\t// \t\t\tthis.dep[label] = {}\n\t// \t\t\tthis.dep[label][label] = []\n\t// \t\t\t}\n\t// \t\tthis.dep[label][label].push(str)\n\n\t// \t\t// intersection, different intents but actual intersection\n\t// \t\tvar found = _.filter(ac, function(num){ return ((num[0] != actual[0]) && (this.intersection(num[1], actual[1]) == true)) }, this);\n\t// \t\t_.each(found, function(sublabel, key, list){\n\t// \t\t\tif (!(sublabel[0] in this.dep[label]))\n\t// \t\t\t\tthis.dep[label][sublabel[0]] = []\n\t// \t\t\tthis.dep[label][sublabel[0]].push([[actual[2],actual[4]], [sublabel[2],sublabel[4]]])\n\t// \t\t}, this)\n\t// \t\t}\n\t// \t}, this)\n\n\t// \tvar explanations = {};\n\t// \texplanations['TP'] = []; explanations['FP'] = []; explanations['FN'] = [];\n\t\t\n\t// \tvar explanations_detail = {};\n\t// \texplanations_detail['TP'] = []; explanations_detail['FP'] = []; explanations_detail['FN'] = [];\n\t\t\n\t// \tvar allTrue = true;\n\t// \tfor (var actualClassindex in ac) {\n\t\t\t\n\t// \t\tif (!(ac[actualClassindex][0] in this.labels)) {\n\t// \t\t\tthis.labels[ac[actualClassindex][0]]={}\n\t// \t\t\tthis.labels[ac[actualClassindex][0]]['TP']=0\n\t// \t\t\tthis.labels[ac[actualClassindex][0]]['FP']=0\n\t// \t\t\tthis.labels[ac[actualClassindex][0]]['FN']=0\n\t// \t\t\t}\n\n\t// \t\tvar found = false\n\t// \t\t_.each(ex, function(exc, key, list){\n\t// \t\t\tif (ac[actualClassindex][0] == exc[0])\n\t// \t\t\t\t{\n\t// \t\t\t\tif ((exc[1].length == 0) || (ac[actualClassindex][1][0] == -1))\n\t// \t\t\t\t\t{\n\t// \t\t\t\t\tfound = true\n\t// \t\t\t\t\tmatchlist.push(ac[actualClassindex])\n\t// \t\t\t\t\t}\n\t// \t\t\t\telse\n\t// \t\t\t\t\t{\n\t// \t\t\t\t\tif (this.intersection(ac[actualClassindex][1], exc[1]))\n\t// \t\t\t\t\t\t{\n\t// \t\t\t\t\t\tfound = true\n\t// \t\t\t\t\t\tmatchlist.push(ac[actualClassindex])\n\t// \t\t\t\t\t\t}\n\t// \t\t\t\t\t}\n\t// \t\t\t\t}\n\t// \t\t}, this)\n\n\t// \t\tif (found) { \n\t// \t\t\tif (logTruePositives)\n\t// \t\t\t\t{\n\t// \t\t\t\t\texplanations['TP'].push(ac[actualClassindex][0]);\n\t// \t\t\t\t\texplanations_detail['TP'].push(ac[actualClassindex]);\n\t// \t\t\t\t\tthis.labels[ac[actualClassindex][0]]['TP'] += 1\n\t// \t\t\t\t\tthis.TP++\n\t// \t\t\t\t}\n\t// \t\t} else {\n\t// \t\t\texplanations['FP'].push(ac[actualClassindex][0]);\n\t// \t\t\texplanations_detail['FP'].push(ac[actualClassindex]);\n\t// \t\t\tthis.labels[ac[actualClassindex][0]]['FP'] += 1\n\t// \t\t\tthis.FP++\n\t// \t\t\tallTrue = false;\n\t// \t\t}\n\t// \t}\n\n\t// \tfor (var expectedClassindex in ex) {\n\t// \t\tvar found = false\n\n\t// \t\tif (!(ex[expectedClassindex][0] in this.labels)) {\n\t// \t\t\tthis.labels[ex[expectedClassindex][0]]={}\n\t// \t\t\tthis.labels[ex[expectedClassindex][0]]['TP']=0\n\t// \t\t\tthis.labels[ex[expectedClassindex][0]]['FP']=0\n\t// \t\t\tthis.labels[ex[expectedClassindex][0]]['FN']=0\n\t// \t\t\t}\n\n\t// \t\t_.each(ac, function(acc, key, list){ \n\t// \t\t\tif (ex[expectedClassindex][0] == acc[0])\n\t// \t\t\t\t{\n\t// \t\t\t\t\tif ((ex[expectedClassindex][1].length == 0) || (acc[1][0] == -1))\n\t// \t\t\t\t\t\tfound = true\n\t// \t\t\t\t\telse\n\t// \t\t\t\t\t\t{\n\t// \t\t\t\t\t\tif (this.intersection(ex[expectedClassindex][1], acc[1]))\n\t// \t\t\t\t\t\t\tfound = true\n\t// \t\t\t\t\t\t}\n\t// \t\t\t\t}\n\t// \t\t}, this)\n\n\t// \t\tif (!found)\n\t// \t\t\t{\n\t// \t\t\texplanations['FN'].push(ex[expectedClassindex][0]);\n\t// \t\t\texplanations_detail['FN'].push(ex[expectedClassindex]);\n\t// \t\t\tthis.labels[ex[expectedClassindex][0]]['FN'] += 1\n\t// \t\t\tthis.FN++;\n\t// \t\t\tallTrue = false;\n\t// \t\t\t}\n\t// \t}\n\n\t// \tif (allTrue) {\n\t// \t\t// if ((logTruePositives)&& (!only_false_cases)) explanations.push(\"\\t\\t*** ALL TRUE!\");\n\t// \t\tthis.TRUE++;\n\t// \t}\n\t// \tthis.count++;\n\n\t// \t// _.each(explanations, function(value, key, list){ \n\t// \t\t// explanations[key] = _.sortBy(explanations[key], function(num){ num });\n\t// \t\t// explanations[key].sort()\n\t// \t// }, this)\n\n\t// \t// console.log(explanations)\n\t// \t// console.log(matchlist)\n\t\t\n\t// \t// if (expectedClasses.length > 1)\n\t// \t\t// process.exit(0)\n\n\t// \treturn {\n\t// \t\t\t'explanations': explanations,\n\t// \t\t\t'match': matchlist,\n\t// \t\t\t'explanations_detail': explanations_detail\n\t// \t\t\t}\n\t// },\n\n\t// simple intersection\n\t// intersection:function(begin, end)\n\t// {\n\t// \tif ((begin[0]<=end[0])&&(begin[1]>=end[0]))\n\t// \t\treturn true\n\t// \tif ((begin[0]>=end[0])&&(begin[0]<=end[1]))\n\t// \t\treturn true\n\t// \treturn false\n\t// },\n\t","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/partitions.js":"/**\n * Utilities for partitioning datasets of documents for training and testing.\n * \n * @author Erel Segal-haLevi\n * @since 2013-06\n */\nvar _ = require(\"underscore\")._;\n\n\n/**\n * Create a single partition of the given dataset.\n *\n * @param dataset an array.\n * @param testSetStart an index into the array.\n * @param testSetCount int - the num of samples in the test set, starting from testSetStart.\n * @return an object {train: trainSet, test: testSet}s\n */\nexports.partition = function(dataset, testSetStart, testSetCount) {\n\t\tvar datasetclone = JSON.parse(JSON.stringify(dataset));\n\t\tvar testSet = datasetclone.splice(testSetStart, testSetCount);\n\t\tvar trainSet = datasetclone; // without the test-set\n\t\treturn {train: trainSet, test: testSet};\n}\n\n/**\n * Create several different partitions of the given dataset to train and test.\n * Useful for cross-validation. \n * \n * @param dataset any array.\n * @param numOfPartitions number of different partitions to generate.\n * @param callback a function to call for each partition.\n * \n * @return an object: {train: [array-for-train], test: [array-for-test]}\n * @note code adapted from Heather Arthur:  https://github.com/harthur/classifier/blob/master/test/cross-validation/cross-validate.js\n */\nexports.partitions = function(dataset, numOfPartitions, callback) {\n\tvar shuffledDataset = _.shuffle(dataset);\n\tvar testSetCount = dataset.length / numOfPartitions;\n\t\n\tfor (var iPartition=0; iPartition<numOfPartitions; ++iPartition) {\n\t\tvar testSetStart = iPartition*testSetCount;\n\t\tvar partition = exports.partition(dataset, testSetStart, testSetCount);\n\t\tcallback(partition.train, partition.test, iPartition);\n\t}\n}\n\n/**\n * Create several different partitions of the given dataset to train and test without doing shuffling\n * Useful for cross-validation in Threshold classifier.\n * \n*/\n\nexports.partitions_consistent_by_fold = function(dataset, numOfPartitions, partitionIndex) {\n\n\tif (!_.isArray(dataset))\n\t\tthrow new Error(\"dataset is not an array\")\n\n\tif (_.isUndefined(numOfPartitions))\n\t\tthrow new Error(\"numOfPartitions \"+ numOfPartitions)\n\n\tif (_.isUndefined(partitionIndex))\n\t\tthrow new Error(\"partitionIndex \"+ partitionIndex)\n\n\tvar testSetCount = dataset.length / numOfPartitions;\n\n\tvar result = {'train': [], 'test': []}\n\t\n\tfor (var iPartition=0; iPartition<numOfPartitions; ++iPartition) {\n\t\tvar testSetStart = iPartition*testSetCount;\n\t\tvar partition = exports.partition(dataset, testSetStart, testSetCount);\n\n\t\tif (iPartition == partitionIndex)\n\t\t\t{\n\t\t\t\tresult['train'] = partition.train\n\t\t\t\tresult['test'] = partition.test\n\t\t\t}\n\t}\n\treturn result\n}\n\nexports.partitions_consistent = function(dataset, numOfPartitions, callback) {\n\tvar testSetCount = dataset.length / numOfPartitions;\n\t\n\tfor (var iPartition=0; iPartition<numOfPartitions; ++iPartition) {\n\t\tvar testSetStart = iPartition*testSetCount;\n\t\tvar partition = exports.partition(dataset, testSetStart, testSetCount);\n\t\tcallback(partition.train, partition.test, iPartition);\n\t}\n}\n\nexports.partitions_reverese = function(dataset, numOfPartitions, callback) {\n\tvar testSetCount = dataset.length / numOfPartitions;\n\t\n\tfor (var iPartition=0; iPartition<numOfPartitions; ++iPartition) {\n\t\tvar testSetStart = iPartition*testSetCount;\n\t\tvar partition = exports.partition(dataset, testSetStart, testSetCount);\n\t\tcallback(partition.test, partition.train, iPartition);\n\t}\n}\n\n\nexports.partitions_hash = function(datasetor, numOfPartitions, callback) {\n\n\tvar count = datasetor[Object.keys(datasetor)[0]].length\n\tvar testSetCount = Math.floor(count / numOfPartitions)\n\n\tfor (var iPartition=0; iPartition<numOfPartitions; ++iPartition) {\n\t\tvar testSetStart = iPartition*testSetCount;\n\n\t\tvar dataset = JSON.parse(JSON.stringify(datasetor))\n\n\t\tvar test = []\n\t\tvar train = []\n\n\t\t_(count - testSetCount).times(function(n){ train.push([]) })\n\t\t\n\t\t_.each(dataset, function(value, key, list){ \n\t\t\ttest = test.concat(value.splice(testSetStart, testSetCount))\n\t\t\t_.each(value, function(elem, key1, list1){ \n\t\t\t\ttrain[key1].push(elem)\n\t\t\t}, this)\n\t\t}, this)\n\t\n\n\t\tcallback(train, test, iPartition);\n\t}\n}\n\n\nexports.partitions_hash_fold = function(datasetor, numOfPartitions, fold ) {\n\n\tvar count = datasetor[Object.keys(datasetor)[0]].length\n\tvar testSetCount = Math.floor(count / numOfPartitions)\n\n\tvar testSetStart = fold*testSetCount;\n\t// var dataset = JSON.parse(JSON.stringify(datasetor))\n\n\tvar test = []\n\tvar train = []\n\n\t_(count - testSetCount).times(function(n){ train.push([]) })\n\t\t\n\t_.each(datasetor, function(value, key, list){ \n\t\ttest = test.concat(value.splice(testSetStart, testSetCount))\n\t\t_.each(value, function(elem, key1, list1){ \n\t\t\ttrain[key1].push(elem)\n\t\t}, this)\n\t}, this)\n\t\n\treturn {\"train\": train, \"test\": test}\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/list.js":"/**\n * Utilities for lists\n * \n * @author Vasily Konovalov\n */\nvar _ = require('underscore');\n\n// Calculating the median of an array basically involves sorting the array and picking the middle number. \n// If it’s an even amount of numbers you take the two numbers in the middle and average them.\nexports.median =  function(values) {\n \t    values.sort(function(a,b) {return a - b;} );\n\t    var half = Math.floor(values.length/2);\n\t    if(values.length % 2)\n\t        return values[half];\n\t    else\n\t        return (values[half-1] + values[half]) / 2.0;\n\t}\n\nexports.variance = function(list)\n\t{\n\t\tsum = _.reduce(list, function(memo, num){ return memo + num; }, 0);\n\t\texp = sum/list.length\n\t\tsum2 = _.reduce(list, function(memo, num){ return memo + num*num; }, 0);\n\t\texp2 = sum2/list.length\n\t\treturn exp2-exp*exp\n\t}\n\nexports.average = function(list)\n\t{\n\t\tsum = _.reduce(list, function(memo, num){ return memo + num; }, 0);\n\t\treturn sum/list.length\n\t}\n\n// @input - list \n// @output - embedded list\nexports.listembed = function(label)\n\t{\n\t\tif ((label === null) || (label == undefined) || (typeof label == 'undefined'))\n\t\t\treturn [[]]\n\t\t// if (typeof label != 'undefined')\n\t\t// else\n\t\t// {\n\t\tif ((_.isObject(label))&&!(_.isArray(label)))\n\t\t// if ('classes' in JSON.parse(label))\n\t\tif ('classes' in label)\n\t\t\tlabel = label.classes\n\n\t\tif (!(label[0] instanceof Array))\n\t\t\treturn [label]\n\t\telse \n\t\t\treturn label\n\t\t// }\n\t\t// else\n\t\t// {\n\t\t\t// return [label]\n\t\t// }\n\t}\n\nexports.clonedataset = function(set)\n\t{\n\tset1 = []\n\t_.each(set, function(value, key, list){\n\t\tset1.push(_.clone(value))\n\t\t})\n\treturn set1\n\t}\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/Adaboost.js":"var hash = require(\"../../utils/hash\");\nvar sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar fs = require('fs');\nvar partitions = require('../../utils/partitions');\nvar crypto = require('crypto')\nvar execSync = require('child_process').execSync\n/**\n * Adaptive Boosting (Adaboost) is a greedy search for a linear combination of \n * classifiers by overweighting the examples that are misclassified by each \n * classifier. icsiboost implements Adaboost over stumps (one-level decision trees) \n * on discrete and continuous attributes (words and real values). \n * See http://en.wikipedia.org/wiki/AdaBoost and the papers by Y. Freund and R. Schapire for more details.\n * \n * @param opts\n *            ngram_length (optional) \n *            iterations (optional) \n *  \n * The class uses icsiboost open-source implementation of Boostexter\n * https://code.google.com/p/icsiboost/\n */\n\nvar Adaboost = function(opts) {\n\tif (!Adaboost.isInstalled()) {\n\t\tvar msg = \"Cannot find the executable 'icsiboost'.\";\n\t\tconsole.error(msg)\n\t\tthrow new Error(msg); \n\t}\n\n\tthis.set_of_labels = []\n\tthis.text_expert = 'ngram'\n\tthis.assigner = crypto.randomBytes(20).toString('hex');\n\tthis.folder = \"icsiboost_data\"\n\n\tthis.ngram_length = opts.ngram_length || 2\n\tthis.iterations = opts.iterations || 2000\n}\n\nAdaboost.isInstalled = function() {\n    try {\n        var result = execSync(\"icsiboost\");\n        return true;\n    } catch (err) {\n        return false;\n    }\n}\n\nAdaboost.prototype = {\n\n\ttrainOnline: function(sample, labels) {\n\t\n\t},\n\n\ttrainBatch : function(dataset) {\n\n\t\tset_of_labels = []\n\t\t_.times(1, function(){dataset = _.shuffle(dataset)})\n\t\t_.each(dataset, function(value, key, list){ \n\t\t\t_.each(value['output'], function(value1, key, list){\n\t\t\t\tset_of_labels.push(this.stringifyClass(value1))\n\t\t\t},this)\n\t\t}, this);\n\n\t\tthis.set_of_labels = _.uniq(set_of_labels)\n\n\t\tif (this.set_of_labels.length == 1) {return 0}\n\n\t\tdataset = _.map(dataset, function(value){ \n\t\t\tvalues = []\n\t\t\t\t_.each(value['output'], function(value1, key, list){ \n\t\t\t\tvalues.push(this.set_of_labels.indexOf(this.stringifyClass(value1))+1)\n\t\t\t}, this);\n\n\t\t\treturn {'input':value['input'], 'output': values}\n\t\t}, this);\n\t\n\t\tar = []\n\t\t_.times(this.set_of_labels.length, function(n){ar.push(n+1)})\n\n  \t\ttry {(!fs.statSync(this.folder).isDirectory())}\n  \t\tcatch(e) {fs.mkdirSync(this.folder)}\n  \t\t\t\n\t\tnames = ar.join()+\".\\nsentence:text:expert_type=\"+this.text_expert+\" expert_length=\"+this.ngram_length+\".\"\n\t \tfs.writeFileSync(\"./\"+this.folder+\"/\"+this.assigner+'.names', names)\n\n\t\tset = {}\n\t\tdataset = partitions.partition(dataset, 1, Math.round(dataset.length*0.3))\n\t\tset['data'] = dataset['train']\n\t\tset['dev']  = dataset['test']\t\n\n\t\t_.each(set, function(valueset, key1, list){ \n\t\t\tstr = \"\"\n\t\t\t_.each(valueset, function(value, key, list){\n\t\t\t\t\tif (value['input'].length <= 1) return\n\t    \t\t\tstr += value['input'].replace(/\\,/g,'') + ',' + value['output'].join(\" \")+ \".\\n\"\n\t    \t\t\t//str += value['input']+ ',' + value['output'].join(\" \")+ \".\\n\"\n\t    \t\t})   \n\n\t\t\tfs.writeFileSync(\"./\"+this.folder+\"/\"+this.assigner+\".\"+key1, str)\n\t\t}, this)\n\n\t\tvar result = execSync(\"icsiboost -S ./\"+this.folder+\"/\"+this.assigner+\" -n \"+this.iterations)\n\t\tconsole.log(result)\n\t},\n\n\tclassify: function(sample, explain) {\n\n\t\tif (this.set_of_labels.length == 1) {return this.set_of_labels[0]}\n\n\t\tfs.writeFileSync(\"./\"+this.folder+\"/\"+this.assigner+\".test\", sample.replace(/\\,/g,'')+\"\\n\")\n\t\tfs.writeFileSync(\"./\"+this.folder+\"/\"+this.assigner+\".test\", sample+\"\\n\")\n\t\tvar result = execSync(\"icsiboost -S ./\"+this.folder+\"/\"+this.assigner +\" -W \"+this.ngram_length+\" -N \"+this.text_expert+\" -C < ./\"+this.folder+\"/\"+this.assigner+\".test > ./\"+this.folder+\"/\"+this.assigner+\".output\")\n\t\tvar stats = fs.readFileSync(\"./\"+this.folder+\"/\"+this.assigner+\".output\", \"utf8\");\n\n\t\tset_of_labels = this.set_of_labels\n\n\t\tstats = stats.replace(/^\\s+|\\s+$/g, \"\");\n\t\tar = stats.split(\" \")\n\n\t\tactual = ar.slice(ar.length/2, ar.length)\n\n\t\tactual1 = []\n\n\t\t_.each(actual, function(value, key){ \n\t\t\tif (value>0) {\n\t\t\t\tactual1.push(set_of_labels[key])}\n\t\t\t})\n\t\n\t\treturn actual1\n\t},\n\t\n\tgetAllClasses: function() {\n\t},\n\n\tstringifyClass: function (aClass) {\n\t\treturn (_(aClass).isString()? aClass: JSON.stringify(aClass));\n\t},\n\n\ttoJSON : function() {\n\t},\n\n\tfromJSON : function(json) {\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\n\t},\n}\n\n\nmodule.exports = Adaboost;\n\n// ./icsiboost  -C  -W 3 -N ngram  -S agent < agent.test\n// ./icsiboost  -S agent -n 1500\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/PartialClassification.js":"var sprintf = require(\"sprintf\").sprintf;\nvar _ = require(\"underscore\")._;\nvar multilabelutils = require('./multilabelutils');\n\n/**\n *  PartialClassification is a test classifier that learns and classifies the components\n * of the labels separately according to the splitLabel routine. One of the examples could be \n * classifying intent, attribute, value separately by three different classifiers.\n * When performing test by trainAndTest module, there is a check for toFormat routine, if it exists\n * then pretest format converting occurs.\n *\n * @author Vasily Konovalov\n * @since March 2014\n */\n\nvar PartialClassification = function(opts) {\n\t\n\topts = opts || {};\n\tif (!opts.multilabelClassifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.multilabelClassifierType is null\");\n\t}\n\n\tif (!opts.numberofclassifiers) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts.numberofclassifiers is null\");\n\t}\n\n\t// this.splitLabel = opts.splitLabel || function(label)      {return label.split(/@/);}\n\tthis.classifier = this.intializeClassifiers(opts.numberofclassifiers, opts.multilabelClassifierType)\n}\n\nPartialClassification.prototype = {\n\n\tintializeClassifiers: function(numberofclassifiers, multilabelClassifierType) {\n\t\tclassifier = []\n\t_(numberofclassifiers).times(function(n){ \n\t\tclassif = new multilabelClassifierType;\n\t\tclassifier.push(classif);\n \t});\n \treturn classifier\n\t},\n\n\ttrainOnline: function(sample, labels) {\n\t\tthrow new Error(\"PartialClassification does not support online training\");\n\t},\n\n\ttrainBatch : function(dataset) {\n\t\tnum_of_classifiers = 0\n\n\t\t_.each(dataset, function(value, key, list){\n\t\t\tnum_of_classifiers =  Math.max(num_of_classifiers, (value['output']).length)\n\t\t}, this);\n\n\t\t\t\n\t\t_(num_of_classifiers).times(function(n){\n\t\t\tdata = []\n\t\t\t_.each(dataset, function(value, key, list){\n\t\t\t\tif (value.output.length - 1 >= n)\n\t\t\t\t\t{\n\t\t\t\t\t\tvalue1 = _.clone(value)\n\t\t\t\t\t\tvalue1['output'] = value.output[n]\n\t\t\t\t\t\tdata.push(value1)\n\t\t\t\t\t}\n\n\t\t\t },this)\n\n\t\t\t// classifier = new this.multilabelClassifierType();\n\t\t\tthis.classifier[n].trainBatch(data)\n\t\t\t// classifier.trainBatch(data)\n\t\t\t// this.classifier.push(classifier)\n\t\t\t}, this)\n\t\n\t},\n\n\tclassify: function(sample, explain, continuous_output) {\n\t\t\t\t\n\t\tvar labels = []\n\t\tvar explanation = []\n\t\tvar scores = {}\n\t\t\n\t \t_.each(this.classifier, function(classif, key, list){\n\t \t\tvar value = classif.classify(sample, explain, continuous_output)\n\t \t \tif (explain>0)\n\t \t \t\tlabels.push(value.classes)\n\t \t \telse\n \t\t\t\tlabels.push(value)\n\t \t \texplanation.push(value.explanation)\n\t \t \tscores = _.extend(scores, value.scores)\n\t \t})\n\n\t\tif (explain>0)\n\t\t\t{\n\t\t\t\tvar positive = {}\n\t\t\t\tvar negative = {}\n\n\t\t\t\t_.each(_.pluck(explanation, 'positive'), function(value, key, list){ \n\t\t\t\t\tpositive = _.extend(positive, value)\n\t\t\t\t\t}, this)\n\n\t\t\t\t_.each(_.pluck(explanation, 'negative'), function(value, key, list){ \n\t\t\t\t\tnegative = _.extend(negative, value)\n\t\t\t\t\t}, this)\n\t\t\t\n\n\t\t\tif (_.keys(negative)!=0)\n\t\t\t\texplanation = {\n\t\t\t\t\tpositive: positive, \n\t\t\t\t\tnegative: negative,\n\t\t\t\t}\n\t\t\t}\n\n\t\treturn (explain>0?\n\t\t\t{\n\t\t\t\tclasses: labels, \n\t\t\t\tscores: scores,\n\t\t\t\texplanation: explanation\n\t\t\t}:\n\t\t\tlabels);\n\n\t\t\n\t\t// console.log(JSON.stringify(explanation, null, 4))\n\t\t// return (explain>0?\n\t\t// \t{\n\t\t// \t\tclasses: labels, \n\t\t// \t\tscores: scores,\n\t\t// \t\texplanation: explanation\n\t\t// \t}:\n\t\t// \tlabels);\n\n \t},\n\n\n \tsetFeatureLookupTable: function(featureLookupTable) {\n \t\t_.each(this.classifier, function(classif, key, list){\n\t \t\tclassif.setFeatureLookupTable(featureLookupTable)\n\t \t})\n \t},\n\t\n\tgetAllClasses: function() {\n\t\tthrow new Error(\"No implementation in PartialClassification\");\n\t},\n\n\ttoJSON : function() {\n\t\tthrow new Error(\"No implementation in PartialClassification\");\n\t},\n\n\tfromJSON : function(json) {\n\t\tthrow new Error(\"No implementation in PartialClassification\");\n\t},\n\t\n}\n\nmodule.exports = PartialClassification;\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/EnhancedClassifier.js":"/*\nTODO: SpellChecker should be reorganized\n*/\n\nvar ftrs = require('../features');\nvar _ = require('underscore')._;\nvar hash = require('../utils/hash');\nvar util = require('../utils/list');\nvar multilabelutils = require('./multilabel/multilabelutils');\n\n/**\n * EnhancedClassifier - wraps any classifier with feature-extractors and feature-lookup-tables.\n * \n * @param opts\n * Obligatory option: 'classifierType', which is the base type of the classifier.\n * Optional:\n * * 'inputSplitter' - a function that splits the input samples into sub-samples, for multi-label classification (useful mainly for sentences). \n * * 'normalizer' - a function that normalizes the input samples, before they are sent to feature extraction.\n * * 'featureExtractor' - a single feature-extractor (see the \"features\" folder), or an array of extractors, for extracting features from training and classification samples.\n * * 'featureExtractorForClassification' - additional feature extractor[s], for extracting features from samples during classification. Used for domain adaptation.\n * * 'featureLookupTable' - an instance of FeatureLookupTable for converting features (in the input) to numeric indices and back.\n * * 'labelLookupTable' - an instance of FeatureLookupTable for converting labels (classes, in the output) to numeric indices and back.\n * * 'multiplyFeaturesByIDF' - boolean - if true, multiply each feature value by log(documentCount / (1+featureDocumentFrequency))\n * * 'minFeatureDocumentFrequency' - int - if positive, ignore features that appeared less than this number in the training set.\n * * 'pastTrainingSamples' - an array that keeps all past training samples, to enable retraining.\n * * 'spellChecker' - an initialized spell checker from the 'wordsworth' package, to spell-check features during classification.\n * * 'bias' - a 'bias' feature with a constant value (usually 1).\n * * 'InputSplitLabel' - a method for special separation of input labels before training\n * * 'OutputSplitLabel' - a method for special separation of output labesl after classification.\n * * 'TestSplitLabel' - a method for special separation before a testing\n * * 'TfIdfImpl' - implementation of tf-idf algorithm\n * * 'tokenizer' - implementation of tokenizer\n * * 'instanceFilter' - filter of instance of training data and test data, if training instance is filtered is not used for training, if triaging instance is filtered by classify,\n it's classified empty class.\n*/\n\nvar EnhancedClassifier = function(opts) {\n\tif (!opts.classifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain classifierType\");\n\t}\n\n\tthis.classifier = new opts.classifierType();\n\n\tthis.inputSplitter = opts.inputSplitter;\n\tthis.setNormalizer(opts.normalizer);\n\tthis.setFeatureExtractor(opts.featureExtractor);\n\tthis.setFeatureExtractorForClassification(opts.featureExtractorForClassification);\n\tthis.setFeatureLookupTable(opts.featureLookupTable);\n\tthis.setLabelLookupTable(opts.labelLookupTable);\n\n\tthis.multiplyFeaturesByIDF = opts.multiplyFeaturesByIDF;\n\tthis.minFeatureDocumentFrequency = opts.minFeatureDocumentFrequency || 0;\n\tif (opts.multiplyFeaturesByIDF||opts.minFeatureDocumentFrequency) \n\t\t{\n    \tthis.tfidf = new opts.TfIdfImpl\n\t\tthis.featureDocumentFrequency = {};\n\t\t}\n\tthis.bias = opts.bias;\n\n\tthis.spellChecker = opts.spellChecker;\n\tthis.tokenizer = opts.tokenizer;\n\tthis.instanceFilterRule = opts.instanceFilter\n\n\t// this.spellChecker =  [require('wordsworth').getInstance(), require('wordsworth').getInstance()],\n\t// this.pastTrainingSamples = opts.pastTrainingSamples;\n\t// TODO: it looks like the method with creating an array at the definition \n\t// create an array with the same pointer for every classifier of the given class\n\t\n\tthis.pastTrainingSamples = []\n\n\tthis.InputSplitLabel = opts.InputSplitLabel\n\tthis.OutputSplitLabel = opts.OutputSplitLabel\n\tthis.TestSplitLabel = opts.TestSplitLabel\n}\n\n\nEnhancedClassifier.prototype = {\n\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetFeatureExtractor: function (featureExtractor) {\n\t\tthis.featureExtractors = ftrs.normalize(featureExtractor);\n\t},\n\t\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetNormalizer: function (normalizer) {\n\t\tif (normalizer)\n\t\t\tthis.normalizers = (Array.isArray(normalizer)? normalizer: [normalizer]);\n\t},\n\n\t/** Set an additional feature extractor, for classification only. */\n\tsetFeatureExtractorForClassification: function (featureExtractorForClassification) {\n\t\tif (featureExtractorForClassification) {\n\t\t\tif (Array.isArray(featureExtractorForClassification)) {\n\t\t\t\tfeatureExtractorForClassification.unshift(this.featureExtractors);\n\t\t\t} else {\n\t\t\t\tfeatureExtractorForClassification = [this.featureExtractors, featureExtractorForClassification];\n\t\t\t}\n\t\t\tthis.featureExtractorsForClassification = new ftrs.CollectionOfExtractors(featureExtractorForClassification);\n\t\t}\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (featureLookupTable) {\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t\tif (this.classifier.setFeatureLookupTable)\n\t\t\t\tthis.classifier.setFeatureLookupTable(featureLookupTable);  // for generating clearer explanations only\n\t\t}\n\t},\n\t\n\tsetLabelLookupTable: function(labelLookupTable) {\n\t\tif (labelLookupTable) {\n\t\t\tthis.labelLookupTable = labelLookupTable;\n\t\t\tif (this.classifier.setLabelLookupTable)\n\t\t\t\tthis.classifier.setLabelLookupTable(labelLookupTable);  // for generating clearer explanations only\n\t\t}\n\t},\n\n\t// private function: use this.normalizers to normalize the given sample:\n\tnormalizedSample: function(sample) {\n\t\tif (!(_.isArray(sample)))\n\t\t{\n\t\t\tif (this.normalizers) {\n\t\t\t\ttry {\n\t\t\t\t\tfor (var i in this.normalizers) {\t\t\t\t\t\n\t\t\t\t\t\tsample = this.normalizers[i](sample);\n\t\t\t\t\t}\n\t\t\t\t} catch (err) {\n\t\t\t\t\tconsole.log(err)\n\t\t\t\t\tthrow new Error(\"Cannot normalize '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn sample;\n\t},\n\n\tsampleToFeatures: function(sample, featureExtractor) {\n\t\tvar features = sample;\n\t\tif (featureExtractor) {\n\t\t\ttry {\n\t\t\t\tfeatures = {};\n\t\t\t\tfeatureExtractor(sample, features);\n\t\t\t} catch (err) {\n\t\t\t\tthrow new Error(\"Cannot extract features from '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t}\n\t\t}\n\n\t\treturn features;\n\t},\n\n\tinstanceFilter: function(data) {\n\t\tif (this.instanceFilterRule) \n\t\t\treturn this.instanceFilterRule(data)\n\t},\n\t\n\ttrainSpellChecker: function(features) {\n\t\tif (this.spellChecker) {\n\t\t\tvar tokens = this.tokenizer.tokenize(features);\n\t\t\t_.each(tokens, function(word, key, list){ \n\t\t\t\tthis.spellChecker[1].understand(word); // Adds the given word to the index of the spell-checker.\n\t\t\t\tthis.spellChecker[1].train(word);\n\t\t\t}, this)\n\t\t}\n\t},\n\t\n\tcorrectFeatureSpelling: function(sample) {\n\t\tif (this.spellChecker) {\n\t\t\tvar features = this.tokenizer.tokenize(sample);\n\t\t\tfor (var index in features) {\n\t\t\t\tvar feature = features[index]\n\t\t\t\tif (!isNaN(parseInt(feature)))  // don't spell-correct numeric features\n\t\t\t\t\t{\n\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif (!(this.spellChecker[1].exists(feature)))\n\t\t\t\t\t{\n\t\t\t\t\t\tif (this.spellChecker[1].suggest(feature).length != 0)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[1].suggest(feature)[0]\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (!(this.spellChecker[0].exists(feature)))\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tif (this.spellChecker[0].suggest(feature).length != 0)\n\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[0].suggest(feature)[0]\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t}\n\t\tsample = features.join(\" \")\n\t\t}\n\t\treturn sample\n\t},\n\t\n\tfeaturesToArray: function(features) {\n\t\tvar array = features;\n\t\tif (this.featureLookupTable) {\n\t\t\tarray = this.featureLookupTable.hashToArray(features);\n\t\t}\n\t\treturn array;\n\t},\n\t\n\tcountFeatures: function(features) {\n\t\tif (this.featureDocumentFrequency) {\n\t\t\t// this.tfidf.addDocument(datum.input);\n\t\t\tfor (var feature in features)\n\t\t\t\tthis.featureDocumentFrequency[feature] = (this.featureDocumentFrequency[feature] || 0)+1;\n\t\t\tthis.documentCount = (this.documentCount||0)+1;\n\t\t}\n\t},\n\t\n\teditFeatureValues: function(features, remove_unknown_features) {\n\n\t\tif (this.multiplyFeaturesByIDF) { \n\t\t\tfor (var feature in features) { \n\t\t\t\tvar IDF = this.tfidf.idf(feature)\n\t\t\t\tif (IDF != Infinity)\n\t\t\t\t\tfeatures[feature] *= IDF\n\t\t\t\telse\n\t\t\t\t\tdelete features[feature]\n\t\t\t}\n\n\t\t\tif (this.bias && !features.bias)\n\t\t\tfeatures.bias = this.bias;\n\n\t\t}\n\t\t// if (remove_unknown_features && this.minFeatureDocumentFrequency>0)\n\t\t\t// for (var feature in features)\n\t\t\t\t// if ((this.featureDocumentFrequency[feature]||0)<this.minFeatureDocumentFrequency)\n\t\t\t\t\t// delete features[feature];\n\t\t\n\t},\n\t\n\n\t/**\n\t * Online training: \n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * @param sample a document.\n\t * @param classes an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, classes) {\n\t\tclasses = normalizeClasses(classes, this.labelLookupTable);\n\t\tsample = this.normalizedSample(sample);\n\t\tvar features = this.sampleToFeatures(sample, this.featureExtractors);\n\t\tthis.countFeatures(features);\n\t\tthis.trainSpellChecker(features);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\tvar array = this.featuresToArray(features);\n\t\tthis.classifier.trainOnline(array, classes);\n\t\tif (this.pastTrainingSamples)\n\t\t\tthis.pastTrainingSamples.push({input: sample, output: classes});\n\t},\n\n\t/**\n\t * Batch training: \n\t * Train the classifier with all the given documents.\n\t * @param dataset an array with objects of the format: {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch: function(dataset) {\n\t\tvar featureLookupTable = this.featureLookupTable;\n\t\tvar pastTrainingSamples = this.pastTrainingSamples;\n\n\t\t\tif (this.spellChecker) {\n\t\t\t\t// var seeds = fs.readFileSync('./node_modules/wordsworth/data/seed.txt')\n\t\t\t\t// var trainings = fs.readFileSync('./node_modules/wordsworth/data/training.txt')\n\t\t\t\tvar seeds = []\n\t\t\t\tvar trainings = []\n\t\t\t\tthis.spellChecker[0].initializeSync(seeds.toString().split(\"\\r\\n\"), trainings.toString().split(\"\\r\\n\"))\n\t\t\t\t}\n\n\t\t\tdataset = dataset.map(function(datum) {\n\n\t\t\t\tif (typeof this.InputSplitLabel === 'function') {\n\t\t\t\t\tdatum.output = (this.InputSplitLabel(multilabelutils.normalizeOutputLabels(datum.output)))\t\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tdatum.output = normalizeClasses(datum.output, this.labelLookupTable);\n\t\t\t\t}\n\n\t\t\t\tif (pastTrainingSamples && dataset!=pastTrainingSamples)\n\t\t\t\t\tpastTrainingSamples.push(datum);\n\t\t\t\tdatum = _(datum).clone();\n\n\t\t\t\tdatum.input = this.normalizedSample(datum.input);\n\n\t\t\t\t/*true - this instance is filtered as not useful*/\n\t\t\t\tif (this.instanceFilter(datum) == true)\n\t\t\t\t\treturn null\n\n\t\t\t\tthis.trainSpellChecker(datum.input);\n\n\t\t\t\tvar features = this.sampleToFeatures(datum.input, this.featureExtractors);\n\t\t\t\t\n\t\t\t\tif (this.tfidf)\n\t\t\t\t\tthis.tfidf.addDocument(features);\n\t\t\t\t// this.trainSpellChecker(features);\n\t\t\t\tif (featureLookupTable)\n\t\t\t\t\tfeatureLookupTable.addFeatures(features);\n\n\t\t\t\tdatum.input = features;\n\t\t\t\treturn datum;\n\t\t\t}, this);\n\n\t\t\tdataset = _.compact(dataset)\n\n\t\tdataset.forEach(function(datum) {\n\t\t\t// run on single sentence\n\t\t\tthis.editFeatureValues(datum.input, /*remove_unknown_features=*/false);\n\t\t\tif (featureLookupTable)\n\t\t\t\tdatum.input = featureLookupTable.hashToArray(datum.input);\n\t\t}, this);\n\n\t\tthis.classifier.trainBatch(dataset);\n\t},\n\n\t/**\n\t * internal function - classify a single segment of the input (used mainly when there is an inputSplitter) \n\t * @param sample a document.\n\t * @return an array whose VALUES are classes.\n\t */\n\tclassifyPart: function(sample, explain, continuous_output) {\n\t\t\n\t\tvar samplecorrected = this.correctFeatureSpelling(sample);\n\t\tvar features = this.sampleToFeatures(samplecorrected, this.featureExtractors);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/true);\n\t\tvar array = this.featuresToArray(features);\n\t\tvar classification = this.classifier.classify(array, explain, continuous_output);\n\t\t\n\t\t// if (this.spellChecker && classification.explanation) {\n\t\t\t// if (Array.isArray(classification.explanation))\n\t\t\t\t// classification.explanation.unshift({SpellCorrectedFeatures: JSON.stringify(features)});\n\t\t\t// else\n\t\t\t\t// classification.explanation['SpellCorrectedFeatures']=JSON.stringify(features);\n\t\t// }\n\t\treturn classification;\n\t},\n\n\toutputToFormat: function(data) {\n\t\tdataset = util.clonedataset(data)\n\t\tdataset = dataset.map(function(datum) {\n\t\tvar normalizedLabels = multilabelutils.normalizeOutputLabels(datum.output);\n\t\treturn {\n\t\t\tinput: datum.input,\n\t\t\toutput: this.TestSplitLabel(normalizedLabels)\n\t\t}\n\t\t}, this);\n\t\treturn dataset\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * @param sample a document.\n\t * @return an array whose VALUES are classes.\n\t * @original is the original gold standard labels is used only for statistics.\n\t */\n\tclassify: function(sample, explain, continuous_output, original, classifier_compare) {\n\t\tvar initial = sample\n\t\tsample = this.normalizedSample(sample)\n\n\t\tif (this.instanceFilter(sample))\n\t\t\t{\tif (explain>0) \n\t\t\t\treturn {\n\t\t\t\t\tclasses: [],\n\t\t\t\t\tscores: {},\n\t\t\t\t\texplanation: {} \n\t\t\t\t\t// bonus: bonus\n\t\t\t\t};\n\t\t\telse\n\t\t\t\treturn []\n\t\t\t}\t\t\n\t\t\n\t\tif (!this.inputSplitter) {\n\t\t\tvar classesWithExplanation = this.classifyPart(sample, explain, continuous_output);\n\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\tvar scores =  (continuous_output? classesWithExplanation.scores: null)\n\t\t\tvar explanations = (explain>0? classesWithExplanation.explanation: null);\n\t\t} else {\n\t\t\tvar parts = this.inputSplitter(sample);\n\t\t\t// var accumulatedClasses = {};\n\t\t\tvar accumulatedClasses = [];\n\t\t\tvar explanations = [];\n\t\t\tparts.forEach(function(part) {\n\t\t\t\tif (part.length==0) return;\n\t\t\t\tvar classesWithExplanation = this.classifyPart(part, explain, continuous_output);\n\t\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\t\t// for (var i in classes)\n\t\t\t\t// \taccumulatedClasses[classes[i]]=true;\n\t\t\t\taccumulatedClasses.push(classes)\n\t\t\t\tif (explain>0) {\n\t\t\t\t\t// explanations.push(part);\n\t\t\t\t\texplanations.push(classesWithExplanation.explanation);\n\t\t\t\t}\n\t\t\t}, this);\n    \t\tclasses = []\n    \t\tif (accumulatedClasses[0])\n    \t\t{\n\t\t\tif (accumulatedClasses[0][0] instanceof Array)\n\t\t\t\t_(accumulatedClasses[0].length).times(function(n){\n\t\t\t\t\tclasses.push(_.flatten(_.pluck(accumulatedClasses,n)))\n\t\t\t\t });\n\t\t\telse\n\t\t\t{\n\t\t\t\tclasses = _.flatten(accumulatedClasses)\n\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (this.labelLookupTable) {\n\t\t\tif (Array.isArray(classes)) {\n\t\t\t\tclasses = classes.map(function(label) {\n\t\t\t\t\t\tif (_.isArray(label))\n\t\t\t\t\t\t\tlabel[0] = this.labelLookupTable.numberToFeature(label[0]);\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tlabel = this.labelLookupTable.numberToFeature(label);\n\t\t\t\t\t\treturn label;\n\t\t\t\t\t}, this);\n\t\t\t} else {\n\t\t\t\tclasses = this.labelLookupTable.numberToFeature(classes);\n\t\t\t}\n\t\t}\n\n\t\tif ((typeof this.OutputSplitLabel === 'function')) {\n\n\t\t\t// classes = this.OutputSplitLabel(classes, this.Observable, sample, explanations)\n\t\t\t// var classes = []\n\t\t\t// if (_.isArray(explanations))\n\t\t\t// var bonus = []\n\t\t\n\t\t\tif ((explain>0) && (this.inputSplitter))\n\t\t\t\t{ nclasses = []\n\t\t\t\t_(explanations.length).times(function(n){\n\t\t\t\t\tvar clas = this.OutputSplitLabel(classes, this, parts[n], explanations[n], original, classifier_compare, initial)\n\t\t\t\t\tnclasses = nclasses.concat(clas)\n\t\t\t\t}, this)\n\t\t\t\tclasses = nclasses\n\t\t\t\t}\n\t\t\telse\n\t\t\t\t{\n\t\t\t\tvar classes = this.OutputSplitLabel(classes, this, sample, explanations, original, classifier_compare, initial)\n\t\t\t\t}\n\t\t\t}\n\n\t\tif (explain>0) \n\t\t\treturn {\n\t\t\t\tclasses: classes,\n\t\t\t\tscores: scores,\n\t\t\t\texplanation: explanations\n\t\t\t\t// bonus: bonus\n\t\t\t};\n\t\telse\n\t\t\treturn classes;\n\t},\n\n\t\n\t/**\n\t * Train on past training samples\n\t * currently doesn't work\n\t */\n\tretrain: function() {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't retrain\");\n\t\t\n\t\tthis.trainBatch(this.pastTrainingSamples);\n\t},\n\t\n\t/**\n\t * @return an array with all samples whose class is the given class.\n\t * Available only if the pastTrainingSamples are saved.\n\t */\n\tbackClassify: function(theClass) {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't backClassify\");\n\n\t\tif (!(theClass instanceof Array))\n\t\t\ttheClass = [theClass];\n\t\tvar samples = [];\n\t\tthis.pastTrainingSamples.forEach(function(datum) {\n\t\t\tif (_(datum.output).isEqual(theClass))\n\t\t\t\tsamples.push(datum.input);\n\t\t});\n\t\treturn samples;\n\t},\n\n\ttoJSON : function(callback) {\n\t\treturn {\n\t\t\tclassifier: this.classifier.toJSON(callback),\n\t\t\tfeatureLookupTable: (this.featureLookupTable? this.featureLookupTable.toJSON(): undefined),\n\t\t\tlabelLookupTable: (this.labelLookupTable? this.labelLookupTable.toJSON(): undefined),\n\t\t\tspellChecker:  (this.spellChecker? this.spellChecker/*.toJSON()*/: undefined),\n\t\t\tpastTrainingSamples: (this.pastTrainingSamples? this.pastTrainingSamples: undefined),\n\t\t\tfeatureDocumentFrequency: this.featureDocumentFrequency,\n\t\t\tdocumentCount: this.documentCount,\n\t\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not serializable! */ \n\t\t};\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.classifier.fromJSON(json.classifier);\n\t\tif (this.featureLookupTable) {\n\t\t\tthis.featureLookupTable.fromJSON(json.featureLookupTable);\n\t\t\tthis.setFeatureLookupTable(this.featureLookupTable);\n\t\t}\n\t\tif (this.labelLookupTable) {\n\t\t\tthis.labelLookupTable.fromJSON(json.labelLookupTable);\n\t\t\tthis.setLabelLookupTable(this.labelLookupTable);\n\t\t}\n\t\tif (this.spellChecker) this.spellChecker = json.spellChecker; \n\t\tif (this.pastTrainingSamples) this.pastTrainingSamples = json.pastTrainingSamples;\n\t\tthis.featureDocumentFrequency = json.featureDocumentFrequency;\n\t\tthis.documentCount = json.documentCount;\n\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not deserializable! */ \n\t},\n\n\tgetAllClasses: function() {  // relevant for multilabel classifiers\n\t\treturn this.classifier.getAllClasses();\n\t},\n}  // end of EnhancedClassifier prototype\n\n\nvar stringifyClass = function (aClass) {\n\treturn (_(aClass).isString()? aClass: JSON.stringify(aClass));\n}\n\nvar normalizeClasses = function (classes, labelLookupTable) {\n\tif (!Array.isArray(classes))\n\t\tclasses = [classes];\n\tclasses = classes.map(stringifyClass);\n\tif (labelLookupTable)\n\t\tclasses = classes.map(labelLookupTable.featureToNumber, labelLookupTable);\n\tclasses.sort();\n\treturn classes;\n}\n\nmodule.exports = EnhancedClassifier;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/formats/index.js":"module.exports = {\n\tarff: require(\"./arff\"),\n\tjson: require(\"./json\"),\n\ttsv: require(\"./tsv\"),\n\tsvmlight: require(\"./svmlight\"),\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/formats/arff.js":"/**\n * Static Utilities for writing files in ARFF format - the format used by WEKA.\n *\n * @note for READING files in ARFF format, see https://github.com/chesles/node-arff\n * \n * @author Erel Segal-Halevi\n * @since 2013-08\n */\n\n\nvar _ = require('underscore')._;\nvar FeaturesUnit = require('../features');\n\n/**\n * convert a single dataset to Weka ARFF string.\n * @param dataset an array of samples in the format {input: {feature1: xxx, feature2: yyy, ...}, output: [1,2,3]}\n * @param relationName string for the @relation on the top of the file.\n * @param featureExtractor [optional]\n * @return an ARFF string. \n */\nexports.toARFF = function(dataset, relationName, featureExtractor) {\n\tif (!featureExtractor) featureExtractor=_.identity;\n\t\n\tvar featureLookupTable = new FeaturesUnit.FeatureLookupTable();\n\t\n\t// Extract the input attributes (- features):\n\tdataset.forEach(function(datum) {\n\t\tdatum.input = featureExtractor(datum.input, {});\n\t\tif (!_.isObject(datum.input))\n\t\t\tthrow new Error(\"Expected feature vector to be a hash, but found \"+JSON.stringify(datum.input));\n\t\tfeatureLookupTable.addFeatures(datum.input);\n\t});\n\t\n\t// Extract the target attributes (- classes):\n\tdataset.forEach(function(datum) {\n\t\tif (!_.isArray(datum.output))\n\t\t\tdatum.output = [datum.output];\n\t\tdatum.output = datum.output.map(function(anOutput) {\n\t\t\treturn _.isString(anOutput)? anOutput: JSON.stringify(anOutput);\n\t\t});\n\t\tfeatureLookupTable.addFeatures(datum.output);\n\t});\n\n\t//console.dir(featureLookupTable);\n\treturn toARFFLocal(dataset, relationName, featureLookupTable);\n}\n\n/**\n * convert many dataset to Weka ARFF files.\n * @param mapFileNameToDataset an array of samples in the format {input: {feature1: xxx, feature2: yyy, ...}, output: [1,2,3]}\n * @return an ARFF file. \n */\nexports.toARFFs = function(outputFolder, mapFileNameToDataset, featureExtractor) {\n\tif (!featureExtractor) featureExtractor=_.identity;\n\tvar featureLookupTable = new FeaturesUnit.FeatureLookupTable();\n\t\n\t// Extract the input attributes (- features):\n\tfor (var relationName in mapFileNameToDataset) {\n\t\tmapFileNameToDataset[relationName].forEach(function(datum) {\n\t\t\tdatum.input = featureExtractor(datum.input, {});\n\t\t\tif (!_.isObject(datum.input))\n\t\t\t\tthrow new Error(\"Expected feature vector to be a hash, but found \"+JSON.stringify(datum.input));\n\t\t\tfeatureLookupTable.addFeatures(datum.input);\n\t\t});\n\t}\n\t\n\t\n\t// Extract the target attributes (- classes):\n\tfor (var relationName in mapFileNameToDataset) {\n\t\tmapFileNameToDataset[relationName].forEach(function(datum) {\n\t\t\tif (!_.isArray(datum.output))\n\t\t\t\tdatum.output = [datum.output];\n\t\t\tdatum.output = datum.output.map(function(anOutput) {\n\t\t\t\treturn _.isString(anOutput)? anOutput: JSON.stringify(anOutput);\n\t\t\t});\n\t\t\tfeatureLookupTable.addFeatures(datum.output);\n\t\t});\n\t}\n\t\n\n\t//console.dir(featureLookupTable);\n\n\tvar fs = require('fs');\n\tfor (var relationName in mapFileNameToDataset) {\n\t\tfs.writeFileSync(outputFolder+\"/\"+relationName+\".arff\", \n\t\t\ttoARFFLocal(mapFileNameToDataset[relationName], relationName, featureLookupTable));\n\t}\n}\n\n\n/**\n * convert a single dataset to Weka ARFF string.\n * @param dataset an array of samples in the format {input: {feature1: xxx, feature2: yyy, ...}, output: [1,2,3]}\n * @param relationName string for the @relation on the top of the file.\n * @param featureLookupTable maps features to indices\n * @return an ARFF string. \n */\nvar toARFFLocal = function(dataset, relationName, featureLookupTable) {\n\tvar arff = \"% Automatically generated by Node.js\\n\";\n\tarff += \"@relation \"+relationName+\"\\n\";\n\n\tfeatureLookupTable.featureIndexToFeatureName.forEach(function(featureName) {\n\t\tif (_.isUndefined(featureName)) \n\t\t\tarff += \"@attribute undefined {0,1}\"+\"\\n\";\n\t\telse if (!_.isString(featureName))\n\t\t\tthrow new Error(\"Expected featureName to be a string, but found \"+JSON.stringify(featureName));\n\t\telse arff += \"@attribute \"+featureName.replace(/[^a-zA-Z0-9]/g, \"_\")+\" \"+\"{0,1}\"+\"\\n\";\n\t});\n\n\tarff += \"\\n@data\\n\";\n\t\n\tdataset.forEach(function(datum) {\n\t\tvar datumArff = _.clone(datum.input, {});\n\t\tfor (var i=0; i<datum.output.length; ++i)\n\t\t\tdatumArff[datum.output[i]]=1;\n\t\t//console.dir(datumArff);\n\t\tvar array = featureLookupTable.hashToArray(datumArff);\n\t\tarff += array + \"\\n\";\n\t});\n\n\treturn arff;\n};\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/formats/json.js":"/**\n * Small utility for writing a dataset in JSON format.\n *\n * @author Erel Segal-Halevi\n * @since 2013-08\n */\n\n\n/**\n * convert a single dataset to compact JSON format.\n * @param dataset an array of samples in the format {input: {feature1: xxx, feature2: yyy, ...}, output: [1,2,3]}\n */\nexports.toJSON = function(dataset) {\n\tjson = \"[\";\n\tfor (var i=0; i<dataset.length; ++i) {\n\t\tjson += (\n\t\t\t(i>0? \"\\n, \": \"\\n  \")+\n\t\t\tJSON.stringify(dataset[i]));\n\t}\t\n\tjson += \"\\n]\\n\";\n\treturn json;\n}\n\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/formats/tsv.js":"/**\n * Small utility for writing a dataset in tab-separated-values format.\n *\n * @author Erel Segal-Halevi\n * @since 2013-08\n */\n\n\n/**\n * Write the dataset, one sample per line, with the given separator between sample and output. \n */\nexports.toTSV = function(dataset, separator) {\n\tif (!separator) separator=\"\\t\"; \n\tdataset.forEach(function(sample) {\n\t\tconsole.log(JSON.stringify(sample.input)+separator+\"[\"+sample.output+\"]\");\n\t});\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/index.js":"//var trainAndTest = require(\"./trainAndTest\"); \nmodule.exports = {\n\thash: require(\"./hash\"),\n\tpartitions: require(\"./partitions\"),\n\tPrecisionRecall: require(\"./PrecisionRecall\"),\n//\tbars: require(\"./bars.js\"),\n//\ttestLite: trainAndTest.testLite,\n//\ttest: trainAndTest.test,\n//\tcompare: trainAndTest.compare,\n//\ttrainAndTestLite: trainAndTest.trainAndTestLite,\n//\ttrainAndTest: trainAndTest.trainAndTest,\n//\ttrainAndCompare: trainAndTest.trainAndCompare,\n//\tlearningCurve: trainAndTest.learningCurve,\n//\tsplitToEasyAndHard: trainAndTest.splitToEasyAndHard,\n\t\n//\twriteDataset: require(\"./trainAndTest\").writeDataset,\n\thammingDistance: require(\"./hamming\").hammingDistance,\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/hamming.js":"/** \n *\n * Calculate Hamming distance between two sets \n *\n */\n\n\n/**\n * @param a, b - arrays\n * @return number of elements in a-b plus number of elements in b-a\n */\nvar hammingDistance = function(a,b) {\n\tvar d = 0;\n\tfor (var i=0; i<a.length; ++i) {\n\t\tif (b.indexOf(a[i])<0)\n\t\t\td++;\n\t}\n\tfor (var i=0; i<b.length; ++i) {\n\t\tif (a.indexOf(b[i])<0)\n\t\t\td++;\n\t}\n\treturn d;\n};\n\nmodule.exports = {hammingDistance:  hammingDistance};\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/EnhancedClassifier.2014.js":"/*\nTODO: SpellChecker should be reorganized\n*/\n\nvar ftrs = require('../features');\nvar _ = require('underscore')._;\nvar hash = require('../utils/hash');\nvar util = require('../utils/list');\nvar multilabelutils = require('./multilabel/multilabelutils');\n\n/**\n * EnhancedClassifier - wraps any classifier with feature-extractors and feature-lookup-tables.\n * \n * @param opts\n * Obligatory option: 'classifierType', which is the base type of the classifier.\n * Optional:\n * * 'inputSplitter' - a function that splits the input samples into sub-samples, for multi-label classification (useful mainly for sentences). \n * * 'normalizer' - a function that normalizes the input samples, before they are sent to feature extraction.\n * * 'featureExtractor' - a single feature-extractor (see the \"features\" folder), or an array of extractors, for extracting features from training and classification samples.\n * * 'featureExtractorForClassification' - additional feature extractor[s], for extracting features from samples during classification. Used for domain adaptation.\n * * 'featureLookupTable' - an instance of FeatureLookupTable for converting features (in the input) to numeric indices and back.\n * * 'labelLookupTable' - an instance of FeatureLookupTable for converting labels (classes, in the output) to numeric indices and back.\n * * 'multiplyFeaturesByIDF' - boolean - if true, multiply each feature value by log(documentCount / (1+featureDocumentFrequency))\n * * 'minFeatureDocumentFrequency' - int - if positive, ignore features that appeared less than this number in the training set.\n * * 'pastTrainingSamples' - an array that keeps all past training samples, to enable retraining.\n * * 'spellChecker' - an initialized spell checker from the 'wordsworth' package, to spell-check features during classification.\n * * 'bias' - a 'bias' feature with a constant value (usually 1).\n * * 'InputSplitLabel' - a method for special separation of input labels before training\n * * 'OutputSplitLabel' - a method for special separation of output labesl after classification.\n * * 'TestSplitLabel' - a method for special separation before a testing\n * * 'TfIdfImpl' - implementation of tf-idf algorithm\n * * 'tokenizer' - implementation of tokenizer\n * * 'instanceFilter' - filter of instance of training data and test data, if training instance is filtered is not used for training, if triaging instance is filtered by classify,\n it's classified empty class.\n*/\n\nvar EnhancedClassifier = function(opts) {\n\tif (!opts.classifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain classifierType\");\n\t}\n\n\tthis.classifier = new opts.classifierType();\n\n\tthis.inputSplitter = opts.inputSplitter;\n\tthis.setNormalizer(opts.normalizer);\n\tthis.setFeatureExtractor(opts.featureExtractor);\n\tthis.setFeatureExtractorForClassification(opts.featureExtractorForClassification);\n\tthis.setFeatureLookupTable(opts.featureLookupTable);\n\tthis.setLabelLookupTable(opts.labelLookupTable);\n\n\tthis.multiplyFeaturesByIDF = opts.multiplyFeaturesByIDF;\n\tthis.minFeatureDocumentFrequency = opts.minFeatureDocumentFrequency || 0;\n\tif (opts.multiplyFeaturesByIDF||opts.minFeatureDocumentFrequency) \n\t\t{\n    \tthis.tfidf = new opts.TfIdfImpl\n\t\tthis.featureDocumentFrequency = {};\n\t\t}\n\tthis.bias = opts.bias;\n\n\tthis.spellChecker = opts.spellChecker;\n\tthis.tokenizer = opts.tokenizer;\n\tthis.instanceFilterRule = opts.instanceFilter\n\n\t// this.spellChecker =  [require('wordsworth').getInstance(), require('wordsworth').getInstance()],\n\t// this.pastTrainingSamples = opts.pastTrainingSamples;\n\t// TODO: it looks like the method with creating an array at the definition \n\t// create an array with the same pointer for every classifier of the given class\n\t\n\tthis.pastTrainingSamples = []\n\n\tthis.InputSplitLabel = opts.InputSplitLabel\n\tthis.OutputSplitLabel = opts.OutputSplitLabel\n\tthis.TestSplitLabel = opts.TestSplitLabel\n}\n\n\nEnhancedClassifier.prototype = {\n\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetFeatureExtractor: function (featureExtractor) {\n\t\tthis.featureExtractors = ftrs.normalize(featureExtractor);\n\t},\n\t\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetNormalizer: function (normalizer) {\n\t\tif (normalizer)\n\t\t\tthis.normalizers = (Array.isArray(normalizer)? normalizer: [normalizer]);\n\t},\n\n\t/** Set an additional feature extractor, for classification only. */\n\tsetFeatureExtractorForClassification: function (featureExtractorForClassification) {\n\t\tif (featureExtractorForClassification) {\n\t\t\tif (Array.isArray(featureExtractorForClassification)) {\n\t\t\t\tfeatureExtractorForClassification.unshift(this.featureExtractors);\n\t\t\t} else {\n\t\t\t\tfeatureExtractorForClassification = [this.featureExtractors, featureExtractorForClassification];\n\t\t\t}\n\t\t\tthis.featureExtractorsForClassification = new ftrs.CollectionOfExtractors(featureExtractorForClassification);\n\t\t}\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (featureLookupTable) {\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t\tif (this.classifier.setFeatureLookupTable)\n\t\t\t\tthis.classifier.setFeatureLookupTable(featureLookupTable);  // for generating clearer explanations only\n\t\t}\n\t},\n\t\n\tsetLabelLookupTable: function(labelLookupTable) {\n\t\tif (labelLookupTable) {\n\t\t\tthis.labelLookupTable = labelLookupTable;\n\t\t\tif (this.classifier.setLabelLookupTable)\n\t\t\t\tthis.classifier.setLabelLookupTable(labelLookupTable);  // for generating clearer explanations only\n\t\t}\n\t},\n\n\t// private function: use this.normalizers to normalize the given sample:\n\tnormalizedSample: function(sample) {\n\t\tif (!(_.isArray(sample)))\n\t\t{\n\t\t\tif (this.normalizers) {\n\t\t\t\ttry {\n\t\t\t\t\tfor (var i in this.normalizers) {\t\t\t\t\t\n\t\t\t\t\t\tsample = this.normalizers[i](sample);\n\t\t\t\t\t}\n\t\t\t\t} catch (err) {\n\t\t\t\t\tconsole.log(err)\n\t\t\t\t\tthrow new Error(\"Cannot normalize '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn sample;\n\t},\n\n\tsampleToFeatures: function(sample, featureExtractor) {\n\t\tvar features = sample;\n\t\tif (featureExtractor) {\n\t\t\ttry {\n\t\t\t\tfeatures = {};\n\t\t\t\tfeatureExtractor(sample, features);\n\t\t\t} catch (err) {\n\t\t\t\tthrow new Error(\"Cannot extract features from '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t}\n\t\t}\n\n\t\treturn features;\n\t},\n\n\tinstanceFilter: function(data) {\n\t\tif (this.instanceFilterRule) \n\t\t\treturn this.instanceFilterRule(data)\n\t},\n\t\n\ttrainSpellChecker: function(features) {\n\t\tif (this.spellChecker) {\n\t\t\tvar tokens = this.tokenizer.tokenize(features);\n\t\t\t_.each(tokens, function(word, key, list){ \n\t\t\t\tthis.spellChecker[1].understand(word); // Adds the given word to the index of the spell-checker.\n\t\t\t\tthis.spellChecker[1].train(word);\n\t\t\t}, this)\n\t\t}\n\t},\n\t\n\tcorrectFeatureSpelling: function(sample) {\n\t\tif (this.spellChecker) {\n\t\t\tvar features = this.tokenizer.tokenize(sample);\n\t\t\tfor (var index in features) {\n\t\t\t\tvar feature = features[index]\n\t\t\t\tif (!isNaN(parseInt(feature)))  // don't spell-correct numeric features\n\t\t\t\t\t{\n\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif (!(this.spellChecker[1].exists(feature)))\n\t\t\t\t\t{\n\t\t\t\t\t\tif (this.spellChecker[1].suggest(feature).length != 0)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[1].suggest(feature)[0]\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (!(this.spellChecker[0].exists(feature)))\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tif (this.spellChecker[0].suggest(feature).length != 0)\n\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[0].suggest(feature)[0]\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t}\n\t\tsample = features.join(\" \")\n\t\t}\n\t\treturn sample\n\t},\n\t\n\tfeaturesToArray: function(features) {\n\t\tvar array = features;\n\t\tif (this.featureLookupTable) {\n\t\t\tarray = this.featureLookupTable.hashToArray(features);\n\t\t}\n\t\treturn array;\n\t},\n\t\n\tcountFeatures: function(features) {\n\t\tif (this.featureDocumentFrequency) {\n\t\t\t// this.tfidf.addDocument(datum.input);\n\t\t\tfor (var feature in features)\n\t\t\t\tthis.featureDocumentFrequency[feature] = (this.featureDocumentFrequency[feature] || 0)+1;\n\t\t\tthis.documentCount = (this.documentCount||0)+1;\n\t\t}\n\t},\n\t\n\teditFeatureValues: function(features, remove_unknown_features) {\n\n\t\tif (this.multiplyFeaturesByIDF) { \n\t\t\tfor (var feature in features) { \n\t\t\t\tvar IDF = this.tfidf.idf(feature)\n\t\t\t\tif (IDF != Infinity)\n\t\t\t\t\tfeatures[feature] *= IDF\n\t\t\t\telse\n\t\t\t\t\tdelete features[feature]\n\t\t\t}\n\n\t\t\tif (this.bias && !features.bias)\n\t\t\tfeatures.bias = this.bias;\n\n\t\t}\n\t\t// if (remove_unknown_features && this.minFeatureDocumentFrequency>0)\n\t\t\t// for (var feature in features)\n\t\t\t\t// if ((this.featureDocumentFrequency[feature]||0)<this.minFeatureDocumentFrequency)\n\t\t\t\t\t// delete features[feature];\n\t\t\n\t},\n\t\n\n\t/**\n\t * Online training: \n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * @param sample a document.\n\t * @param classes an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, classes) {\n\t\tclasses = normalizeClasses(classes, this.labelLookupTable);\n\t\tsample = this.normalizedSample(sample);\n\t\tvar features = this.sampleToFeatures(sample, this.featureExtractors);\n\t\tthis.countFeatures(features);\n\t\tthis.trainSpellChecker(features);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\tvar array = this.featuresToArray(features);\n\t\tthis.classifier.trainOnline(array, classes);\n\t\tif (this.pastTrainingSamples)\n\t\t\tthis.pastTrainingSamples.push({input: sample, output: classes});\n\t},\n\n\t/**\n\t * Batch training: \n\t * Train the classifier with all the given documents.\n\t * @param dataset an array with objects of the format: {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch: function(dataset) {\n\t\tvar featureLookupTable = this.featureLookupTable;\n\t\tvar pastTrainingSamples = this.pastTrainingSamples;\n\n\t\t\tif (this.spellChecker) {\n\t\t\t\t// var seeds = fs.readFileSync('./node_modules/wordsworth/data/seed.txt')\n\t\t\t\t// var trainings = fs.readFileSync('./node_modules/wordsworth/data/training.txt')\n\t\t\t\tvar seeds = []\n\t\t\t\tvar trainings = []\n\t\t\t\tthis.spellChecker[0].initializeSync(seeds.toString().split(\"\\r\\n\"), trainings.toString().split(\"\\r\\n\"))\n\t\t\t\t}\n\n\t\t\tdataset = dataset.map(function(datum) {\n\n\t\t\t\tif (typeof this.InputSplitLabel === 'function') {\n\t\t\t\t\tdatum.output = (this.InputSplitLabel(multilabelutils.normalizeOutputLabels(datum.output)))\t\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tdatum.output = normalizeClasses(datum.output, this.labelLookupTable);\n\t\t\t\t}\n\n\t\t\t\tif (pastTrainingSamples && dataset!=pastTrainingSamples)\n\t\t\t\t\tpastTrainingSamples.push(datum);\n\t\t\t\tdatum = _(datum).clone();\n\n\t\t\t\tdatum.input = this.normalizedSample(datum.input);\n\n\t\t\t\t/*true - this instance is filtered as not useful*/\n\t\t\t\tif (this.instanceFilter(datum) == true)\n\t\t\t\t\treturn null\n\n\t\t\t\tthis.trainSpellChecker(datum.input);\n\n\t\t\t\tvar features = this.sampleToFeatures(datum.input, this.featureExtractors);\n\t\t\t\t\n\t\t\t\tif (this.tfidf)\n\t\t\t\t\tthis.tfidf.addDocument(features);\n\t\t\t\t// this.trainSpellChecker(features);\n\t\t\t\tif (featureLookupTable)\n\t\t\t\t\tfeatureLookupTable.addFeatures(features);\n\n\t\t\t\tdatum.input = features;\n\t\t\t\treturn datum;\n\t\t\t}, this);\n\n\t\t\tdataset = _.compact(dataset)\n\n\t\tdataset.forEach(function(datum) {\n\t\t\t// run on single sentence\n\t\t\tthis.editFeatureValues(datum.input, /*remove_unknown_features=*/false);\n\t\t\tif (featureLookupTable)\n\t\t\t\tdatum.input = featureLookupTable.hashToArray(datum.input);\n\t\t}, this);\n\n\t\tthis.classifier.trainBatch(dataset);\n\t},\n\n\t/**\n\t * internal function - classify a single segment of the input (used mainly when there is an inputSplitter) \n\t * @param sample a document.\n\t * @return an array whose VALUES are classes.\n\t */\n\tclassifyPart: function(sample, explain, continuous_output) {\n\t\t\n\t\tvar samplecorrected = this.correctFeatureSpelling(sample);\n\t\tvar features = this.sampleToFeatures(samplecorrected, this.featureExtractors);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/true);\n\t\tvar array = this.featuresToArray(features);\n\t\tvar classification = this.classifier.classify(array, explain, continuous_output);\n\t\t\n\t\t// if (this.spellChecker && classification.explanation) {\n\t\t\t// if (Array.isArray(classification.explanation))\n\t\t\t\t// classification.explanation.unshift({SpellCorrectedFeatures: JSON.stringify(features)});\n\t\t\t// else\n\t\t\t\t// classification.explanation['SpellCorrectedFeatures']=JSON.stringify(features);\n\t\t// }\n\t\treturn classification;\n\t},\n\n\toutputToFormat: function(data) {\n\t\tdataset = util.clonedataset(data)\n\t\tdataset = dataset.map(function(datum) {\n\t\tvar normalizedLabels = multilabelutils.normalizeOutputLabels(datum.output);\n\t\treturn {\n\t\t\tinput: datum.input,\n\t\t\toutput: this.TestSplitLabel(normalizedLabels)\n\t\t}\n\t\t}, this);\n\t\treturn dataset\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * @param sample a document.\n\t * @return an array whose VALUES are classes.\n\t * @original is the original gold standard labels is used only for statistics.\n\t */\n\tclassify: function(sample, explain, continuous_output, original, classifier_compare) {\n\t\tvar initial = sample\n\t\tsample = this.normalizedSample(sample)\n\n\t\tif (this.instanceFilter(sample))\n\t\t\t{\tif (explain>0) \n\t\t\t\treturn {\n\t\t\t\t\tclasses: [],\n\t\t\t\t\tscores: {},\n\t\t\t\t\texplanation: {} \n\t\t\t\t\t// bonus: bonus\n\t\t\t\t};\n\t\t\telse\n\t\t\t\treturn []\n\t\t\t}\t\t\n\t\t\n\t\tif (!this.inputSplitter) {\n\t\t\tvar classesWithExplanation = this.classifyPart(sample, explain, continuous_output);\n\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\tvar scores =  (continuous_output? classesWithExplanation.scores: null)\n\t\t\tvar explanations = (explain>0? classesWithExplanation.explanation: null);\n\t\t} else {\n\t\t\tvar parts = this.inputSplitter(sample);\n\t\t\t// var accumulatedClasses = {};\n\t\t\tvar accumulatedClasses = [];\n\t\t\tvar explanations = [];\n\t\t\tparts.forEach(function(part) {\n\t\t\t\tif (part.length==0) return;\n\t\t\t\tvar classesWithExplanation = this.classifyPart(part, explain, continuous_output);\n\t\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\t\t// for (var i in classes)\n\t\t\t\t// \taccumulatedClasses[classes[i]]=true;\n\t\t\t\taccumulatedClasses.push(classes)\n\t\t\t\tif (explain>0) {\n\t\t\t\t\t// explanations.push(part);\n\t\t\t\t\texplanations.push(classesWithExplanation.explanation);\n\t\t\t\t}\n\t\t\t}, this);\n    \t\tclasses = []\n    \t\tif (accumulatedClasses[0])\n    \t\t{\n\t\t\tif (accumulatedClasses[0][0] instanceof Array)\n\t\t\t\t_(accumulatedClasses[0].length).times(function(n){\n\t\t\t\t\tclasses.push(_.flatten(_.pluck(accumulatedClasses,n)))\n\t\t\t\t });\n\t\t\telse\n\t\t\t{\n\t\t\t\tclasses = _.flatten(accumulatedClasses)\n\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (this.labelLookupTable) {\n\t\t\tif (Array.isArray(classes)) {\n\t\t\t\tclasses = classes.map(function(label) {\n\t\t\t\t\t\tif (_.isArray(label))\n\t\t\t\t\t\t\tlabel[0] = this.labelLookupTable.numberToFeature(label[0]);\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tlabel = this.labelLookupTable.numberToFeature(label);\n\t\t\t\t\t\treturn label;\n\t\t\t\t\t}, this);\n\t\t\t} else {\n\t\t\t\tclasses = this.labelLookupTable.numberToFeature(classes);\n\t\t\t}\n\t\t}\n\n\t\tif ((typeof this.OutputSplitLabel === 'function')) {\n\n\t\t\t// classes = this.OutputSplitLabel(classes, this.Observable, sample, explanations)\n\t\t\t// var classes = []\n\t\t\t// if (_.isArray(explanations))\n\t\t\t// var bonus = []\n\t\t\n\t\t\tif ((explain>0) && (this.inputSplitter))\n\t\t\t\t{ nclasses = []\n\t\t\t\t_(explanations.length).times(function(n){\n\t\t\t\t\tvar clas = this.OutputSplitLabel(classes, this, parts[n], explanations[n], original, classifier_compare, initial)\n\t\t\t\t\tnclasses = nclasses.concat(clas)\n\t\t\t\t}, this)\n\t\t\t\tclasses = nclasses\n\t\t\t\t}\n\t\t\telse\n\t\t\t\t{\n\t\t\t\tvar classes = this.OutputSplitLabel(classes, this, sample, explanations, original, classifier_compare, initial)\n\t\t\t\t}\n\t\t\t}\n\n\t\tif (explain>0) \n\t\t\treturn {\n\t\t\t\tclasses: classes,\n\t\t\t\tscores: scores,\n\t\t\t\texplanation: explanations\n\t\t\t\t// bonus: bonus\n\t\t\t};\n\t\telse\n\t\t\treturn classes;\n\t},\n\n\t\n\t/**\n\t * Train on past training samples\n\t * currently doesn't work\n\t */\n\tretrain: function() {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't retrain\");\n\t\t\n\t\tthis.trainBatch(this.pastTrainingSamples);\n\t},\n\t\n\t/**\n\t * @return an array with all samples whose class is the given class.\n\t * Available only if the pastTrainingSamples are saved.\n\t */\n\tbackClassify: function(theClass) {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't backClassify\");\n\n\t\tif (!(theClass instanceof Array))\n\t\t\ttheClass = [theClass];\n\t\tvar samples = [];\n\t\tthis.pastTrainingSamples.forEach(function(datum) {\n\t\t\tif (_(datum.output).isEqual(theClass))\n\t\t\t\tsamples.push(datum.input);\n\t\t});\n\t\treturn samples;\n\t},\n\n\ttoJSON : function(callback) {\n\t\treturn {\n\t\t\tclassifier: this.classifier.toJSON(callback),\n\t\t\tfeatureLookupTable: (this.featureLookupTable? this.featureLookupTable.toJSON(): undefined),\n\t\t\tlabelLookupTable: (this.labelLookupTable? this.labelLookupTable.toJSON(): undefined),\n\t\t\tspellChecker:  (this.spellChecker? this.spellChecker/*.toJSON()*/: undefined),\n\t\t\tpastTrainingSamples: (this.pastTrainingSamples? this.pastTrainingSamples: undefined),\n\t\t\tfeatureDocumentFrequency: this.featureDocumentFrequency,\n\t\t\tdocumentCount: this.documentCount,\n\t\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not serializable! */ \n\t\t};\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.classifier.fromJSON(json.classifier);\n\t\tif (this.featureLookupTable) {\n\t\t\tthis.featureLookupTable.fromJSON(json.featureLookupTable);\n\t\t\tthis.setFeatureLookupTable(this.featureLookupTable);\n\t\t}\n\t\tif (this.labelLookupTable) {\n\t\t\tthis.labelLookupTable.fromJSON(json.labelLookupTable);\n\t\t\tthis.setLabelLookupTable(this.labelLookupTable);\n\t\t}\n\t\tif (this.spellChecker) this.spellChecker = json.spellChecker; \n\t\tif (this.pastTrainingSamples) this.pastTrainingSamples = json.pastTrainingSamples;\n\t\tthis.featureDocumentFrequency = json.featureDocumentFrequency;\n\t\tthis.documentCount = json.documentCount;\n\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not deserializable! */ \n\t},\n\n\tgetAllClasses: function() {  // relevant for multilabel classifiers\n\t\treturn this.classifier.getAllClasses();\n\t},\n}  // end of EnhancedClassifier prototype\n\n\nvar stringifyClass = function (aClass) {\n\treturn (_(aClass).isString()? aClass: JSON.stringify(aClass));\n}\n\nvar normalizeClasses = function (classes, labelLookupTable) {\n\tif (!Array.isArray(classes))\n\t\tclasses = [classes];\n\tclasses = classes.map(stringifyClass);\n\tif (labelLookupTable)\n\t\tclasses = classes.map(labelLookupTable.featureToNumber, labelLookupTable);\n\tclasses.sort();\n\treturn classes;\n}\n\nmodule.exports = EnhancedClassifier;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/EnhancedClassifier.2015.js":"/*\nTODO: SpellChecker should be reorganized\n*/\n\nvar async = require('async');\nvar ftrs = require('../features');\nvar _ = require('underscore')._;\nvar hash = require('../utils/hash');\nvar util = require('../utils/list');\nvar multilabelutils = require('./multilabel/multilabelutils');\n\n/**\n * EnhancedClassifier - wraps any classifier with feature-extractors and feature-lookup-tables.\n * \n * @param opts\n * Obligatory option: 'classifierType', which is the base type of the classifier.\n * Optional:\n * * 'inputSplitter' - a function that splits the input samples into sub-samples, for multi-label classification (useful mainly for sentences). \n * * 'normalizer' - a function that normalizes the input samples, before they are sent to feature extraction.\n * * 'featureExtractor' - a single feature-extractor (see the \"features\" folder), or an array of extractors, for extracting features from training and classification samples.\n * * 'featureExtractorForClassification' - additional feature extractor[s], for extracting features from samples during classification. Used for domain adaptation.\n * * 'featureLookupTable' - an instance of FeatureLookupTable for converting features (in the input) to numeric indices and back.\n * * 'labelLookupTable' - an instance of FeatureLookupTable for converting labels (classes, in the output) to numeric indices and back.\n * * 'multiplyFeaturesByIDF' - boolean - if true, multiply each feature value by log(documentCount / (1+featureDocumentFrequency))\n * * 'minFeatureDocumentFrequency' - int - if positive, ignore features that appeared less than this number in the training set.\n * * 'pastTrainingSamples' - an array that keeps all past training samples, to enable retraining.\n * * 'spellChecker' - an initialized spell checker from the 'wordsworth' package, to spell-check features during classification.\n * * 'bias' - a 'bias' feature with a constant value (usually 1).\n * * 'InputSplitLabel' - a method for special separation of input labels before training\n * * 'OutputSplitLabel' - a method for special separation of output labels after classification.\n * * 'TestSplitLabel' - a method for special separation before a testing\n * * 'TfIdfImpl' - implementation of tf-idf algorithm\n * * 'tokenizer' - implementation of tokenizer\n * * 'featureExpansion' - a function that given the the list of known features generates the list of paraphrase features\n * * 'featureExpansionScale' - a list than defines the scale of feature expansion, it goes from high Precision to high Recall, can be used in chains\n * * 'featureExpansionPhrase' - a boolean, whether to explore only phrase-based expansion, this option measures the contribution of phrases\n * * 'featureFine' - a boolean, fine expanded features by similarity score.\n\n * * 'instanceFilter' - filter of instance of training data and test data, if training instance is filtered is not used for training, if triaging instance is filtered by classify,\n it's classified empty class.\n*/\n\nvar EnhancedClassifier = function(opts) {\n\tif (!opts.classifierType) {\n\t\tconsole.dir(opts);\n\t\tthrow new Error(\"opts must contain classifierType\");\n\t}\n\n\tthis.classifier = new opts.classifierType();\n\n\tthis.inputSplitter = opts.inputSplitter;\n\tthis.setNormalizer(opts.normalizer);\n\tthis.setFeatureExtractor(opts.featureExtractor);\n\tthis.setFeatureExtractorForClassification(opts.featureExtractorForClassification);\n\t// this.setFeatureLookupTable(opts.featureLookupTable);\n\tthis.setFeatureLookupTable(new ftrs.FeatureLookupTable());\n\n\tthis.setLabelLookupTable(opts.labelLookupTable);\n\tthis.setInstanceFilter(opts.instanceFilter);\n\n\tthis.setFeatureExpansion(opts.featureExpansion);\n\tthis.featureExpansionScale = opts.featureExpansionScale;\n\tthis.featureExpansionPhrase = opts.featureExpansionPhrase;\n\tthis.featureFine = opts.featureFine;\n\n\tthis.multiplyFeaturesByIDF = opts.multiplyFeaturesByIDF;\n\tthis.minFeatureDocumentFrequency = opts.minFeatureDocumentFrequency || 0;\n\tif (opts.multiplyFeaturesByIDF||opts.minFeatureDocumentFrequency) \n\t\t{\n    \tthis.tfidf = new opts.TfIdfImpl\n\t\tthis.featureDocumentFrequency = {};\n\t\t}\n\tthis.bias = opts.bias;\n\n\tthis.spellChecker = opts.spellChecker;\n\tthis.tokenizer = opts.tokenizer;\n\n\t// this.spellChecker =  [require('wordsworth').getInstance(), require('wordsworth').getInstance()],\n\t// this.pastTrainingSamples = opts.pastTrainingSamples;\n\t// TODO: it looks like the method with creating an array at the definition \n\t// create an array with the same pointer for every classifier of the given class\n\t\n\tthis.pastTrainingSamples = []\n\n\tthis.preProcessor = opts.preProcessor\n\tthis.postProcessor = opts.postProcessor\n\t// this.TestSplitLabel = opts.TestSplitLabel\n}\n\n\nEnhancedClassifier.prototype = {\n\n\tsetInstanceFilter: function (instanceFilter) {\n\t\tthis.instanceFilter = instanceFilter;\n\t},\n\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetFeatureExtractor: function (featureExtractor) {\n\t\tthis.featureExtractors = ftrs.normalize(featureExtractor);\n\t},\n\n\tsetFeatureExpansion: function (featureExpansion) {\n\t\tthis.featureExpansion = featureExpansion\n\t},\n\t\n\t/** Set the main feature extractor, used for both training and classification. */\n\tsetNormalizer: function (normalizer) {\n\t\tif (normalizer)\n\t\t\tthis.normalizers = (Array.isArray(normalizer)? normalizer: [normalizer]);\n\t},\n\n\t/** Set an additional feature extractor, for classification only. */\n\tsetFeatureExtractorForClassification: function (featureExtractorForClassification) {\n\t\tif (featureExtractorForClassification) {\n\t\t\tif (Array.isArray(featureExtractorForClassification)) {\n\t\t\t\tfeatureExtractorForClassification.unshift(this.featureExtractors);\n\t\t\t} else {\n\t\t\t\tfeatureExtractorForClassification = [this.featureExtractors, featureExtractorForClassification];\n\t\t\t}\n\t\t\tthis.featureExtractorsForClassification = new ftrs.CollectionOfExtractors(featureExtractorForClassification);\n\t\t}\n\t},\n\t\n\tsetFeatureLookupTable: function(featureLookupTable) {\n\t\tif (featureLookupTable) {\n\t\t\tthis.featureLookupTable = featureLookupTable;\n\t\t\tif (this.classifier.setFeatureLookupTable)\n\t\t\t\tthis.classifier.setFeatureLookupTable(featureLookupTable);  // for generating clearer explanations only\n\t\t}\n\t},\n\t\n\tsetLabelLookupTable: function(labelLookupTable) {\n\t\tif (labelLookupTable) {\n\t\t\tthis.labelLookupTable = labelLookupTable;\n\t\t\tif (this.classifier.setLabelLookupTable)\n\t\t\t\tthis.classifier.setLabelLookupTable(labelLookupTable);  // for generating clearer explanations only\n\t\t\tthis.applyFeatureExpansion();\n\t\t}\n\t},\n\n\tapplyFeatureExpansion: function(){\n\t \tthis.featureExpansioned = this.featureExpansion(this.featureLookupTable['featureIndexToFeatureName'], this.featureExpansionScale, this.featureExpansionPhrase)\n\t},\n\t// private function: use this.normalizers to normalize the given sample:\n\tnormalizedSample: function(sample) {\n\t\tif (!(_.isArray(sample)))\n\t\t{\n\t\t\tif (this.normalizers) {\n\t\t\t\ttry {\n\t\t\t\t\tfor (var i in this.normalizers) {\t\t\t\t\t\n\t\t\t\t\t\tsample = this.normalizers[i](sample);\n\t\t\t\t\t}\n\t\t\t\t} catch (err) {\n\t\t\t\t\tconsole.log(err)\n\t\t\t\t\tthrow new Error(\"Cannot normalize '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn sample;\n\t},\n\n\tsampleToFeaturesAsync: function(sample, featureExtractor, callback) {\n\t\t// features = {}\n\t\t// \tasync.eachSeries(featureExtractor, function(FE, callback1){\n  //               FE(sample, features, function(err, results){\n  //                   callback1()\n  //               })\n  //           }, function(err){\n  //               callback(null, features)\n  //               })\n\t\n\t\tfeatures = {}\n\t\tfeatureExtractor(sample, features, function(err, results){\n  \t\t\tcallback(null, features)\n  \t\t})\n    },\n\n\tsampleToFeatures: function(sample, featureExtractor) {\n\t\tvar features = sample;\n\t\tif (featureExtractor) {\n\t\t\ttry {\n\t\t\t\tfeatures = {};\n\t\t\t\tfeatureExtractor(sample, features);\n\t\t\t} catch (err) {\n\t\t\t\tconsole.log(err)\n\t\t\t\tthrow new Error(\"Cannot extract features from '\"+sample+\"': \"+JSON.stringify(err));\n\t\t\t}\n\t\t}\n\n\t\treturn features;\n\t},\n\n\tinstanceFilterRun: function(data) {\n\t\tif (this.instanceFilter) \n\t\t\treturn this.instanceFilter(data)\n\t},\n\t\n\ttrainSpellChecker: function(features) {\n\t\tif (this.spellChecker) {\n\t\t\tvar tokens = this.tokenizer.tokenize(features);\n\t\t\t_.each(tokens, function(word, key, list){ \n\t\t\t\tthis.spellChecker[1].understand(word); // Adds the given word to the index of the spell-checker.\n\t\t\t\tthis.spellChecker[1].train(word);\n\t\t\t}, this)\n\t\t}\n\t},\n\t\n\tcorrectFeatureSpelling: function(sample) {\n\t\tif (this.spellChecker) {\n\t\t\tvar features = this.tokenizer.tokenize(sample);\n\t\t\tfor (var index in features) {\n\t\t\t\tvar feature = features[index]\n\t\t\t\tif (!isNaN(parseInt(feature)))  // don't spell-correct numeric features\n\t\t\t\t\t{\n\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif (!(this.spellChecker[1].exists(feature)))\n\t\t\t\t\t{\n\t\t\t\t\t\tif (this.spellChecker[1].suggest(feature).length != 0)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[1].suggest(feature)[0]\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (!(this.spellChecker[0].exists(feature)))\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tif (this.spellChecker[0].suggest(feature).length != 0)\n\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tfeatures[index] = this.spellChecker[0].suggest(feature)[0]\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t}\n\t\tsample = features.join(\" \")\n\t\t}\n\t\treturn sample\n\t},\n\t\n\tfeaturesToArray: function(features) {\n\t\tvar array = features;\n\t\tif (this.featureLookupTable) {\n\t\t\tarray = this.featureLookupTable.hashToArray(features);\n\t\t}\n\t\treturn array;\n\t},\n\t\n\tcountFeatures: function(features) {\n\t\tif (this.featureDocumentFrequency) {\n\t\t\t// this.tfidf.addDocument(datum.input);\n\t\t\tfor (var feature in features)\n\t\t\t\tthis.featureDocumentFrequency[feature] = (this.featureDocumentFrequency[feature] || 0)+1;\n\t\t\tthis.documentCount = (this.documentCount||0)+1;\n\t\t}\n\t},\n\t\n\teditFeatureValues: function(features, remove_unknown_features) {\n\n\t\tif (this.multiplyFeaturesByIDF) { \n\t\t\tfor (var feature in features) { \n\n\t\t\t\t// Skip word2vec features\n\t\t\t\tif (typeof feature.match(/w2v/g) == \"undefined\" || feature.match(/w2v/g) == null)\n\t\t\t\t\tcontinue\n\n\t\t\t\tvar IDF = this.tfidf.idf(feature)\n\n\t\t\t\tif (IDF != Infinity)\n\t\t\t\t\tfeatures[feature] *= IDF\n\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\tconsole.error(\"Infinity \"+feature)\n\t\t\t\t\tdelete features[feature]\n\t\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (this.bias && !features.bias)\n\t\t\tfeatures.bias = this.bias;\n\n\t\t}\n\t\t// if (remove_unknown_features && this.minFeatureDocumentFrequency>0)\n\t\t\t// for (var feature in features)\n\t\t\t\t// if ((this.featureDocumentFrequency[feature]||0)<this.minFeatureDocumentFrequency)\n\t\t\t\t\t// delete features[feature];\n\t\t\n\t},\n\t\n\n\t/**\n\t * Online training: \n\t * Tell the classifier that the given sample belongs to the given classes.\n\t * @param sample a document.\n\t * @param classes an array whose VALUES are classes.\n\t */\n\ttrainOnline: function(sample, classes) {\n\t\tclasses = normalizeClasses(classes, this.labelLookupTable);\n\t\tsample = this.normalizedSample(sample);\n\t\tvar features = this.sampleToFeatures(sample, this.featureExtractors);\n\t\tthis.countFeatures(features);\n\t\tthis.trainSpellChecker(features);\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\tvar array = this.featuresToArray(features);\n\t\tthis.classifier.trainOnline(array, classes);\n\t\tif (this.pastTrainingSamples)\n\t\t\tthis.pastTrainingSamples.push({input: sample, output: classes});\n\t},\n\n\ttrainBatchAsync: function(dataset, callbackg) {\n\t\tvar featureLookupTable = this.featureLookupTable;\n\t\tvar pastTrainingSamples = this.pastTrainingSamples;\n\n\t\tprocessed_dataset = []\n\n\t\tasync.forEachOfSeries(dataset, (function(datum, dind, callback2){ \n\t\t\t\n\t\t\tif (_.isObject(datum.input))\n\t\t\t\tdatum.input.text = this.normalizedSample(datum.input.text);\n\t\t\telse\t\n\t\t\t\tdatum.input = this.normalizedSample(datum.input);\n\n\t\t\tif (typeof this.preProcessor === 'function')\n\t\t\t\tdatum = this.preProcessor(datum)\n\n\t\t\tif (!_.isUndefined(datum))\n\t\t\t{\n\t\t\t\tthis.sampleToFeaturesAsync(datum.input, this.featureExtractors, (function(err, features){\n\t\t\t\n\t\t\t\t\t// this.omitStopWords(features, this.stopwords)\n\n\t\t\t\t\tif (this.tfidf)\n\t\t\t\t\t\tthis.tfidf.addDocument(features)\n\t\t\t\t\t\n\t\t\t\t\tif (featureLookupTable)\n\t\t\t\t\t\tfeatureLookupTable.addFeatures(features)\n\n\t\t\t\t\tdatum.input = features\n\t\t\t\t\tprocessed_dataset.push(datum)\n\n\t\t\t\t\tcallback2()\n\t\t\t\t}).bind(this))\n\t\t\t}\n\t\t\telse\n\t\t\tcallback2()\n\n\t\t}).bind(this), (function(err){\n\n\t\t\tprocessed_dataset = _.compact(processed_dataset)\n\n\t\t\tprocessed_dataset.forEach(function(datum) {\n\t\t\t\n\t\t\t\tthis.editFeatureValues(datum.input, /*remove_unknown_features=*/false);\n\t\t\t\tif (featureLookupTable)\n\t\t\t\t\tdatum.input = featureLookupTable.hashToArray(datum.input);\n\t\t\t}, this)\n\n\t\t\tthis.classifier.trainBatch(processed_dataset)\n\t\t\tcallbackg(null,[])\n\t\t\n\t\t}).bind(this))\n\t},\n\n\t/**\n\t * Batch training: \n\t * Train the classifier with all the given documents.\n\t * @param dataset an array with objects of the format: {input: sample1, output: [class11, class12...]}\n\t */\n\ttrainBatch: function(dataset) {\n\t\tvar featureLookupTable = this.featureLookupTable;\n\t\tvar pastTrainingSamples = this.pastTrainingSamples;\n\n\t\t\t// if (this.spellChecker) {\n\t\t\t\t// var seeds = []\n\t\t\t\t// var trainings = []\n\t\t\t\t// this.spellChecker[0].initializeSync(seeds.toString().split(\"\\r\\n\"), trainings.toString().split(\"\\r\\n\"))\n\t\t\t// }\n\n\t\t\tdataset = _.map(dataset, function(datum){ \n\t\t\t\t\t\tif (_.isObject(datum.input))\n\t\t\t\t\t\t\tdatum.input.text = this.normalizedSample(datum.input.text);\n\t\t\t\t\t\telse\t\n\t\t\t\t\t\t\tdatum.input = this.normalizedSample(datum.input);\n\t\t\t\t\t\treturn datum\n\t\t\t\t\t}, this);\n\t\t\t\n\t\t\tif ((typeof this.preProcessor === 'function')) {\n\t\t\t\tdataset = _.map(dataset, function(value){ return this.preProcessor(value) }, this);\n\t\t\t}\n\n\t\t\tdataset = _.compact(dataset)\n\n\t\t\tdataset = dataset.map(function(datum) {\n\t\t\t\tdatum = _(datum).clone();\n\n\t\t\t\tvar features = this.sampleToFeatures(datum.input, this.featureExtractors);\n\t\t\t\tif (this.tfidf)\n\t\t\t\t\tthis.tfidf.addDocument(features);\n\t\t\t\tif (featureLookupTable)\n\t\t\t\t\tfeatureLookupTable.addFeatures(features);\n\n\t\t\t\tdatum.input = features;\n\t\t\t\treturn datum;\n\t\t\t}, this);\n\n\t\t\tdataset = _.compact(dataset)\n\n\t\tdataset.forEach(function(datum) {\n\t\t\t// run on single sentence\n\t\t\tthis.editFeatureValues(datum.input, /*remove_unknown_features=*/false);\n\t\t\tif (featureLookupTable)\n\t\t\t\tdatum.input = featureLookupTable.hashToArray(datum.input);\n\t\t}, this);\n\n\t\tthis.classifier.trainBatch(dataset);\n\n\t},\n\n\tclassifyBatch: function(testSet)\n        {\n            _.each(testSet, function(value, key, list){\n                testSet[key][\"input\"] = this.normalizedSample(testSet[key][\"input\"])\n                var features = this.sampleToFeatures(testSet[key][\"input\"], this.featureExtractors, this.stopwords);\n                this.editFeatureValues(features, /*remove_unknown_features=*/false);\n                var array = this.featuresToArray(features);\n                testSet[key][\"input\"] = array\n            }, this)\n            return this.classifier.classifyBatch(testSet);\n        },\n\t\n\tclassifyPartAsync: function(sample, explain, callback) {\n\t\tthis.sampleToFeaturesAsync(sample, this.featureExtractors, (function(err, results){\n\t\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);\n\t\t\tvar array = this.featuresToArray(features);\n\t\t\tvar classification = this.classifier.classify(array, explain);\n\t\t\tclassification['features'] = features\n\t\t\tcallback(null, classification)\n\t\t}).bind(this))\n\t},\n\n\tclassifyPart: function(sample, explain, continuous_output) {\n\t\tvar features = this.sampleToFeatures(sample, this.featureExtractors);\n        //console.log(\"features=\"+JSON.stringify(features));\n\t\tthis.editFeatureValues(features, /*remove_unknown_features=*/false);  // change the value of each feature according to the TF/IDF method\n        //console.log(\"features=\"+JSON.stringify(features));\n\t\tvar array = this.featuresToArray(features);  // convert textual features to numeric features using a lookup table\n        //console.log(\"array=\"+JSON.stringify(array));\n\t\tvar classification = this.classifier.classify(array, explain, continuous_output);\n        //console.log(\"classification=\"+JSON.stringify(classification));\n\t\tclassification['features'] = features\n\t\treturn classification;\n\t},\n\t\n\tclassifyAsync: function(sample, explain, callback_global) {\n\t\tvar classes = []\n\n\t\tasync.series([\n   \t\t\t(function(callback){\n       \t\t\n        \t\tif(!this.inputSplitter) {\n        \t\t\tif (typeof this.preProcessor === 'function')\n\t\t\t\t\t\tsample = this.preProcessor(sample)\n\n\t\t\t\t\tthis.classifyPartAsync(sample, explain, function(error, classesWithExplanation){\n\t\t\t\t\t\tclasses = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\t\t\t\tvar scores =  classesWithExplanation.scores\n\t\t\t\t\t\t// var scores =  (continuous_output? classesWithExplanation.scores: null)\n\t\t\t\t\t\tvar explanations = (explain>0? classesWithExplanation.explanation: null);\n\t\t\t\t\t\tcallback(null, null);\n\t\t\t\t\t});\t\t\t\t\t\n    \n        \t\t} else {\n          \t\t\tcallback(null, null);\n        \t\t}\n   \t\t\t}).bind(this),\n    \t\t(function(callback){\n\n    \t\t\tif (typeof this.inputSplitter === 'function')\n    \t\t\t{\n\n\t          \t\tvar parts = this.inputSplitter(sample);\n\t\t\t\t\tvar accumulatedClasses = [];\n\t\t\t\t\tvar explanations = [];\n\t\t\t\n\t\t\t\t\tasync.eachSeries(parts, (function(part, callback1){\n\t\t\t\t\t\tif (part.length==0) return;\n\n\t\t\t\t\t\tvar part_filtered = part\n\n\t\t\t\t\t\tif (typeof this.preProcessor === 'function')\n\t\t\t\t\t\t\tpart_filtered = this.preProcessor(part)\n\t\t\t\t\t\n\t\t\t\t\t\tthis.classifyPartAsync(part_filtered, explain, (function(error, classesWithExplanation){\n\n\t\t\t\t\t\t\tclasses = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\t\t\n\t\t\t\t\t\t\tif (typeof this.postProcessor === 'function')\n\t\t\t\t\t\t\t\tclasses = this.postProcessor(part, classes)\n\n\t\t\t\t\t\t\taccumulatedClasses.push(classes)\n\t\t\t\t\t\t\tif (explain>0) \n\t\t\t\t\t\t\t\texplanations.push(classesWithExplanation.explanation);\n\n\t\t\t\t\t\t\tcallback1()\n\t\t\t\t\t\t}).bind(this))\n\n\t\t\t\t    }).bind(this), function(err){\n\t   \t\t\t\t\tclasses = _.flatten(accumulatedClasses)\n\t             \t   \tcallback(null, null)\n\t                })\n        \t\t} else {\n          \t\t\tcallback(null, null);\n        \t\t}\n    \t\t}).bind(this)\n\t\t], function () {\n\n\t\t\tif (this.labelLookupTable) {\n\t\t\t\tif (Array.isArray(classes)) {\n\t\t\t\t\tclasses = classes.map(function(label) {\n\t\t\t\t\t\tif (_.isArray(label))\n\t\t\t\t\t\t\tlabel[0] = this.labelLookupTable.numberToFeature(label[0]);\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tlabel = this.labelLookupTable.numberToFeature(label);\n\t\t\t\t\t\treturn label;\n\t\t\t\t\t}, this);\n\t\t\t\t} else {\n\t\t\t\t\tclasses = this.labelLookupTable.numberToFeature(classes);\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n    \t\tcallback_global(null, classes)\n\t\t})\n\t},\n\n\t/**\n\t * Use the model trained so far to classify a new sample.\n\t * @param sample a document.\n\t * @return an array whose VALUES are classes.\n\t * @original is the original gold standard labels is used only for statistics.\n\t */\n\tclassify: function(sample, explain) {\n\t\tvar initial = sample\n\n\t\tif (_.isObject(sample)) \n\t\t\tsample.text = this.normalizedSample(sample.text)\n\t\telse\n\t\t\tsample = this.normalizedSample(sample)\n\n\t\tif (!this.inputSplitter) {\n\n\t\t\tif (typeof this.preProcessor === 'function')\n\t\t\t\tsample = this.preProcessor(sample)\n\n\t\t\tvar classesWithExplanation = this.classifyPart(sample, explain);\n            //console.log(\"classesWithExplanation=\"+JSON.stringify(classesWithExplanation));\n\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n            //console.log(\"classes=\"+JSON.stringify(classes));\n\t\t\tvar scores =  classesWithExplanation.scores\n            //console.log(\"scores=\"+JSON.stringify(scores));\n\t\t\tvar explanations = (explain>0? classesWithExplanation.explanation: null);\n            //console.log(\"explanations=\"+JSON.stringify(explanations));\n\t\t} else {\n\t\t\tvar parts = this.inputSplitter(sample);\n\t\t\tvar accumulatedClasses = [];\n\t\t\tvar explanations = [];\n\t\t\tparts.forEach(function(part) {\n\t\t\t\tif (part.length==0) return;\n\t\t\t\tvar part_filtered = part\n\t\t\t\tif (typeof this.preProcessor === 'function')\n\t\t\t\t\tpart_filtered = this.preProcessor(part)\n\t\t\t\tvar classesWithExplanation = this.classifyPart(part_filtered, explain);\n\t\t\t\tvar classes = (explain>0? classesWithExplanation.classes: classesWithExplanation);\n\t\t\t\tif (typeof this.postProcessor === 'function')\n\t\t\t\t\tclasses = this.postProcessor(part, classes)\n\t\t\t\taccumulatedClasses.push(classes)\n\t\t\t\tif (explain>0) {\n\t\t\t\t\texplanations.push(classesWithExplanation.explanation);\n\t\t\t\t}\n\t\t\t}, this);\n\t\t\tclasses = _.flatten(accumulatedClasses)\n\t\t}\n\n\t\tif (this.labelLookupTable) {\n\t\t\tif (Array.isArray(classes)) {\n\t\t\t\tclasses = classes.map(function(label) {\n\t\t\t\t\t\tif (_.isArray(label))\n\t\t\t\t\t\t\tlabel[0] = this.labelLookupTable.numberToFeature(label[0]);\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tlabel = this.labelLookupTable.numberToFeature(label);\n\t\t\t\t\t\treturn label;\n\t\t\t\t\t}, this);\n\t\t\t} else {\n\t\t\t\tclasses = this.labelLookupTable.numberToFeature(classes);\n\t\t\t}\n\t\t}\n\n\t\tif (explain>0) \n\t\t\treturn {\n\t\t\t\tclasses: classes,\n\t\t\t\tscores: scores,\n\t\t\t\t// expansioned: classesWithExplanation.expansioned,\n\t\t\t\t// features: classesWithExplanation.features,\n\t\t\t\t// explanation: explanations\n\t\t\t};\n\t\telse\n\t\t\treturn classes;\n\t},\n\n\t\n\t/**\n\t * Train on past training samples\n\t * currently doesn't work\n\t */\n\tretrain: function() {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't retrain\");\n\t\t\n\t\tthis.trainBatch(this.pastTrainingSamples);\n\t},\n\t\n\t/**\n\t * @return an array with all samples whose class is the given class.\n\t * Available only if the pastTrainingSamples are saved.\n\t */\n\tbackClassify: function(theClass) {\n\t\tif (!this.pastTrainingSamples)\n\t\t\tthrow new Error(\"No pastTrainingSamples array - can't backClassify\");\n\n\t\tif (!(theClass instanceof Array))\n\t\t\ttheClass = [theClass];\n\t\tvar samples = [];\n\t\tthis.pastTrainingSamples.forEach(function(datum) {\n\t\t\tif (_(datum.output).isEqual(theClass))\n\t\t\t\tsamples.push(datum.input);\n\t\t});\n\t\treturn samples;\n\t},\n\n\ttoJSON : function(callback) {\n\t\treturn {\n\t\t\tclassifier: this.classifier.toJSON(callback),\n\t\t\tfeatureLookupTable: (this.featureLookupTable? this.featureLookupTable.toJSON(): undefined),\n\t\t\tlabelLookupTable: (this.labelLookupTable? this.labelLookupTable.toJSON(): undefined),\n\t\t\tspellChecker:  (this.spellChecker? this.spellChecker/*.toJSON()*/: undefined),\n\t\t\tpastTrainingSamples: (this.pastTrainingSamples? this.pastTrainingSamples: undefined),\n\t\t\tfeatureDocumentFrequency: this.featureDocumentFrequency,\n\t\t\tdocumentCount: this.documentCount,\n\t\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not serializable! */ \n\t\t};\n\t},\n\n\tfromJSON : function(json) {\n\t\tthis.classifier.fromJSON(json.classifier);\n\t\tif (this.featureLookupTable) {\n\t\t\tthis.featureLookupTable.fromJSON(json.featureLookupTable);\n\t\t\tthis.setFeatureLookupTable(this.featureLookupTable);\n\t\t}\n\t\tif (this.labelLookupTable) {\n\t\t\tthis.labelLookupTable.fromJSON(json.labelLookupTable);\n\t\t\tthis.setLabelLookupTable(this.labelLookupTable);\n\t\t}\n\t\tif (this.spellChecker) this.spellChecker = json.spellChecker; \n\t\tif (this.pastTrainingSamples) this.pastTrainingSamples = json.pastTrainingSamples;\n\t\tthis.featureDocumentFrequency = json.featureDocumentFrequency;\n\t\tthis.documentCount = json.documentCount;\n\t\t/* Note: the feature extractors are functions - they should be created at initialization - they are not deserializable! */ \n\t},\n\n\tgetAllClasses: function() {  // relevant for multilabel classifiers\n\t\treturn this.classifier.getAllClasses();\n\t},\n}  // end of EnhancedClassifier prototype\n\n\nvar stringifyClass = function (aClass) {\n\treturn (_(aClass).isString()? aClass: JSON.stringify(aClass));\n}\n\nvar normalizeClasses = function (classes, labelLookupTable) {\n\tif (!Array.isArray(classes))\n\t\tclasses = [classes];\n\tclasses = classes.map(stringifyClass);\n\tif (labelLookupTable)\n\t\tclasses = classes.map(labelLookupTable.featureToNumber, labelLookupTable);\n\tclasses.sort();\n\treturn classes;\n}\n\nmodule.exports = EnhancedClassifier;\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/utils/unseen_correlation.js":"/*\n\tCorrelation between unseen words and False Negative \n\n\tThe assumption is that previously unseen word mostly might cause false negative type of mistake.\n\tModule does cross-validation on the given dataset, in the test utterances where there is \n\tunseen words and false negative mistake the the dict is build, where the key is a word and \n\tthe value is the list of false negative mistakes.\n\n\t@author Vasily Konovalov\n */\n\nvar _ = require('underscore')._;\nvar fs = require('fs');\nvar partitions = require('./partitions');\nvar trainAndTest = require('./trainAndTest').trainAndTest;\nvar trainAndTest_hash= require('./trainAndTest').trainAndTest_hash;\n\nfunction normalizer(sentence) {\n\tif (typeof sentence == 'undefined')\n\t\t{return \"\"}\n\telse\n\t\t{\n\t\treturn sentence.toLowerCase().trim();\n\t\t}\n}\n\nfunction tokenizedataset(dataset, tokenize)\n{ \n\tvocabulary = []\n\tfor (var sample in dataset) \n    {\n\t\tif (dataset[sample].length!=0)\n\t   \t{\n\t   \tvar words = tokenize(normalizer(dataset[sample]['input']));\n    \tvocabulary = vocabulary.concat(words);\n    \t}\n\t }\n    return _.uniq(vocabulary);\n}\n\nmodule.exports.tokenize = function(str)\n\t{\n\t\tpattern = new RegExp(/(\\w+|\\!|\\'|\\\"\")/i);\n\t\tstr = str.split(pattern)\n\t\treturn _.without(str,'',' ')\n\t}\n/*\n\t@params dataset - dataset to estimate the correlation\n\t@params classifier - classifier to estimate false negative mistakes.\n\n\t*/\nmodule.exports.unseen_correlation = function(dataset, classifier, tokenize) {\n\tunseen_correlation = {}\n\n\tpartitions.partitions(dataset, 5, function(trainSet, testSet, index) { \n\t\tunseen_vocabulary = tokenizedataset(testSet, tokenize)\n\t\tseen_vocabulary = tokenizedataset(trainSet, tokenize)\n\t\tvar stats  = trainAndTest_hash(classifier, trainSet, testSet, 5);\n\t\n\t_.each(stats['data'],  function(report, key, list){ \n\t\tif (report['explanations']['FN'].length > 0)\n\t\t\t{\n\t\t\tunseen_words = _.difference(tokenize(normalizer(report['input'])), seen_vocabulary)\n\t\t\t_.each(unseen_words, function(word, key, list) {\n\t    \t\tif (!(word in unseen_correlation))\n\t    \t\t\t{\n    \t\t\t\tunseen_correlation[word] = []\n\t    \t\t\t}\n\t    \t\tunseen_correlation[word].push(report['explanations']['FN'])\n\t    \t\t})\n\t\t\t}\n\t\t})\n  \t})\n  \treturn unseen_correlation\n}\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/decisiontree/DecisionTreeDemo.js":"console.log(\"Decision Tree demo start\");\nvar DecisionTree = require('./DecisionTree');\n\nvar classifier = new DecisionTree({\n});\n\ndataset = [{input: {a:1 , b:0  }, output: 0},\n\t\t   {input: {a:0 , b:1  }, output: 0},\n\t\t   {input: {a:0 , b:0  }, output: 1}]\n\nclassifier.trainBatch(dataset);\n\nconsole.dir(classifier.classify({'a': 0, 'b': 0}, /*explain=*/1));\nconsole.dir(classifier.classify({'a': 1, 'b': 1}, /*explain=*/3));\n\nconsole.log(\"Decision Tree demo end\");\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/multilabel/BinaryRelevanceDemo.js":"// simple demonstration of Binary Relevance (one-vs.-all) classifier\n\nvar classifiers = require('..');\n\nvar trainSet = [\n\t\t{input: {'I':1,'want':1,'aa':1}, output: 'a'},\n\t\t{input: {'I':1,'want':1,'bb':1}, output: 'b'},\n\t\t{input: {'I':1,'want':1,'cc':1}, output: 'c'},\n\t\t];\n\nvar classifier = new classifiers.multilabel.BinaryRelevance({\n\tbinaryClassifierType: classifiers.Winnow.bind(0,{retrain_count:10})\n});\nclassifier.trainBatch(trainSet);\n\nconsole.log(\"simple classification: \");\nconsole.dir(classifier.classify({'I':1,'want':1,'aa':1}));  // a\nconsole.dir(classifier.classify({'I':1,'need':1,'bb':1}));  // b\nconsole.dir(classifier.classify({'I':1,'feel':1,'cc':1}));  // c\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1,'bb':1}));  // a,b\n\n//console.log(\"model: \");\n//console.dir(classifier);\n\nconsole.log(\"explained classification: \");\nconsole.dir(classifier.classify({'I':1,'want':1,'aa':1},5));  // a\nconsole.dir(classifier.classify({'I':1,'need':1,'bb':1},5));  // b\nconsole.dir(classifier.classify({'I':1,'feel':1,'cc':1},5));  // c\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1,'bb':1},5));  // a,b\n\nconsole.log(\"classification with scores: \");\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1},0,true));  // a\nconsole.dir(classifier.classify({'I':1,'need':1,'bb':1},0,true));  // b\nconsole.dir(classifier.classify({'I':1,'need':1,'cc':1},0,true));  // c\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1,'bb':1},0,true));  // a,b\n\nconsole.log(\"explained classification with scores: \");\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1},5,true));  // a\nconsole.dir(classifier.classify({'I':1,'need':1,'bb':1},5,true));  // b\nconsole.dir(classifier.classify({'I':1,'need':1,'cc':1},5,true));  // c\nconsole.dir(classifier.classify({'I':1,'need':1,'aa':1,'bb':1},5,true));  // a,b\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmJsDemo.js":"// simple demonstration of SVM\n\nvar SvmJs = require('./SvmJs');\n\nvar svm = new SvmJs({C: 1.0});\n\nvar traindata = [\n     {input: [0,0], output: 0},\n     {input: [0,1], output: 0}, \n     {input: [1,0], output: 1}, \n   \t {input: [1,1], output: 1},\n     ];\n\nsvm.trainBatch(traindata);\n\nconsole.dir(svm.classify([0,2]));  // 0\nconsole.dir(svm.classify([1,3]));  // 1\n\n// explain:\nconsole.dir(svm.classify([0,2], 3));  // 0\nconsole.dir(svm.classify([1,3], 3));  // 1\n\n\n//continuous output:\nconsole.dir(svm.classify([0,2], 0, true));  // -1\nconsole.dir(svm.classify([1,3], 0, true));  // 1\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmLinearDemo.js":"// simple demonstration of binary SVM, based on LibLinear\n\nvar SvmLinear = require('./SvmLinear');\nvar partitions = require(__dirname+'/../../utils/partitions');\n\nvar dataset = [\n\t\t{input: [0,0], output: 0},\n\t\t{input: [1,1], output: 0},\n\t\t{input: [0,1], output: 1},\n\t\t{input: [1,2], output: 1} ];\n\n// the separating line goes through [0,0.5] and [1,1.5]. It is:\n//       0.5+x-y = 0\n// or:   -1-2x+2y = 0\n\nvar classifier = new SvmLinear(\n\t{\n\t\tlearn_args: \"-c 20\", \n\t\tmodel_file_prefix: \"tempfiles/SvmLinearDemo\",\n\t\ttrain_command: \"liblinear_train\",\n\t\ttest_command: \"liblinear_test\",\n\t        multiclass: false\n\t}\n);\nclassifier.trainBatch(dataset);\n\nconsole.log(\"simple classification: \");\nconsole.dir(classifier.classify([0,2]));  // 1\nconsole.dir(classifier.classify([1,0]));  // 0\n\nconsole.log(\"model: \");\nconsole.dir(classifier.mapLabelToMapFeatureToWeight);   // { '0': -1, '1': -2, '2': 2 }\n\npartitions.partitions(dataset.concat(dataset), 2, function(train, test, index) {\n\n\tconsole.log(\"fold: \"+index)\n\tclassifier.trainBatch(train)\n\t\t\n\ttest.forEach(function(instance) {\n\t\tconsole.dir(\"Classify instance:\")\n\t\tconsole.dir(instance)\n\t\tconsole.dir(classifier.classify(instance.input));\n\t});\n});","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmLinearMulticlassDemo.js":"// simple demonstration of multiclass SVM, based on LibLinear\n\nvar SvmLinear = require('./SvmLinear');\n\nvar trainSet = [\n\t\t{input: [0,0], output: 3},\n\t\t{input: [1,1], output: 3},\n\t\t\n\t\t{input: [0,1], output: 4},\n\t\t{input: [1,2], output: 4},\n\t\t\n\t\t{input: [0,2], output: 5},\n\t\t{input: [1,3], output: 5},\n\t\t];\n\n// One separating line goes through [0,0.5] and [1,1.5]. It is:\n//        0.5+x-y = 0\n// or:   -1-2x+2y = 0\n\n//Another separating line goes through [0,1.5] and [1,2.5]. It is:\n//       1.5+x-y = 0\n//or:   -3-2x+2y = 0\n\n\nvar classifier = new SvmLinear(\n\t{\n\t\tlearn_args: \"-c 20\", \n\t\tmodel_file_prefix: \"tempfiles/SvmLinearMulticlassDemo\",\n\t\tmulticlass: true,\n\t\tdebug: false\n\t}\n);\nclassifier.trainBatch(trainSet);\n\nconsole.log(\"simple classification: \");\nconsole.dir(classifier.classify([1,0]));  // 3\nconsole.dir(classifier.classify([0,1.3]));  // 4\nconsole.dir(classifier.classify([0,1.7]));  // 5\nconsole.dir(classifier.classify([0,3]));  // 5\n\nconsole.log(\"model: \");\nconsole.dir(classifier.mapLabelToMapFeatureToWeight);   // { '0': -1, '1': -2, '2': 2 }\n\nconsole.log(\"explained classification: \");\nconsole.dir(classifier.classify([1,0],3));  // 3\nconsole.dir(classifier.classify([0,1.3],3));  // 4\nconsole.dir(classifier.classify([0,1.7],3));  // 5\nconsole.dir(classifier.classify([0,3],3));  // 5\n\nconsole.log(\"classification with scores: \");\nconsole.dir(classifier.classify([1,0],0,true));  // 3\nconsole.dir(classifier.classify([0,1.3],0,true));  // 4\nconsole.dir(classifier.classify([0,1.7],0,true));  // 5\nconsole.dir(classifier.classify([0,3],0,true));  // 5\n\nconsole.log(\"explained classification with scores: \");\nconsole.dir(classifier.classify([1,0],3,true));  // 3\nconsole.dir(classifier.classify([0,1.3],3,true));  // 4\nconsole.dir(classifier.classify([0,1.7],3,true));  // 5\nconsole.dir(classifier.classify([0,3],3,true));  // 5\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/svm/SvmPerfDemo.js":"// simple demonstration of binary SVM, based on SVM-Perf\n\nvar SvmPerf = require('./SvmPerf');\n\nvar trainSet = [\n\t\t{input: [0,0], output: 0},\n\t\t{input: [1,1], output: 0},\n\t\t{input: [0,1], output: 1},\n\t\t{input: [1,2], output: 1} ];\n\n// the separating line goes through [0,0.5] and [1,1.5]. It is:\n//       0.5+x-y = 0\n// or:   2y-2x-1 = 0\n\n\nvar classifier = new SvmPerf(\n\t{\n\t\tlearn_args: \"-c 20.0\", \n\t\tmodel_file_prefix: \"tempfiles/SvmPerfDemo\",\n\t\tdebug:false\n\t}\n);\nclassifier.trainBatch(trainSet);\n\n// binary output:\nconsole.dir(classifier.classify([0,2]));  // 1\nconsole.dir(classifier.classify([1,0]));  // 0\n\nconsole.dir(classifier.modelMap);   // { '0': -1, '1': -2, '2': 2 }\n\n// explain:\nconsole.dir(classifier.classify([0,2], 3));  // 1\nconsole.dir(classifier.classify([1,0], 3));  // 0\n\n// continuous output:\nconsole.dir(classifier.classify([0,2], 0, true));  // 3\nconsole.dir(classifier.classify([1,0], 0, true));  // -3\n","/home/travis/build/npmtest/node-npmtest-limdu/node_modules/limdu/classifiers/winnow/WinnowHashDemo.js":"/**\n * Demonstrates the winnow classification algorithm.\n * \n * @author Erel Segal-Halevi\n * @since 2013-07\n */\n\nconsole.log(\"Winnow demo start\");\nvar Winnow = require('./WinnowHash');\n\nvar classifier = new Winnow({\n\tdefault_positive_weight: 1,\n\tdefault_negative_weight: 1,\n\tthreshold: 0,\n\tdo_averaging: false,\n\tmargin: 1,\n});\n\nclassifier.trainOnline({'a': 1, 'b': 0}, 0);\nclassifier.trainOnline({'a': 0, 'b': 1}, 0);\nclassifier.trainOnline({'a': 0, 'b': 0}, 1);\n\nconsole.dir(classifier.classify({'a': 0, 'b': 0}, /*explain=*/1));\nconsole.dir(classifier.classify({'a': 1, 'b': 1}, /*explain=*/3));\n\nconsole.log(\"Winnow demo end\");\n"}